[2024-10-20 18:23:37,219][root][INFO] - 

[2024-10-20 18:23:37,219][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:23:37,219][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:23:37,219][root][INFO] - 

[2024-10-20 18:23:37,219][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:23:39,383][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:23:39,383][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:23:39,384][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:23:39,384][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:23:39,385][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:23:39,385][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:23:39,386][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:23:39,386][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:23:39,387][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:23:39,387][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:23:39,388][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:23:39,388][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:23:39,389][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:23:39,390][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:23:39,390][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:23:39,391][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:23:39,391][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:23:39,392][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:23:39,392][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:23:39,393][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:23:39,393][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:23:39,394][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:23:39,394][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:23:39,395][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:23:39,396][root][INFO] - Trainable params: 1769472 || all params: 126933504 || trainable: 1.39 %
[2024-10-20 18:23:39,603][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:23:39,604][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:23:42,192][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:25:52,062][root][INFO] - 

[2024-10-20 18:25:52,062][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:25:52,062][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:25:52,063][root][INFO] - 

[2024-10-20 18:25:52,063][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:25:54,266][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:25:54,268][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:25:54,269][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:25:54,269][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:25:54,270][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:25:54,270][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:25:54,271][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:25:54,271][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:25:54,272][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:25:54,273][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:25:54,273][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:25:54,274][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:25:54,275][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:25:54,275][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:25:54,276][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:25:54,276][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:25:54,277][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:25:54,278][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:25:54,278][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:25:54,279][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:25:54,279][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:25:54,280][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:25:54,280][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:25:54,281][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:25:54,283][root][INFO] - Trainable params: 1769472 || all params: 126933504 || trainable: 1.39 %
[2024-10-20 18:25:54,493][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:25:54,493][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:25:57,089][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:27:24,672][root][INFO] - 

[2024-10-20 18:27:24,672][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:27:24,672][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:27:24,673][root][INFO] - 

[2024-10-20 18:27:24,673][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:27:26,834][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:26,835][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:26,836][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:26,837][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:26,838][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:26,839][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:26,840][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:26,840][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:26,841][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:26,842][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:26,843][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:26,844][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:26,844][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:26,845][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:26,845][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:26,846][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:26,847][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:26,847][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:26,848][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:26,848][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:26,849][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:26,849][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:26,850][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:26,850][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:26,852][root][INFO] - Trainable params: 1769472 || all params: 126933504 || trainable: 1.39 %
[2024-10-20 18:27:27,057][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:27:27,058][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:27:29,666][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:27:42,624][root][INFO] - 

[2024-10-20 18:27:42,624][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:27:42,624][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:27:42,624][root][INFO] - 

[2024-10-20 18:27:42,624][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:27:44,801][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:44,802][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:44,803][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:44,804][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:44,805][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:44,805][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:44,806][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:44,807][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:44,808][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:44,808][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:44,809][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:44,810][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:44,810][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:44,811][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:44,811][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:44,812][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:44,813][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:44,813][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:44,814][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:44,814][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:44,815][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:44,816][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:44,816][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:27:44,817][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:27:44,819][root][INFO] - Trainable params: 1769472 || all params: 126933504 || trainable: 1.39 %
[2024-10-20 18:27:45,027][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:27:45,027][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:27:47,788][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:28:04,917][root][INFO] - 

[2024-10-20 18:28:04,917][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:28:04,917][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:28:04,918][root][INFO] - 

[2024-10-20 18:28:04,918][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:28:07,316][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:07,317][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:07,318][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:07,318][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:07,319][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:07,320][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:07,320][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:07,321][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:07,321][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:07,322][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:07,322][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:07,322][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:07,323][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:07,324][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:07,324][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:07,325][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:07,325][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:07,326][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:07,326][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:07,327][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:07,327][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:07,328][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:07,329][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:07,329][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:07,331][root][INFO] - Trainable params: 1769472 || all params: 126933504 || trainable: 1.39 %
[2024-10-20 18:28:07,539][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:28:07,540][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:28:10,134][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:28:55,080][root][INFO] - 

[2024-10-20 18:28:55,080][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:28:55,080][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:28:55,080][root][INFO] - 

[2024-10-20 18:28:55,081][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:28:57,268][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:57,269][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:57,270][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:57,271][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:57,271][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:57,272][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:57,272][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:57,273][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:57,273][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:57,274][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:57,274][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:57,275][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:57,275][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:57,276][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:57,276][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:57,277][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:57,277][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:57,278][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:57,278][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:57,279][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:57,279][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:57,280][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:57,281][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:28:57,281][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:28:57,283][root][INFO] - Trainable params: 1769472 || all params: 126933504 || trainable: 1.39 %
[2024-10-20 18:28:57,491][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:28:57,491][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:29:00,051][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:34:18,161][root][INFO] - 

[2024-10-20 18:34:18,161][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:34:18,161][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:34:18,161][root][INFO] - 

[2024-10-20 18:34:18,161][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:34:20,397][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:20,399][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:20,400][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:20,401][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:20,401][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:20,402][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:20,403][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:20,404][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:20,404][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:20,405][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:20,405][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:20,406][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:20,407][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:20,407][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:20,408][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:20,408][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:20,409][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:20,410][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:20,410][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:20,411][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:20,412][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:20,412][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:20,413][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:20,414][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:20,416][root][INFO] - Trainable params: 1769472 || all params: 126933504 || trainable: 1.39 %
[2024-10-20 18:34:20,625][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:34:20,626][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:34:23,199][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:34:57,883][root][INFO] - 

[2024-10-20 18:34:57,883][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:34:57,883][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:34:57,883][root][INFO] - 

[2024-10-20 18:34:57,883][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:34:59,982][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:59,983][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:59,984][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:59,984][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:59,985][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:59,986][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:59,986][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:59,987][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:59,988][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:59,988][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:59,989][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:59,989][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:59,990][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:59,991][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:59,991][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:59,992][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:59,993][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:59,993][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:59,994][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:59,994][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:59,995][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:59,995][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:59,996][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:34:59,996][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:34:59,999][root][INFO] - Trainable params: 1769472 || all params: 126933504 || trainable: 1.39 %
[2024-10-20 18:35:00,209][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:35:00,210][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:35:02,802][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:35:35,508][root][INFO] - 

[2024-10-20 18:35:35,508][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:35:35,508][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:35:35,508][root][INFO] - 

[2024-10-20 18:35:35,508][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:35:37,612][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:35:37,613][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:35:37,614][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:35:37,615][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:35:37,616][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:35:37,616][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:35:37,617][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:35:37,617][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:35:37,618][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:35:37,618][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:35:37,619][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:35:37,619][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:35:37,620][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:35:37,620][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:35:37,621][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:35:37,622][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:35:37,622][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:35:37,623][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:35:37,623][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:35:37,624][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:35:37,624][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:35:37,625][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:35:37,625][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:35:37,626][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:35:37,628][root][INFO] - Trainable params: 1769472 || all params: 126933504 || trainable: 1.39 %
[2024-10-20 18:35:37,835][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:35:37,836][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:35:40,384][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:37:21,129][root][INFO] - 

[2024-10-20 18:37:21,129][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:37:21,129][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:37:21,129][root][INFO] - 

[2024-10-20 18:37:21,130][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:37:23,242][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:37:23,243][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:37:23,244][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:37:23,244][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:37:23,245][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:37:23,246][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:37:23,246][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:37:23,247][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:37:23,248][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:37:23,248][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:37:23,249][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:37:23,249][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:37:23,250][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:37:23,250][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:37:23,251][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:37:23,251][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:37:23,252][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:37:23,252][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:37:23,253][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:37:23,253][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:37:23,254][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:37:23,254][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:37:23,255][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:37:23,255][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:37:23,460][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:37:23,460][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:37:25,999][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:39:42,769][root][INFO] - 

[2024-10-20 18:39:42,769][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:39:42,769][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:39:42,769][root][INFO] - 

[2024-10-20 18:39:42,770][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:39:49,435][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:39:49,436][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:39:49,437][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:39:49,438][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:39:49,439][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:39:49,439][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:39:49,440][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:39:49,441][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:39:49,441][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:39:49,442][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:39:49,442][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:39:49,443][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:39:49,444][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:39:49,444][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:39:49,445][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:39:49,446][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:39:49,446][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:39:49,447][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:39:49,447][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:39:49,448][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:39:49,449][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:39:49,450][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:39:49,450][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:39:49,451][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:39:49,662][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:39:49,663][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:39:52,253][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:44:47,604][root][INFO] - 

[2024-10-20 18:44:47,605][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:44:47,605][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:44:47,605][root][INFO] - 

[2024-10-20 18:44:47,605][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:44:49,747][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:44:49,748][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:44:49,749][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:44:49,749][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:44:49,750][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:44:49,751][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:44:49,751][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:44:49,752][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:44:49,752][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:44:49,753][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:44:49,753][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:44:49,754][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:44:49,754][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:44:49,755][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:44:49,755][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:44:49,756][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:44:49,756][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:44:49,757][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:44:49,758][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:44:49,758][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:44:49,759][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:44:49,759][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:44:49,760][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:44:49,760][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:44:49,965][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:44:49,966][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:44:52,569][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:50:01,823][root][INFO] - 

[2024-10-20 18:50:01,823][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:50:01,823][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:50:01,823][root][INFO] - 

[2024-10-20 18:50:01,823][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:50:05,760][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:05,761][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:05,762][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:05,762][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:05,763][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:05,764][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:05,764][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:05,765][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:05,766][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:05,766][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:05,767][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:05,768][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:05,769][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:05,769][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:05,770][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:05,770][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:05,771][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:05,771][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:05,772][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:05,772][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:05,773][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:05,774][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:05,774][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:05,775][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:05,984][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:50:05,985][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:50:08,591][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:50:31,697][root][INFO] - 

[2024-10-20 18:50:31,697][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:50:31,697][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:50:31,697][root][INFO] - 

[2024-10-20 18:50:31,697][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:50:33,788][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:33,789][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:33,790][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:33,790][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:33,792][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:33,792][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:33,793][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:33,794][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:33,795][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:33,795][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:33,796][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:33,797][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:33,797][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:33,798][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:33,798][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:33,799][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:33,799][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:33,800][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:33,800][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:33,801][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:33,801][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:33,802][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:33,802][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:50:33,803][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:50:34,012][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:50:34,013][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:50:36,499][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:50:58,333][root][INFO] - 

[2024-10-20 18:50:58,333][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:50:58,333][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:50:58,333][root][INFO] - 

[2024-10-20 18:50:58,333][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:51:00,490][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:00,490][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:00,491][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:00,492][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:00,492][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:00,493][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:00,493][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:00,494][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:00,495][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:00,495][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:00,496][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:00,496][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:00,497][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:00,498][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:00,499][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:00,499][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:00,500][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:00,500][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:00,501][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:00,502][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:00,503][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:00,503][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:00,504][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:00,505][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:00,758][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:51:00,759][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:51:03,363][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:51:14,117][root][INFO] - 

[2024-10-20 18:51:14,117][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:51:14,117][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:51:14,117][root][INFO] - 

[2024-10-20 18:51:14,117][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:51:16,469][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:16,471][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:16,472][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:16,473][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:16,474][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:16,475][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:16,476][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:16,477][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:16,478][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:16,478][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:16,479][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:16,480][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:16,481][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:16,482][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:16,482][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:16,483][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:16,484][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:16,485][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:16,485][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:16,486][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:16,487][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:16,487][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:16,488][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:16,488][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:16,697][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:51:16,698][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:51:19,332][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:51:27,394][root][INFO] - 

[2024-10-20 18:51:27,394][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:51:27,394][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:51:27,394][root][INFO] - 

[2024-10-20 18:51:27,394][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:51:29,543][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:29,544][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:29,544][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:29,545][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:29,545][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:29,546][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:29,546][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:29,547][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:29,547][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:29,548][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:29,548][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:29,549][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:29,549][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:29,550][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:29,550][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:29,551][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:29,551][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:29,552][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:29,552][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:29,553][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:29,554][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:29,554][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:29,554][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:29,555][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:29,762][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:51:29,763][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:51:32,360][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:51:53,069][root][INFO] - 

[2024-10-20 18:51:53,069][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:51:53,069][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:51:53,069][root][INFO] - 

[2024-10-20 18:51:53,070][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:51:55,192][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:55,193][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:55,194][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:55,195][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:55,195][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:55,196][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:55,197][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:55,197][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:55,198][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:55,199][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:55,199][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:55,200][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:55,200][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:55,201][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:55,201][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:55,202][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:55,202][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:55,203][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:55,203][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:55,204][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:55,204][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:55,205][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:55,205][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:51:55,206][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:51:55,420][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:51:55,420][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:51:58,043][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:52:08,241][root][INFO] - 

[2024-10-20 18:52:08,242][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:52:08,242][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:52:08,242][root][INFO] - 

[2024-10-20 18:52:08,242][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:52:11,411][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:52:11,412][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:52:11,413][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:52:11,413][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:52:11,414][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:52:11,414][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:52:11,415][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:52:11,415][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:52:11,416][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:52:11,416][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:52:11,417][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:52:11,417][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:52:11,418][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:52:11,418][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:52:11,419][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:52:11,419][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:52:11,420][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:52:11,420][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:52:11,421][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:52:11,421][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:52:11,422][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:52:11,423][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:52:11,423][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:52:11,424][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:52:11,646][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:52:11,647][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:52:14,248][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:53:27,511][root][INFO] - 

[2024-10-20 18:53:27,512][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:53:27,512][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:53:27,512][root][INFO] - 

[2024-10-20 18:53:27,512][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:53:29,670][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:53:29,671][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:53:29,672][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:53:29,672][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:53:29,673][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:53:29,674][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:53:29,675][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:53:29,676][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:53:29,676][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:53:29,677][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:53:29,678][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:53:29,678][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:53:29,679][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:53:29,680][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:53:29,680][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:53:29,681][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:53:29,682][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:53:29,682][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:53:29,683][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:53:29,684][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:53:29,684][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:53:29,685][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:53:29,686][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:53:29,686][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:53:29,928][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:53:29,929][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:53:32,803][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:54:24,327][root][INFO] - 

[2024-10-20 18:54:24,327][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:54:24,327][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:54:24,327][root][INFO] - 

[2024-10-20 18:54:24,327][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:54:26,425][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:54:26,426][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:54:26,426][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:54:26,427][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:54:26,428][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:54:26,428][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:54:26,429][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:54:26,429][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:54:26,429][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:54:26,430][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:54:26,430][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:54:26,431][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:54:26,431][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:54:26,432][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:54:26,432][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:54:26,433][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:54:26,433][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:54:26,434][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:54:26,434][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:54:26,435][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:54:26,436][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:54:26,436][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:54:26,437][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:54:26,437][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:54:26,644][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:54:26,645][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:54:29,230][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:55:08,525][root][INFO] - 

[2024-10-20 18:55:08,525][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:55:08,525][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:55:08,525][root][INFO] - 

[2024-10-20 18:55:08,526][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:55:10,716][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:55:10,717][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:55:10,718][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:55:10,719][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:55:10,720][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:55:10,720][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:55:10,721][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:55:10,721][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:55:10,722][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:55:10,722][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:55:10,723][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:55:10,724][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:55:10,724][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:55:10,725][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:55:10,726][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:55:10,726][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:55:10,727][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:55:10,728][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:55:10,728][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:55:10,729][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:55:10,729][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:55:10,730][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:55:10,730][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:55:10,731][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:55:10,941][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:55:10,942][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:55:13,832][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:55:58,195][root][INFO] - 

[2024-10-20 18:55:58,195][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:55:58,195][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:55:58,195][root][INFO] - 

[2024-10-20 18:55:58,195][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:56:00,434][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:00,435][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:00,435][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:00,436][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:00,436][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:00,436][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:00,437][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:00,437][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:00,438][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:00,438][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:00,439][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:00,439][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:00,440][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:00,440][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:00,441][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:00,441][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:00,442][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:00,442][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:00,443][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:00,443][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:00,444][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:00,444][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:00,445][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:00,445][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:00,664][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:56:00,665][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:56:03,323][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:56:33,448][root][INFO] - 

[2024-10-20 18:56:33,448][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:56:33,448][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:56:33,448][root][INFO] - 

[2024-10-20 18:56:33,448][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:56:35,924][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:35,925][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:35,926][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:35,927][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:35,927][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:35,928][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:35,928][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:35,929][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:35,930][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:35,930][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:35,931][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:35,932][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:35,932][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:35,933][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:35,933][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:35,934][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:35,934][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:35,935][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:35,935][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:35,936][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:35,936][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:35,937][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:35,937][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:56:35,938][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:56:36,142][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:56:36,143][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:56:38,651][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:57:24,782][root][INFO] - 

[2024-10-20 18:57:24,782][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:57:24,782][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:57:24,782][root][INFO] - 

[2024-10-20 18:57:24,783][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:57:27,015][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:57:27,016][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:57:27,017][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:57:27,017][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:57:27,018][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:57:27,018][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:57:27,019][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:57:27,019][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:57:27,020][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:57:27,021][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:57:27,021][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:57:27,022][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:57:27,023][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:57:27,023][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:57:27,024][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:57:27,024][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:57:27,025][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:57:27,026][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:57:27,026][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:57:27,027][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:57:27,027][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:57:27,028][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:57:27,028][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:57:27,029][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:57:27,247][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:57:27,248][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:57:29,840][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:58:07,143][root][INFO] - 

[2024-10-20 18:58:07,143][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:58:07,143][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:58:07,143][root][INFO] - 

[2024-10-20 18:58:07,143][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:58:09,240][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:58:09,241][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:58:09,242][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:58:09,243][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:58:09,244][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:58:09,244][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:58:09,245][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:58:09,245][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:58:09,246][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:58:09,246][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:58:09,246][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:58:09,247][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:58:09,248][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:58:09,248][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:58:09,249][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:58:09,249][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:58:09,250][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:58:09,250][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:58:09,251][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:58:09,251][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:58:09,252][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:58:09,252][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:58:09,253][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:58:09,253][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:58:09,465][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:58:09,466][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:58:12,055][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:59:10,318][root][INFO] - 

[2024-10-20 18:59:10,318][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:59:10,319][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:59:10,319][root][INFO] - 

[2024-10-20 18:59:10,319][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:59:12,734][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:12,734][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:12,736][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:12,737][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:12,737][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:12,738][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:12,739][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:12,740][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:12,740][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:12,741][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:12,741][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:12,742][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:12,742][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:12,743][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:12,743][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:12,744][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:12,744][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:12,745][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:12,745][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:12,746][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:12,746][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:12,747][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:12,747][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:12,748][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:12,957][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:59:12,958][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:59:15,497][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 18:59:46,814][root][INFO] - 

[2024-10-20 18:59:46,814][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 18:59:46,814][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:59:46,814][root][INFO] - 

[2024-10-20 18:59:46,814][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 18:59:48,944][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:48,945][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:48,946][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:48,947][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:48,948][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:48,949][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:48,949][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:48,950][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:48,951][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:48,951][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:48,952][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:48,952][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:48,953][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:48,954][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:48,954][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:48,955][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:48,956][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:48,956][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:48,957][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:48,957][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:48,958][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:48,959][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:48,959][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 18:59:48,960][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 18:59:49,220][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 18:59:49,221][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 18:59:52,117][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:00:06,488][root][INFO] - 

[2024-10-20 19:00:06,488][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:00:06,489][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:00:06,489][root][INFO] - 

[2024-10-20 19:00:06,489][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:00:08,628][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:08,629][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:08,629][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:08,630][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:08,630][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:08,631][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:08,631][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:08,632][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:08,632][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:08,632][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:08,633][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:08,633][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:08,634][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:08,634][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:08,635][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:08,635][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:08,636][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:08,636][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:08,637][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:08,637][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:08,638][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:08,638][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:08,638][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:08,639][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:08,873][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:00:08,874][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:00:11,484][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:00:53,849][root][INFO] - 

[2024-10-20 19:00:53,849][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:00:53,849][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:00:53,849][root][INFO] - 

[2024-10-20 19:00:53,849][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:00:55,972][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:55,973][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:55,974][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:55,975][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:55,976][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:55,976][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:55,977][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:55,978][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:55,979][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:55,979][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:55,980][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:55,981][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:55,981][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:55,982][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:55,983][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:55,983][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:55,984][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:55,984][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:55,985][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:55,986][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:55,986][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:55,987][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:55,988][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:00:55,988][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:00:56,198][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:00:56,199][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:00:58,766][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:01:24,926][root][INFO] - 

[2024-10-20 19:01:24,927][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:01:24,927][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:01:24,927][root][INFO] - 

[2024-10-20 19:01:24,927][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:01:27,037][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:01:27,038][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:01:27,039][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:01:27,040][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:01:27,040][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:01:27,041][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:01:27,042][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:01:27,043][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:01:27,043][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:01:27,044][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:01:27,044][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:01:27,045][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:01:27,045][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:01:27,046][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:01:27,047][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:01:27,047][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:01:27,048][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:01:27,049][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:01:27,049][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:01:27,050][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:01:27,050][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:01:27,051][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:01:27,051][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:01:27,052][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:01:27,262][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:01:27,263][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:01:29,890][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:03:15,812][root][INFO] - 

[2024-10-20 19:03:15,812][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:03:15,812][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:03:15,812][root][INFO] - 

[2024-10-20 19:03:15,812][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:03:18,206][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:18,207][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:18,208][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:18,208][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:18,209][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:18,210][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:18,211][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:18,211][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:18,212][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:18,212][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:18,213][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:18,213][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:18,214][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:18,214][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:18,215][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:18,215][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:18,216][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:18,216][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:18,217][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:18,218][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:18,218][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:18,219][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:18,219][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:18,220][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:18,428][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:03:18,429][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:03:21,035][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:03:34,936][root][INFO] - 

[2024-10-20 19:03:34,937][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:03:34,937][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:03:34,937][root][INFO] - 

[2024-10-20 19:03:34,937][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:03:37,033][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:37,034][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:37,035][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:37,036][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:37,037][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:37,037][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:37,038][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:37,039][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:37,039][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:37,040][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:37,040][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:37,041][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:37,042][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:37,042][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:37,043][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:37,044][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:37,044][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:37,045][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:37,045][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:37,046][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:37,047][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:37,047][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:37,048][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:37,049][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:37,259][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:03:37,260][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:03:39,898][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:03:54,829][root][INFO] - 

[2024-10-20 19:03:54,829][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:03:54,829][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:03:54,829][root][INFO] - 

[2024-10-20 19:03:54,830][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:03:57,070][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:57,071][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:57,072][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:57,073][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:57,074][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:57,074][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:57,076][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:57,076][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:57,077][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:57,078][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:57,079][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:57,080][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:57,080][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:57,081][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:57,081][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:57,082][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:57,082][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:57,083][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:57,083][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:57,083][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:57,084][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:57,084][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:57,085][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:03:57,085][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:03:57,292][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:03:57,292][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:03:59,799][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:04:28,871][root][INFO] - 

[2024-10-20 19:04:28,871][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:04:28,871][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:04:28,871][root][INFO] - 

[2024-10-20 19:04:28,871][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:04:31,093][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:31,094][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:31,095][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:31,095][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:31,096][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:31,096][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:31,097][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:31,097][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:31,098][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:31,098][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:31,099][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:31,099][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:31,100][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:31,100][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:31,101][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:31,101][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:31,102][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:31,102][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:31,103][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:31,103][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:31,104][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:31,104][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:31,105][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:31,105][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:31,319][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:04:31,320][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:04:33,887][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:04:42,825][root][INFO] - 

[2024-10-20 19:04:42,826][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:04:42,826][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:04:42,826][root][INFO] - 

[2024-10-20 19:04:42,826][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:04:45,472][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:45,472][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:45,473][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:45,474][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:45,474][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:45,475][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:45,476][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:45,476][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:45,477][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:45,477][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:45,478][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:45,479][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:45,479][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:45,480][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:45,480][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:45,481][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:45,481][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:45,482][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:45,482][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:45,482][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:45,483][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:45,484][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:45,484][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:04:45,484][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:04:45,693][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:04:45,694][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:04:48,273][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:05:17,897][root][INFO] - 

[2024-10-20 19:05:17,897][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:05:17,897][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:05:17,897][root][INFO] - 

[2024-10-20 19:05:17,897][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:05:20,027][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:20,027][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:20,028][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:20,029][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:20,030][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:20,031][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:20,031][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:20,032][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:20,033][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:20,034][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:20,034][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:20,035][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:20,035][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:20,036][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:20,036][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:20,037][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:20,038][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:20,038][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:20,038][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:20,039][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:20,040][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:20,040][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:20,041][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:20,041][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:20,251][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:05:20,252][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:05:22,865][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:05:52,149][root][INFO] - 

[2024-10-20 19:05:52,149][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:05:52,150][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:05:52,150][root][INFO] - 

[2024-10-20 19:05:52,150][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:05:54,307][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:54,308][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:54,309][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:54,310][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:54,311][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:54,311][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:54,312][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:54,313][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:54,313][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:54,314][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:54,315][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:54,315][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:54,316][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:54,317][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:54,317][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:54,318][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:54,318][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:54,319][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:54,320][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:54,320][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:54,321][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:54,321][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:54,322][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:05:54,322][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:05:54,533][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:05:54,534][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:05:57,131][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:08:01,667][root][INFO] - 

[2024-10-20 19:08:01,667][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:08:01,667][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:08:01,668][root][INFO] - 

[2024-10-20 19:08:01,668][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:08:03,818][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:08:03,819][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:08:03,820][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:08:03,821][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:08:03,823][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:08:03,824][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:08:03,825][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:08:03,826][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:08:03,827][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:08:03,828][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:08:03,829][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:08:03,830][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:08:03,831][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:08:03,831][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:08:03,832][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:08:03,833][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:08:03,833][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:08:03,833][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:08:03,834][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:08:03,834][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:08:03,835][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:08:03,835][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:08:03,836][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:08:03,836][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:08:04,048][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:08:04,049][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:08:06,903][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:09:03,627][root][INFO] - 

[2024-10-20 19:09:03,627][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:09:03,627][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:09:03,627][root][INFO] - 

[2024-10-20 19:09:03,627][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:09:05,789][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:05,790][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:05,791][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:05,791][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:05,792][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:05,792][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:05,793][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:05,793][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:05,794][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:05,794][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:05,795][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:05,796][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:05,796][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:05,797][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:05,797][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:05,798][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:05,798][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:05,799][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:05,799][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:05,800][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:05,801][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:05,801][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:05,802][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:05,802][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:06,010][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:09:06,011][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:09:08,827][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:09:54,294][root][INFO] - 

[2024-10-20 19:09:54,294][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:09:54,294][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:09:54,294][root][INFO] - 

[2024-10-20 19:09:54,295][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:09:56,684][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:56,685][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:56,686][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:56,686][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:56,686][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:56,687][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:56,687][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:56,688][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:56,688][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:56,689][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:56,689][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:56,690][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:56,690][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:56,691][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:56,691][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:56,692][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:56,692][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:56,693][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:56,693][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:56,694][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:56,694][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:56,695][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:56,696][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:09:56,696][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:09:56,902][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:09:56,903][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:09:59,472][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:19:17,226][root][INFO] - 

[2024-10-20 19:19:17,227][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:19:17,227][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:19:17,227][root][INFO] - 

[2024-10-20 19:19:17,227][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:19:19,373][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:19,374][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:19,375][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:19,375][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:19,376][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:19,377][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:19,377][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:19,378][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:19,378][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:19,379][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:19,379][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:19,380][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:19,381][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:19,381][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:19,382][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:19,382][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:19,383][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:19,383][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:19,384][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:19,384][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:19,385][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:19,386][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:19,386][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:19,387][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:19,594][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:19:19,595][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:19:22,517][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:19:23,575][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,578][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,579][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,581][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,581][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,582][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,582][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,583][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,584][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,584][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,584][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,607][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,608][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,609][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,610][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,610][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,611][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,613][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,614][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,615][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,616][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,617][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,618][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,626][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,627][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,628][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,629][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,630][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,631][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,632][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,633][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,634][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,635][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,636][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,637][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,700][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,702][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,703][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,704][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,705][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,706][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,707][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,708][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,709][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,710][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,710][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,711][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,716][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,717][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,718][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,719][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,719][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,720][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,721][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,722][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,723][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,723][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,724][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:23,725][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumPen' not found.
[2024-10-20 19:19:51,999][root][INFO] - 

[2024-10-20 19:19:52,000][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:19:52,000][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:19:52,000][root][INFO] - 

[2024-10-20 19:19:52,000][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:19:54,252][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:54,253][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:54,254][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:54,254][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:54,255][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:54,255][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:54,256][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:54,256][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:54,257][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:54,258][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:54,258][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:54,259][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:54,259][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:54,260][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:54,260][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:54,261][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:54,261][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:54,262][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:54,262][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:54,263][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:54,263][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:54,264][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:54,264][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:19:54,265][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:19:54,470][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:19:54,471][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:19:57,344][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:19:58,349][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,352][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,353][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,355][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,355][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,356][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,356][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,357][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,357][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,358][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,358][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,385][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,386][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,387][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,388][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,389][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,391][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,392][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,393][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,394][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,395][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,396][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,397][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,405][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,406][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,407][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,408][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,409][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,410][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,411][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,412][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,413][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,414][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,415][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,416][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,477][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,478][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,479][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,480][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,480][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,481][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,482][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,483][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,484][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,484][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,485][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,486][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,491][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,492][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,493][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,494][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,494][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,495][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,496][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,497][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,498][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,498][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,499][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:19:58,500][matplotlib.font_manager][WARNING] - findfont: Font family 'Nanum Pen' not found.
[2024-10-20 19:20:41,886][root][INFO] - 

[2024-10-20 19:20:41,886][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:20:41,886][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:20:41,886][root][INFO] - 

[2024-10-20 19:20:41,886][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:20:44,028][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:20:44,029][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:20:44,030][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:20:44,030][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:20:44,031][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:20:44,031][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:20:44,032][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:20:44,033][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:20:44,033][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:20:44,033][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:20:44,034][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:20:44,034][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:20:44,035][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:20:44,035][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:20:44,036][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:20:44,036][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:20:44,037][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:20:44,037][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:20:44,037][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:20:44,038][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:20:44,038][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:20:44,039][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:20:44,039][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:20:44,040][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:20:44,263][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:20:44,264][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:20:46,797][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:20:47,806][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,809][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,810][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,813][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,814][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,814][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,815][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,817][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,818][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,818][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,819][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,848][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,850][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,851][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,852][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,853][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,855][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,856][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,857][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,858][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,859][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,861][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,862][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,870][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,871][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,872][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,873][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,874][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,875][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,877][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,878][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,879][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,880][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,881][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,882][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,943][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,944][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,944][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,945][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,946][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,947][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,948][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,949][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,949][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,950][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,951][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,952][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,957][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,958][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,958][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,959][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,960][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,961][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,962][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,962][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,963][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,964][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,965][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:20:47,965][matplotlib.font_manager][WARNING] - findfont: Font family 'NanumGothic' not found.
[2024-10-20 19:21:27,655][root][INFO] - 

[2024-10-20 19:21:27,655][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:21:27,655][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:21:27,655][root][INFO] - 

[2024-10-20 19:21:27,655][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:21:29,888][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:21:29,889][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:21:29,890][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:21:29,891][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:21:29,894][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:21:29,894][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:21:29,895][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:21:29,895][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:21:29,896][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:21:29,897][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:21:29,897][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:21:29,898][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:21:29,899][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:21:29,899][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:21:29,900][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:21:29,900][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:21:29,901][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:21:29,902][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:21:29,902][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:21:29,903][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:21:29,903][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:21:29,904][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:21:29,904][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:21:29,905][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:21:30,115][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:21:30,115][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:21:32,682][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:21:33,762][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,765][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,766][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,768][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,769][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,769][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,770][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,770][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,771][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,771][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,772][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,794][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,795][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,796][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,797][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,798][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,799][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,800][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,801][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,801][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,802][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,804][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,805][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,811][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,812][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,813][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,814][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,815][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,816][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,817][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,818][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,819][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,819][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,820][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,821][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,876][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,878][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,879][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,881][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,883][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,885][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,886][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,888][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,889][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,890][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,892][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,893][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,903][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,904][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,905][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,906][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,907][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,908][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,909][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,910][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,911][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,912][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,913][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:21:33,913][matplotlib.font_manager][WARNING] - findfont: Font family '' not found.
[2024-10-20 19:24:19,182][root][INFO] - 

[2024-10-20 19:24:19,182][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:24:19,182][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:24:19,182][root][INFO] - 

[2024-10-20 19:24:19,183][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:24:22,055][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:24:22,056][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:24:22,057][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:24:22,058][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:24:22,058][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:24:22,059][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:24:22,060][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:24:22,060][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:24:22,061][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:24:22,062][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:24:22,062][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:24:22,062][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:24:22,063][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:24:22,064][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:24:22,064][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:24:22,065][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:24:22,065][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:24:22,066][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:24:22,066][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:24:22,067][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:24:22,067][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:24:22,068][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:24:22,068][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:24:22,069][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:24:22,276][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:24:22,277][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:24:24,907][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:25:23,411][root][INFO] - 

[2024-10-20 19:25:23,411][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:25:23,411][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:25:23,411][root][INFO] - 

[2024-10-20 19:25:23,411][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:25:25,876][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:25:25,877][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:25:25,877][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:25:25,878][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:25:25,879][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:25:25,883][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:25:25,884][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:25:25,885][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:25:25,885][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:25:25,886][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:25:25,886][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:25:25,887][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:25:25,887][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:25:25,888][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:25:25,888][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:25:25,888][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:25:25,889][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:25:25,889][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:25:25,890][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:25:25,891][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:25:25,891][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:25:25,892][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:25:25,892][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:25:25,893][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:25:26,105][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:25:26,106][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:25:29,034][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:27:59,173][root][INFO] - 

[2024-10-20 19:27:59,174][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:27:59,174][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:27:59,174][root][INFO] - 

[2024-10-20 19:27:59,174][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:28:02,060][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:02,061][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:02,062][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:02,062][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:02,063][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:02,064][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:02,064][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:02,065][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:02,066][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:02,066][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:02,067][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:02,067][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:02,068][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:02,069][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:02,069][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:02,070][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:02,071][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:02,071][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:02,072][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:02,073][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:02,073][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:02,074][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:02,074][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:02,075][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:02,286][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:28:02,287][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:28:04,877][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:28:37,499][root][INFO] - 

[2024-10-20 19:28:37,499][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:28:37,499][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:28:37,499][root][INFO] - 

[2024-10-20 19:28:37,499][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:28:39,785][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:39,786][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:39,787][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:39,788][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:39,788][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:39,789][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:39,790][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:39,790][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:39,791][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:39,792][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:39,792][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:39,793][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:39,794][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:39,795][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:39,795][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:39,796][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:39,796][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:39,797][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:39,797][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:39,798][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:39,798][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:39,799][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:39,799][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:28:39,800][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:28:40,022][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:28:40,023][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:28:42,692][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:29:20,767][root][INFO] - 

[2024-10-20 19:29:20,767][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:29:20,767][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:29:20,767][root][INFO] - 

[2024-10-20 19:29:20,767][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:29:23,054][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:29:23,055][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:29:23,055][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:29:23,056][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:29:23,056][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:29:23,057][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:29:23,057][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:29:23,058][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:29:23,058][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:29:23,059][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:29:23,059][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:29:23,060][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:29:23,060][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:29:23,061][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:29:23,061][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:29:23,062][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:29:23,062][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:29:23,062][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:29:23,063][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:29:23,063][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:29:23,064][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:29:23,064][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:29:23,065][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:29:23,065][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:29:23,281][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:29:23,282][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:29:25,920][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:33:09,878][root][INFO] - 

[2024-10-20 19:33:09,879][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:33:09,879][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:33:09,879][root][INFO] - 

[2024-10-20 19:33:09,879][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:33:12,268][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:12,269][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:12,270][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:12,271][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:12,271][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:12,272][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:12,273][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:12,273][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:12,274][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:12,274][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:12,275][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:12,275][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:12,276][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:12,276][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:12,277][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:12,277][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:12,278][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:12,278][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:12,279][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:12,280][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:12,280][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:12,281][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:12,281][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:12,282][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:12,490][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:33:12,491][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:33:15,061][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:33:56,735][root][INFO] - 

[2024-10-20 19:33:56,736][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:33:56,736][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:33:56,736][root][INFO] - 

[2024-10-20 19:33:56,736][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:33:58,934][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:58,936][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:58,937][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:58,938][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:58,939][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:58,941][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:58,942][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:58,943][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:58,944][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:58,945][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:58,945][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:58,946][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:58,947][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:58,947][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:58,947][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:58,948][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:58,948][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:58,949][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:58,949][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:58,950][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:58,950][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:58,951][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:58,951][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:33:58,952][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:33:59,165][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:33:59,166][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:34:01,772][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:34:24,359][root][INFO] - 

[2024-10-20 19:34:24,359][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:34:24,359][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:34:24,359][root][INFO] - 

[2024-10-20 19:34:24,359][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:34:26,458][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:34:26,459][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:34:26,459][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:34:26,460][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:34:26,462][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:34:26,463][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:34:26,464][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:34:26,465][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:34:26,465][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:34:26,466][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:34:26,467][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:34:26,468][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:34:26,469][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:34:26,470][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:34:26,470][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:34:26,471][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:34:26,472][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:34:26,473][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:34:26,474][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:34:26,474][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:34:26,475][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:34:26,476][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:34:26,477][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:34:26,477][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:34:26,693][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:34:26,694][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:34:29,539][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-20 19:38:24,119][root][INFO] - 

[2024-10-20 19:38:24,120][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-20 19:38:24,120][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:38:24,120][root][INFO] - 

[2024-10-20 19:38:24,120][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 1956, 'do_combination': True, 'embedding_norm': True, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 10, 'max_length': 128, 'max_new_tokens': 30, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 5, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 163, 'num_workers': 4, 'task_name': 'KoCommonGen'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 64, 'eval_batch_size': 16}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-20 19:38:26,208][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-20 19:38:26,209][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-20 19:38:26,210][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-20 19:38:26,210][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-20 19:38:26,211][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-20 19:38:26,212][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-20 19:38:26,213][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-20 19:38:26,214][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-20 19:38:26,214][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-20 19:38:26,215][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-20 19:38:26,216][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-20 19:38:26,216][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-20 19:38:26,217][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-20 19:38:26,218][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-20 19:38:26,218][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-20 19:38:26,219][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-20 19:38:26,220][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-20 19:38:26,220][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-20 19:38:26,221][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-20 19:38:26,221][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-20 19:38:26,222][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-20 19:38:26,222][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-20 19:38:26,223][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-20 19:38:26,224][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-20 19:38:26,431][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-20 19:38:26,431][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-20 19:38:29,085][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 01:41:07,371][root][INFO] - 

[2024-10-21 01:41:07,371][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 01:41:07,371][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-21 01:41:07,371][root][INFO] - 

[2024-10-21 01:41:07,371][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4067, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-21 01:41:10,604][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:10,605][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:10,606][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:10,606][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:10,607][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:10,607][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:10,608][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:10,609][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:10,609][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:10,610][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:10,611][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:10,611][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:10,612][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:10,612][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:10,613][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:10,613][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:10,613][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:10,614][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:10,614][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:10,615][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:10,615][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:10,616][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:10,616][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:10,617][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:10,621][root][INFO] - Change the max_length to 4065 for the kombo_tokenizer's truncation.
[2024-10-21 01:41:10,825][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 01:41:10,825][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-21 01:41:53,747][root][INFO] - 

[2024-10-21 01:41:53,747][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 01:41:53,747][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-21 01:41:53,747][root][INFO] - 

[2024-10-21 01:41:53,748][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4067, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-21 01:41:56,044][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:56,045][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:56,046][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:56,047][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:56,047][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:56,048][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:56,049][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:56,050][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:56,050][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:56,051][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:56,051][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:56,052][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:56,053][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:56,053][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:56,054][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:56,054][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:56,055][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:56,056][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:56,056][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:56,057][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:56,058][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:56,058][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:56,059][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 01:41:56,059][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 01:41:56,065][root][INFO] - Change the max_length to 4065 for the kombo_tokenizer's truncation.
[2024-10-21 01:41:56,273][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 01:41:56,273][root][INFO] - 
Save directory: lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-21 01:41:58,869][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 01:44:30,015][root][INFO] - 

[2024-10-21 01:44:30,015][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 01:44:30,015][root][INFO] - Save the parser information to analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-21 01:44:30,015][root][INFO] - 

[2024-10-21 01:44:30,015][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4067, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/cross_attention/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-21 01:44:32,187][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 01:44:32,188][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 01:44:32,190][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 01:44:32,190][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 01:44:32,191][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 01:44:32,192][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 01:44:32,193][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 01:44:32,194][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 01:44:32,195][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 01:44:32,196][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 01:44:32,197][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 01:44:32,197][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 01:44:32,198][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 01:44:32,199][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 01:44:32,200][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 01:44:32,200][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 01:44:32,201][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 01:44:32,202][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 01:44:32,203][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 01:44:32,203][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 01:44:32,204][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 01:44:32,205][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 01:44:32,206][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 01:44:32,206][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 01:44:32,211][root][INFO] - Change the max_length to 4065 for the kombo_tokenizer's truncation.
[2024-10-21 01:44:32,416][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 01:44:32,417][root][INFO] - 
Save directory: lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-21 01:44:35,244][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 09:32:17,635][root][INFO] - 

[2024-10-21 09:32:17,635][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 09:32:17,635][root][INFO] - Save the parser information to logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_en_punc/128t_64b_8s_5e-05lr_1rs
[2024-10-21 09:32:17,635][root][INFO] - 

[2024-10-21 09:32:17,635][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_en_punc', 'do_hangeulize': True, 'data_remove': True, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_en_punc/128t_64b_8s_5e-05lr_1rs', 'tb_dir': 'logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_en_punc/128t_64b_8s_5e-05lr_1rs/tb', 'save_dir': 'logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_en_punc/128t_64b_8s_5e-05lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 09:32:19,915][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 09:32:19,916][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 09:32:19,916][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 09:32:19,917][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 09:32:19,918][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 09:32:19,918][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 09:32:19,919][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 09:32:19,919][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 09:32:19,920][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 09:32:19,921][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 09:32:19,921][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 09:32:19,922][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 09:32:19,923][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 09:32:19,923][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 09:32:19,924][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 09:32:19,925][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 09:32:19,925][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 09:32:19,926][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 09:32:19,926][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 09:32:19,928][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 09:32:19,929][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 09:32:19,929][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 09:32:19,930][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 09:32:19,930][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 09:32:19,931][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 09:39:02,548][root][INFO] - 

[2024-10-21 09:39:02,548][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 09:39:02,549][root][INFO] - Save the parser information to logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_en_punc/128t_64b_8s_5e-05lr_1rs
[2024-10-21 09:39:02,549][root][INFO] - 

[2024-10-21 09:39:02,549][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_en_punc', 'do_hangeulize': True, 'data_remove': True, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_en_punc/128t_64b_8s_5e-05lr_1rs', 'tb_dir': 'logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_en_punc/128t_64b_8s_5e-05lr_1rs/tb', 'save_dir': 'logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_en_punc/128t_64b_8s_5e-05lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 09:41:27,090][root][INFO] - 

[2024-10-21 09:41:27,090][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 09:41:27,090][root][INFO] - Save the parser information to logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs
[2024-10-21 09:41:27,090][root][INFO] - 

[2024-10-21 09:41:27,090][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs', 'tb_dir': 'logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs/tb', 'save_dir': 'logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 09:41:38,013][root][INFO] - 

[2024-10-21 09:41:38,013][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 09:41:38,014][root][INFO] - Save the parser information to logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs
[2024-10-21 09:41:38,014][root][INFO] - 

[2024-10-21 09:41:38,014][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs', 'tb_dir': 'logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs/tb', 'save_dir': 'logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 09:42:58,718][root][INFO] - 

[2024-10-21 09:42:58,719][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 09:42:58,719][root][INFO] - Save the parser information to logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs
[2024-10-21 09:42:58,719][root][INFO] - 

[2024-10-21 09:42:58,719][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs', 'tb_dir': 'logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs/tb', 'save_dir': 'logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 09:43:00,701][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 09:43:00,701][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 09:43:00,702][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 09:43:00,703][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 09:43:00,703][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 09:43:00,704][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 09:43:00,704][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 09:43:00,705][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 09:43:00,705][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 09:43:00,706][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 09:43:00,706][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 09:43:00,707][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 09:43:00,707][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 09:43:00,708][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 09:43:00,709][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 09:43:00,709][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 09:43:00,710][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 09:43:00,710][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 09:43:00,711][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 09:43:00,711][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 09:43:00,712][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 09:43:00,712][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 09:43:00,713][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 09:43:00,713][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 09:43:00,714][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 09:43:03,159][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 09:44:12,856][root][INFO] - 

[2024-10-21 09:44:12,856][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 09:44:12,856][root][INFO] - Save the parser information to logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs
[2024-10-21 09:44:12,856][root][INFO] - 

[2024-10-21 09:44:12,856][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs', 'tb_dir': 'logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs/tb', 'save_dir': 'logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 09:44:14,795][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:14,795][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:14,796][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:14,796][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:14,797][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:14,797][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:14,798][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:14,798][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:14,799][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:14,799][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:14,800][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:14,800][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:14,800][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:14,801][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:14,801][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:14,802][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:14,802][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:14,803][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:14,803][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:14,804][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:14,804][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:14,805][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:14,805][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:14,805][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:14,806][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 09:44:17,266][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 09:44:26,729][root][INFO] - 

[2024-10-21 09:44:26,729][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 09:44:26,729][root][INFO] - Save the parser information to logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs
[2024-10-21 09:44:26,729][root][INFO] - 

[2024-10-21 09:44:26,730][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs', 'tb_dir': 'logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs/tb', 'save_dir': 'logs/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 09:44:28,664][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:28,665][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:28,666][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:28,666][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:28,667][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:28,667][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:28,667][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:28,668][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:28,668][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:28,669][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:28,669][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:28,670][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:28,670][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:28,671][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:28,671][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:28,672][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:28,672][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:28,673][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:28,673][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:28,674][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:28,675][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:28,675][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:28,676][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 09:44:28,676][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 09:44:28,684][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 09:44:28,889][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 09:44:28,889][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 09:44:31,452][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 09:47:55,162][root][INFO] - 

[2024-10-21 09:47:55,162][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 09:47:55,163][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs
[2024-10-21 09:47:55,163][root][INFO] - 

[2024-10-21 09:47:55,163][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 09:47:57,211][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 09:47:57,212][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 09:47:57,212][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 09:47:57,213][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 09:47:57,213][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 09:47:57,214][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 09:47:57,214][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 09:47:57,215][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 09:47:57,215][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 09:47:57,216][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 09:47:57,216][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 09:47:57,217][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 09:47:57,217][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 09:47:57,218][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 09:47:57,219][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 09:47:57,219][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 09:47:57,220][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 09:47:57,220][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 09:47:57,221][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 09:47:57,221][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 09:47:57,222][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 09:47:57,223][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 09:47:57,223][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 09:47:57,224][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 09:47:57,224][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 09:47:59,687][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 09:48:08,907][root][INFO] - 

[2024-10-21 09:48:08,907][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 09:48:08,907][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs
[2024-10-21 09:48:08,907][root][INFO] - 

[2024-10-21 09:48:08,908][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlu_tasks/KorSTS/ko_punc/128t_64b_8s_5e-05lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 09:48:11,092][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 09:48:11,093][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 09:48:11,093][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 09:48:11,094][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 09:48:11,094][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 09:48:11,095][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 09:48:11,095][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 09:48:11,096][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 09:48:11,096][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 09:48:11,097][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 09:48:11,097][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 09:48:11,098][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 09:48:11,098][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 09:48:11,099][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 09:48:11,099][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 09:48:11,100][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 09:48:11,100][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 09:48:11,101][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 09:48:11,101][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 09:48:11,102][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 09:48:11,103][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 09:48:11,103][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 09:48:11,104][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 09:48:11,104][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 09:48:11,110][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 09:48:11,315][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 09:48:11,316][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 09:48:13,819][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 09:55:50,039][root][INFO] - 

[2024-10-21 09:55:50,039][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 09:55:50,039][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 09:55:50,039][root][INFO] - 

[2024-10-21 09:55:50,040][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 09:55:52,792][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 09:55:52,792][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 09:55:52,793][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 09:55:52,794][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 09:55:52,794][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 09:55:52,794][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 09:55:52,795][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 09:55:52,796][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 09:55:52,796][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 09:55:52,796][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 09:55:52,797][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 09:55:52,797][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 09:55:52,798][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 09:55:52,799][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 09:55:52,799][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 09:55:52,800][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 09:55:52,800][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 09:55:52,801][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 09:55:52,801][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 09:55:52,802][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 09:55:52,802][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 09:55:52,803][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 09:55:52,804][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 09:55:52,804][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 09:55:52,804][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 09:55:55,221][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 09:56:04,154][root][INFO] - 

[2024-10-21 09:56:04,155][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 09:56:04,155][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 09:56:04,155][root][INFO] - 

[2024-10-21 09:56:04,156][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 09:56:06,275][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 09:56:06,276][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 09:56:06,276][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 09:56:06,277][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 09:56:06,277][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 09:56:06,278][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 09:56:06,278][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 09:56:06,279][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 09:56:06,279][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 09:56:06,280][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 09:56:06,280][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 09:56:06,281][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 09:56:06,281][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 09:56:06,282][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 09:56:06,282][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 09:56:06,283][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 09:56:06,283][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 09:56:06,284][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 09:56:06,284][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 09:56:06,285][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 09:56:06,285][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 09:56:06,286][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 09:56:06,286][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 09:56:06,287][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 09:56:06,292][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 09:56:06,496][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 09:56:06,497][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 09:56:09,020][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 09:57:40,482][root][INFO] - 

[2024-10-21 09:57:40,482][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 09:57:40,483][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 09:57:40,483][root][INFO] - 

[2024-10-21 09:57:40,483][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 09:57:42,623][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 09:57:42,624][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 09:57:42,624][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 09:57:42,625][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 09:57:42,625][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 09:57:42,626][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 09:57:42,626][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 09:57:42,626][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 09:57:42,627][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 09:57:42,627][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 09:57:42,628][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 09:57:42,628][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 09:57:42,629][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 09:57:42,629][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 09:57:42,630][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 09:57:42,630][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 09:57:42,631][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 09:57:42,631][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 09:57:42,632][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 09:57:42,632][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 09:57:42,632][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 09:57:42,633][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 09:57:42,633][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 09:57:42,634][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 09:57:42,634][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 09:57:45,086][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 09:57:55,481][root][INFO] - 

[2024-10-21 09:57:55,481][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 09:57:55,481][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 09:57:55,481][root][INFO] - 

[2024-10-21 09:57:55,481][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 09:58:02,587][root][INFO] - 

[2024-10-21 09:58:02,588][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 09:58:02,588][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 09:58:02,588][root][INFO] - 

[2024-10-21 09:58:02,588][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 09:58:06,714][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:06,715][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:06,716][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:06,717][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:06,718][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:06,719][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:06,720][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:06,721][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:06,722][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:06,723][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:06,724][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:06,725][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:06,725][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:06,726][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:06,726][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:06,727][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:06,727][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:06,728][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:06,728][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:06,729][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:06,729][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:06,730][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:06,730][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:06,731][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:06,731][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 09:58:09,123][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 09:58:18,854][root][INFO] - 

[2024-10-21 09:58:18,854][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 09:58:18,854][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 09:58:18,854][root][INFO] - 

[2024-10-21 09:58:18,854][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 09:58:20,771][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:20,771][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:20,772][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:20,773][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:20,774][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:20,774][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:20,775][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:20,775][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:20,776][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:20,777][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:20,777][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:20,778][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:20,779][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:20,780][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:20,780][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:20,781][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:20,782][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:20,782][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:20,783][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:20,784][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:20,784][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:20,785][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:20,786][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 09:58:20,786][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 09:58:20,793][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 09:58:21,047][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 09:58:21,048][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 09:58:23,801][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 09:59:29,356][root][INFO] - 

[2024-10-21 09:59:29,356][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 09:59:29,356][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 09:59:29,356][root][INFO] - 

[2024-10-21 09:59:29,356][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 09:59:31,245][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:31,245][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:31,246][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:31,246][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:31,247][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:31,247][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:31,247][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:31,248][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:31,248][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:31,248][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:31,249][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:31,249][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:31,250][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:31,250][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:31,251][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:31,251][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:31,252][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:31,252][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:31,252][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:31,253][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:31,253][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:31,254][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:31,254][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:31,255][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:31,255][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 09:59:33,707][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 09:59:42,397][root][INFO] - 

[2024-10-21 09:59:42,397][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 09:59:42,397][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 09:59:42,397][root][INFO] - 

[2024-10-21 09:59:42,397][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 09:59:44,319][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:44,320][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:44,320][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:44,320][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:44,321][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:44,321][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:44,322][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:44,322][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:44,322][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:44,323][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:44,323][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:44,324][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:44,324][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:44,325][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:44,325][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:44,325][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:44,326][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:44,326][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:44,327][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:44,327][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:44,328][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:44,328][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:44,329][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 09:59:44,329][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 09:59:44,335][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 09:59:44,538][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 09:59:44,539][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 09:59:47,092][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 11:01:03,588][root][INFO] - 

[2024-10-21 11:01:03,588][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 11:01:03,589][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 11:01:03,589][root][INFO] - 

[2024-10-21 11:01:03,589][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 11:01:05,638][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:05,638][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:05,639][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:05,639][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:05,640][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:05,641][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:05,641][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:05,642][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:05,642][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:05,643][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:05,644][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:05,644][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:05,644][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:05,645][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:05,645][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:05,646][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:05,647][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:05,647][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:05,648][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:05,648][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:05,649][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:05,649][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:05,650][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:05,651][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:05,651][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 11:01:08,226][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 11:01:18,301][root][INFO] - 

[2024-10-21 11:01:18,302][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 11:01:18,302][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 11:01:18,302][root][INFO] - 

[2024-10-21 11:01:18,302][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 11:01:20,310][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:20,310][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:20,311][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:20,312][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:20,312][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:20,313][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:20,313][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:20,314][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:20,315][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:20,315][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:20,315][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:20,316][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:20,316][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:20,317][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:20,318][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:20,318][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:20,319][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:20,319][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:20,320][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:20,320][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:20,321][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:20,321][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:20,322][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:20,323][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:20,326][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 11:01:20,527][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 11:01:20,527][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 11:01:42,630][root][INFO] - 

[2024-10-21 11:01:42,631][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 11:01:42,631][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 11:01:42,631][root][INFO] - 

[2024-10-21 11:01:42,631][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 11:01:44,788][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:44,789][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:44,790][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:44,790][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:44,791][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:44,791][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:44,792][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:44,792][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:44,793][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:44,793][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:44,794][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:44,794][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:44,795][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:44,795][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:44,796][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:44,796][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:44,797][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:44,797][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:44,798][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:44,798][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:44,799][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:44,800][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:44,800][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 11:01:44,801][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 11:01:44,801][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 11:01:47,227][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 11:02:04,228][root][INFO] - 

[2024-10-21 11:02:04,228][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 11:02:04,228][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 11:02:04,228][root][INFO] - 

[2024-10-21 11:02:04,228][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 11:02:06,117][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:06,118][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:06,118][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:06,119][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:06,119][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:06,119][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:06,120][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:06,120][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:06,121][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:06,121][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:06,122][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:06,122][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:06,123][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:06,123][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:06,123][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:06,124][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:06,124][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:06,125][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:06,125][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:06,126][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:06,126][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:06,127][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:06,127][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:06,127][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:06,128][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 11:02:08,585][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 11:02:17,567][root][INFO] - 

[2024-10-21 11:02:17,567][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 11:02:17,567][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 11:02:17,567][root][INFO] - 

[2024-10-21 11:02:17,567][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 11:02:19,463][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:19,463][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:19,464][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:19,464][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:19,465][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:19,465][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:19,466][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:19,466][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:19,467][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:19,467][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:19,467][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:19,468][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:19,468][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:19,469][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:19,469][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:19,470][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:19,470][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:19,471][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:19,471][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:19,472][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:19,472][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:19,473][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:19,473][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 11:02:19,474][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 11:02:19,480][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 11:02:19,682][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 11:02:19,682][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 11:02:22,242][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 11:17:53,146][root][INFO] - 

[2024-10-21 11:17:53,147][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 11:17:53,147][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 11:17:53,147][root][INFO] - 

[2024-10-21 11:17:53,147][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 11:17:55,303][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 11:17:55,304][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 11:17:55,304][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 11:17:55,305][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 11:17:55,305][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 11:17:55,306][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 11:17:55,306][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 11:17:55,307][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 11:17:55,307][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 11:17:55,307][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 11:17:55,308][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 11:17:55,308][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 11:17:55,309][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 11:17:55,309][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 11:17:55,310][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 11:17:55,310][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 11:17:55,311][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 11:17:55,311][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 11:17:55,312][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 11:17:55,312][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 11:17:55,313][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 11:17:55,313][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 11:17:55,314][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 11:17:55,314][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 11:17:55,314][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 11:17:57,727][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 11:18:07,626][root][INFO] - 

[2024-10-21 11:18:07,626][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 11:18:07,627][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 11:18:07,627][root][INFO] - 

[2024-10-21 11:18:07,627][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 11:18:09,622][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 11:18:09,622][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 11:18:09,622][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 11:18:09,623][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 11:18:09,623][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 11:18:09,624][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 11:18:09,624][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 11:18:09,624][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 11:18:09,625][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 11:18:09,625][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 11:18:09,626][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 11:18:09,626][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 11:18:09,627][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 11:18:09,627][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 11:18:09,628][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 11:18:09,628][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 11:18:09,628][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 11:18:09,629][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 11:18:09,629][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 11:18:09,629][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 11:18:09,630][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 11:18:09,630][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 11:18:09,631][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 11:18:09,631][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 11:18:09,635][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 11:18:09,842][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 11:18:09,843][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 11:18:12,395][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 11:19:30,268][root][INFO] - 

[2024-10-21 11:19:30,268][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 11:19:30,268][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 11:19:30,268][root][INFO] - 

[2024-10-21 11:19:30,268][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 11:19:34,330][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:34,330][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:34,331][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:34,331][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:34,332][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:34,332][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:34,333][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:34,333][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:34,334][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:34,334][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:34,335][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:34,335][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:34,336][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:34,336][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:34,337][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:34,337][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:34,338][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:34,338][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:34,339][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:34,339][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:34,340][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:34,341][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:34,341][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:34,342][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:34,342][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 11:19:36,783][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 11:19:46,833][root][INFO] - 

[2024-10-21 11:19:46,834][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 11:19:46,834][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 11:19:46,834][root][INFO] - 

[2024-10-21 11:19:46,834][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 11:19:48,749][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:48,750][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:48,750][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:48,751][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:48,751][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:48,752][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:48,752][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:48,753][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:48,753][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:48,754][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:48,754][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:48,755][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:48,755][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:48,756][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:48,757][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:48,757][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:48,758][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:48,758][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:48,759][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:48,759][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:48,760][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:48,761][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:48,761][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 11:19:48,762][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 11:19:48,768][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 11:19:48,973][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 11:19:48,973][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 11:19:51,538][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 11:21:57,796][root][INFO] - 

[2024-10-21 11:21:57,796][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 11:21:57,797][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 11:21:57,797][root][INFO] - 

[2024-10-21 11:21:57,797][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 11:21:59,783][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 11:21:59,784][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 11:21:59,784][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 11:21:59,785][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 11:21:59,785][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 11:21:59,786][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 11:21:59,786][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 11:21:59,787][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 11:21:59,787][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 11:21:59,788][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 11:21:59,788][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 11:21:59,789][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 11:21:59,789][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 11:21:59,790][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 11:21:59,790][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 11:21:59,791][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 11:21:59,791][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 11:21:59,792][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 11:21:59,792][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 11:21:59,793][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 11:21:59,793][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 11:21:59,794][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 11:21:59,794][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 11:21:59,795][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 11:21:59,795][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 11:22:02,350][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 11:22:23,865][root][INFO] - 

[2024-10-21 11:22:23,865][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 11:22:23,865][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 11:22:23,865][root][INFO] - 

[2024-10-21 11:22:23,865][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 11:22:25,805][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 11:22:25,806][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 11:22:25,806][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 11:22:25,806][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 11:22:25,807][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 11:22:25,807][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 11:22:25,808][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 11:22:25,808][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 11:22:25,808][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 11:22:25,809][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 11:22:25,809][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 11:22:25,810][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 11:22:25,810][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 11:22:25,810][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 11:22:25,811][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 11:22:25,811][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 11:22:25,812][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 11:22:25,812][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 11:22:25,813][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 11:22:25,813][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 11:22:25,813][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 11:22:25,814][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 11:22:25,814][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 11:22:25,815][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 11:22:25,815][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 11:22:28,277][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 11:23:33,605][root][INFO] - 

[2024-10-21 11:23:33,605][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 11:23:33,605][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 11:23:33,605][root][INFO] - 

[2024-10-21 11:23:33,605][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 11:23:36,398][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 11:23:36,398][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 11:23:36,399][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 11:23:36,399][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 11:23:36,400][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 11:23:36,400][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 11:23:36,401][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 11:23:36,401][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 11:23:36,402][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 11:23:36,402][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 11:23:36,403][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 11:23:36,403][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 11:23:36,404][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 11:23:36,404][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 11:23:36,404][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 11:23:36,405][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 11:23:36,405][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 11:23:36,406][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 11:23:36,406][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 11:23:36,407][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 11:23:36,408][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 11:23:36,409][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 11:23:36,409][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 11:23:36,410][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 11:23:36,411][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 11:23:38,828][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 14:48:17,884][root][INFO] - 

[2024-10-21 14:48:17,884][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 14:48:17,884][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 14:48:17,884][root][INFO] - 

[2024-10-21 14:48:17,885][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 14:48:19,887][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 14:48:19,888][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 14:48:19,888][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 14:48:19,889][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 14:48:19,889][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 14:48:19,890][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 14:48:19,890][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 14:48:19,890][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 14:48:19,891][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 14:48:19,891][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 14:48:19,892][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 14:48:19,892][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 14:48:19,893][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 14:48:19,893][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 14:48:19,894][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 14:48:19,894][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 14:48:19,895][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 14:48:19,895][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 14:48:19,896][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 14:48:19,896][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 14:48:19,897][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 14:48:19,897][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 14:48:19,898][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 14:48:19,898][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 14:48:19,899][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 14:48:22,361][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 14:49:10,696][root][INFO] - 

[2024-10-21 14:49:10,696][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 14:49:10,697][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 14:49:10,697][root][INFO] - 

[2024-10-21 14:49:10,697][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 14:49:13,284][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 14:49:13,285][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 14:49:13,286][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 14:49:13,286][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 14:49:13,287][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 14:49:13,288][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 14:49:13,288][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 14:49:13,289][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 14:49:13,289][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 14:49:13,289][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 14:49:13,290][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 14:49:13,290][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 14:49:13,291][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 14:49:13,292][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 14:49:13,292][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 14:49:13,293][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 14:49:13,293][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 14:49:13,294][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 14:49:13,294][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 14:49:13,295][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 14:49:13,295][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 14:49:13,296][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 14:49:13,296][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 14:49:13,297][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 14:49:13,297][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 14:49:15,762][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 14:50:07,049][root][INFO] - 

[2024-10-21 14:50:07,049][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 14:50:07,049][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 14:50:07,049][root][INFO] - 

[2024-10-21 14:50:07,049][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 14:50:09,077][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:09,077][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:09,078][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:09,078][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:09,079][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:09,079][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:09,080][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:09,080][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:09,081][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:09,081][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:09,082][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:09,082][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:09,083][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:09,083][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:09,084][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:09,084][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:09,085][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:09,085][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:09,086][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:09,086][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:09,087][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:09,088][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:09,088][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:09,089][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:09,089][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 14:50:11,528][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 14:50:40,774][root][INFO] - 

[2024-10-21 14:50:40,774][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 14:50:40,775][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 14:50:40,775][root][INFO] - 

[2024-10-21 14:50:40,775][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 14:50:42,938][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:42,938][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:42,939][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:42,939][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:42,940][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:42,940][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:42,940][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:42,941][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:42,941][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:42,942][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:42,942][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:42,943][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:42,943][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:42,944][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:42,945][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:42,945][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:42,946][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:42,946][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:42,947][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:42,947][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:42,948][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:42,948][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:42,949][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 14:50:42,949][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 14:50:42,949][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 14:50:45,424][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 14:51:45,306][root][INFO] - 

[2024-10-21 14:51:45,306][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 14:51:45,306][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 14:51:45,306][root][INFO] - 

[2024-10-21 14:51:45,306][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 14:51:47,348][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 14:51:47,349][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 14:51:47,350][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 14:51:47,350][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 14:51:47,351][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 14:51:47,352][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 14:51:47,352][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 14:51:47,353][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 14:51:47,354][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 14:51:47,354][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 14:51:47,355][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 14:51:47,355][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 14:51:47,356][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 14:51:47,356][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 14:51:47,357][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 14:51:47,357][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 14:51:47,358][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 14:51:47,358][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 14:51:47,359][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 14:51:47,359][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 14:51:47,360][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 14:51:47,361][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 14:51:47,361][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 14:51:47,362][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 14:51:47,362][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 14:51:50,076][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 14:52:20,482][root][INFO] - 

[2024-10-21 14:52:20,482][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 14:52:20,482][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 14:52:20,482][root][INFO] - 

[2024-10-21 14:52:20,482][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 14:52:22,465][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 14:52:22,465][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 14:52:22,466][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 14:52:22,466][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 14:52:22,466][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 14:52:22,467][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 14:52:22,467][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 14:52:22,468][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 14:52:22,468][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 14:52:22,469][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 14:52:22,469][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 14:52:22,469][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 14:52:22,470][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 14:52:22,470][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 14:52:22,471][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 14:52:22,471][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 14:52:22,472][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 14:52:22,472][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 14:52:22,472][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 14:52:22,473][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 14:52:22,473][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 14:52:22,474][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 14:52:22,474][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 14:52:22,474][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 14:52:22,475][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 14:52:24,885][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 14:53:14,285][root][INFO] - 

[2024-10-21 14:53:14,285][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 14:53:14,285][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 14:53:14,285][root][INFO] - 

[2024-10-21 14:53:14,286][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 14:53:16,583][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:16,583][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:16,584][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:16,584][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:16,584][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:16,585][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:16,585][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:16,586][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:16,586][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:16,586][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:16,587][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:16,587][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:16,588][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:16,588][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:16,589][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:16,589][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:16,589][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:16,590][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:16,590][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:16,591][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:16,591][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:16,592][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:16,592][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:16,592][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:16,593][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 14:53:19,069][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 14:53:47,261][root][INFO] - 

[2024-10-21 14:53:47,261][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 14:53:47,261][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 14:53:47,261][root][INFO] - 

[2024-10-21 14:53:47,261][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 14:53:49,171][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:49,172][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:49,172][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:49,173][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:49,173][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:49,173][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:49,174][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:49,174][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:49,175][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:49,175][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:49,176][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:49,176][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:49,177][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:49,177][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:49,178][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:49,178][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:49,179][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:49,179][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:49,180][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:49,181][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:49,182][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:49,183][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:49,183][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 14:53:49,184][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 14:53:49,185][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 14:53:51,605][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 14:54:15,147][root][INFO] - 

[2024-10-21 14:54:15,147][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 14:54:15,147][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 14:54:15,148][root][INFO] - 

[2024-10-21 14:54:15,148][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 14:54:17,230][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:17,231][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:17,231][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:17,232][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:17,232][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:17,233][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:17,233][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:17,234][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:17,234][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:17,235][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:17,235][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:17,235][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:17,236][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:17,236][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:17,237][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:17,237][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:17,238][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:17,238][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:17,239][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:17,239][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:17,240][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:17,240][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:17,241][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:17,241][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:17,242][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 14:54:19,705][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 14:54:29,079][root][INFO] - 

[2024-10-21 14:54:29,079][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 14:54:29,079][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 14:54:29,079][root][INFO] - 

[2024-10-21 14:54:29,079][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 14:54:31,168][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:31,169][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:31,169][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:31,170][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:31,170][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:31,171][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:31,171][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:31,172][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:31,172][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:31,172][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:31,173][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:31,173][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:31,174][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:31,174][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:31,175][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:31,175][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:31,176][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:31,176][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:31,176][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:31,177][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:31,177][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:31,178][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:31,178][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 14:54:31,179][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 14:54:31,184][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 14:54:31,386][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 14:54:31,387][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 14:54:33,941][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 14:56:00,742][root][INFO] - 

[2024-10-21 14:56:00,742][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 14:56:00,742][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 14:56:00,742][root][INFO] - 

[2024-10-21 14:56:00,742][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 14:56:02,740][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:02,741][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:02,741][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:02,742][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:02,742][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:02,743][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:02,743][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:02,744][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:02,744][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:02,744][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:02,745][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:02,745][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:02,746][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:02,746][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:02,747][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:02,747][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:02,748][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:02,748][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:02,748][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:02,749][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:02,750][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:02,750][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:02,751][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:02,751][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:02,751][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 14:56:05,127][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 14:56:15,053][root][INFO] - 

[2024-10-21 14:56:15,053][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 14:56:15,053][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 14:56:15,053][root][INFO] - 

[2024-10-21 14:56:15,053][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 14:56:17,422][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:17,423][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:17,423][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:17,423][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:17,424][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:17,424][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:17,425][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:17,425][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:17,426][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:17,426][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:17,426][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:17,427][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:17,427][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:17,428][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:17,428][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:17,429][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:17,429][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:17,430][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:17,430][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:17,431][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:17,431][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:17,432][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:17,432][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 14:56:17,433][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 14:56:17,438][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 14:56:17,640][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 14:56:17,641][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 14:56:20,165][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 14:59:24,802][root][INFO] - 

[2024-10-21 14:59:24,802][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 14:59:24,802][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 14:59:24,802][root][INFO] - 

[2024-10-21 14:59:24,802][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 14:59:26,825][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 14:59:26,826][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 14:59:26,827][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 14:59:26,827][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 14:59:26,828][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 14:59:26,829][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 14:59:26,830][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 14:59:26,830][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 14:59:26,831][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 14:59:26,831][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 14:59:26,832][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 14:59:26,833][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 14:59:26,833][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 14:59:26,834][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 14:59:26,834][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 14:59:26,835][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 14:59:26,836][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 14:59:26,836][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 14:59:26,837][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 14:59:26,838][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 14:59:26,839][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 14:59:26,839][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 14:59:26,840][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 14:59:26,841][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 14:59:26,841][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 14:59:29,257][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 15:16:21,119][root][INFO] - 

[2024-10-21 15:16:21,119][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 15:16:21,119][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 15:16:21,119][root][INFO] - 

[2024-10-21 15:16:21,119][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 15:16:23,716][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 15:16:23,716][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 15:16:23,717][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 15:16:23,717][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 15:16:23,718][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 15:16:23,718][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 15:16:23,719][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 15:16:23,719][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 15:16:23,719][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 15:16:23,720][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 15:16:23,720][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 15:16:23,721][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 15:16:23,721][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 15:16:23,722][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 15:16:23,722][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 15:16:23,723][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 15:16:23,723][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 15:16:23,723][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 15:16:23,724][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 15:16:23,724][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 15:16:23,725][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 15:16:23,725][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 15:16:23,726][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 15:16:23,726][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 15:16:23,726][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 15:16:26,119][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 15:16:35,352][root][INFO] - 

[2024-10-21 15:16:35,352][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 15:16:35,353][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 15:16:35,353][root][INFO] - 

[2024-10-21 15:16:35,353][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 15:18:14,006][root][INFO] - 

[2024-10-21 15:18:14,006][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 15:18:14,006][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 15:18:14,006][root][INFO] - 

[2024-10-21 15:18:14,006][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 15:18:16,294][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 15:18:16,295][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 15:18:16,296][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 15:18:16,296][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 15:18:16,297][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 15:18:16,298][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 15:18:16,298][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 15:18:16,299][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 15:18:16,300][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 15:18:16,300][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 15:18:16,301][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 15:18:16,302][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 15:18:16,302][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 15:18:16,303][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 15:18:16,304][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 15:18:16,305][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 15:18:16,305][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 15:18:16,306][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 15:18:16,307][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 15:18:16,308][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 15:18:16,309][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 15:18:16,309][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 15:18:16,310][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 15:18:16,311][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 15:18:16,311][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 15:18:18,833][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 15:19:30,475][root][INFO] - 

[2024-10-21 15:19:30,475][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 15:19:30,475][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 15:19:30,475][root][INFO] - 

[2024-10-21 15:19:30,476][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 15:19:32,527][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 15:19:32,528][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 15:19:32,529][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 15:19:32,529][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 15:19:32,530][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 15:19:32,530][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 15:19:32,531][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 15:19:32,531][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 15:19:32,532][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 15:19:32,533][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 15:19:32,534][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 15:19:32,535][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 15:19:32,535][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 15:19:32,536][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 15:19:32,537][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 15:19:32,538][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 15:19:32,538][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 15:19:32,539][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 15:19:32,540][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 15:19:32,540][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 15:19:32,541][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 15:19:32,541][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 15:19:32,542][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 15:19:32,542][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 15:19:32,543][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 15:19:34,884][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 15:22:10,329][root][INFO] - 

[2024-10-21 15:22:10,329][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 15:22:10,329][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 15:22:10,329][root][INFO] - 

[2024-10-21 15:22:10,329][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 15:22:12,549][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 15:22:12,550][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 15:22:12,550][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 15:22:12,551][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 15:22:12,551][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 15:22:12,552][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 15:22:12,552][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 15:22:12,552][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 15:22:12,553][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 15:22:12,553][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 15:22:12,554][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 15:22:12,554][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 15:22:12,555][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 15:22:12,555][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 15:22:12,556][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 15:22:12,556][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 15:22:12,557][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 15:22:12,557][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 15:22:12,557][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 15:22:12,558][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 15:22:12,559][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 15:22:12,559][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 15:22:12,559][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 15:22:12,560][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 15:22:12,560][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 15:22:15,033][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 15:23:28,712][root][INFO] - 

[2024-10-21 15:23:28,713][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 15:23:28,713][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 15:23:28,713][root][INFO] - 

[2024-10-21 15:23:28,713][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 15:23:30,735][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:30,735][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:30,736][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:30,736][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:30,736][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:30,737][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:30,737][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:30,738][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:30,738][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:30,739][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:30,739][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:30,739][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:30,740][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:30,740][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:30,741][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:30,741][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:30,742][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:30,742][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:30,742][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:30,743][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:30,743][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:30,744][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:30,744][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:30,745][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:30,745][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 15:23:33,208][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 15:23:42,636][root][INFO] - 

[2024-10-21 15:23:42,636][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 15:23:42,636][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 15:23:42,636][root][INFO] - 

[2024-10-21 15:23:42,636][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 15:23:44,553][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:44,553][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:44,554][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:44,554][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:44,554][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:44,555][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:44,555][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:44,556][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:44,556][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:44,556][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:44,557][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:44,557][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:44,558][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:44,558][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:44,559][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:44,559][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:44,559][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:44,560][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:44,560][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:44,561][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:44,561][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:44,561][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:44,562][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 15:23:44,562][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 15:23:44,567][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 15:23:44,769][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 15:23:44,770][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 15:23:47,347][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 15:27:32,618][root][INFO] - 

[2024-10-21 15:27:32,618][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 15:27:32,618][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 15:27:32,618][root][INFO] - 

[2024-10-21 15:27:32,618][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 15:27:34,981][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 15:27:34,981][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 15:27:34,982][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 15:27:34,982][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 15:27:34,983][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 15:27:34,983][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 15:27:34,984][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 15:27:34,984][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 15:27:34,985][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 15:27:34,985][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 15:27:34,985][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 15:27:34,986][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 15:27:34,986][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 15:27:34,987][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 15:27:34,987][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 15:27:34,988][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 15:27:34,988][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 15:27:34,989][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 15:27:34,989][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 15:27:34,990][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 15:27:34,990][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 15:27:34,991][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 15:27:34,991][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 15:27:34,991][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 15:27:34,992][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 15:27:37,451][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 15:28:33,780][root][INFO] - 

[2024-10-21 15:28:33,781][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 15:28:33,781][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 15:28:33,781][root][INFO] - 

[2024-10-21 15:28:33,781][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 15:28:35,752][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:35,752][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:35,753][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:35,753][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:35,754][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:35,754][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:35,754][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:35,755][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:35,755][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:35,756][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:35,756][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:35,756][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:35,757][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:35,757][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:35,758][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:35,758][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:35,759][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:35,759][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:35,760][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:35,760][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:35,761][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:35,761][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:35,762][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:35,762][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:35,762][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 15:28:38,256][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 15:28:47,721][root][INFO] - 

[2024-10-21 15:28:47,721][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 15:28:47,722][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 15:28:47,722][root][INFO] - 

[2024-10-21 15:28:47,722][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 15:28:49,717][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:49,718][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:49,718][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:49,719][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:49,719][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:49,720][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:49,720][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:49,721][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:49,721][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:49,722][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:49,722][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:49,723][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:49,723][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:49,724][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:49,724][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:49,725][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:49,725][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:49,726][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:49,726][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:49,727][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:49,727][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:49,728][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:49,728][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 15:28:49,729][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 15:28:49,734][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 15:28:49,940][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 15:28:49,941][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 15:28:52,692][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 15:29:14,479][root][INFO] - 

[2024-10-21 15:29:14,479][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 15:29:14,479][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 15:29:14,479][root][INFO] - 

[2024-10-21 15:29:14,479][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 15:29:16,432][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:16,432][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:16,433][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:16,433][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:16,434][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:16,434][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:16,434][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:16,435][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:16,435][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:16,436][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:16,436][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:16,437][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:16,437][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:16,438][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:16,438][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:16,439][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:16,439][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:16,440][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:16,440][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:16,441][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:16,441][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:16,442][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:16,442][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:16,442][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:16,443][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 15:29:19,242][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 15:29:28,761][root][INFO] - 

[2024-10-21 15:29:28,761][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 15:29:28,761][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 15:29:28,761][root][INFO] - 

[2024-10-21 15:29:28,761][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 15:29:30,707][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:30,707][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:30,708][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:30,708][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:30,709][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:30,709][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:30,709][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:30,710][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:30,710][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:30,711][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:30,711][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:30,712][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:30,712][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:30,712][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:30,713][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:30,713][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:30,714][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:30,714][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:30,715][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:30,715][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:30,716][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:30,716][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:30,716][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 15:29:30,717][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 15:29:30,722][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 15:29:30,937][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 15:29:30,938][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 15:29:33,504][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 15:31:55,298][root][INFO] - 

[2024-10-21 15:31:55,298][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 15:31:55,298][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 15:31:55,298][root][INFO] - 

[2024-10-21 15:31:55,298][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 15:31:57,309][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 15:31:57,310][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 15:31:57,311][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 15:31:57,311][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 15:31:57,312][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 15:31:57,313][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 15:31:57,313][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 15:31:57,314][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 15:31:57,315][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 15:31:57,315][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 15:31:57,316][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 15:31:57,317][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 15:31:57,318][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 15:31:57,318][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 15:31:57,319][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 15:31:57,320][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 15:31:57,321][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 15:31:57,321][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 15:31:57,322][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 15:31:57,323][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 15:31:57,323][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 15:31:57,324][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 15:31:57,325][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 15:31:57,326][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 15:31:57,326][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 15:31:59,778][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 15:32:08,412][root][INFO] - 

[2024-10-21 15:32:08,413][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 15:32:08,413][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 15:32:08,413][root][INFO] - 

[2024-10-21 15:32:08,413][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 15:32:10,508][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 15:32:10,510][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 15:32:10,511][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 15:32:10,512][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 15:32:10,513][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 15:32:10,514][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 15:32:10,515][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 15:32:10,516][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 15:32:10,517][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 15:32:10,518][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 15:32:10,519][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 15:32:10,520][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 15:32:10,521][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 15:32:10,521][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 15:32:10,522][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 15:32:10,522][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 15:32:10,523][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 15:32:10,523][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 15:32:10,524][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 15:32:10,524][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 15:32:10,525][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 15:32:10,526][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 15:32:10,527][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 15:32:10,527][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 15:32:10,532][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 15:32:10,740][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 15:32:10,741][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 15:32:13,287][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 15:33:21,087][root][INFO] - 

[2024-10-21 15:33:21,087][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 15:33:21,087][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 15:33:21,087][root][INFO] - 

[2024-10-21 15:33:21,087][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 15:33:23,672][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:23,673][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:23,673][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:23,674][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:23,674][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:23,674][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:23,675][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:23,675][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:23,676][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:23,676][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:23,677][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:23,677][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:23,677][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:23,678][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:23,678][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:23,679][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:23,679][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:23,680][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:23,680][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:23,680][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:23,681][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:23,681][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:23,682][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:23,682][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:23,682][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 15:33:26,126][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 15:33:35,267][root][INFO] - 

[2024-10-21 15:33:35,267][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 15:33:35,267][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 15:33:35,267][root][INFO] - 

[2024-10-21 15:33:35,267][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 15:33:37,764][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:37,765][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:37,765][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:37,766][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:37,766][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:37,767][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:37,767][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:37,768][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:37,768][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:37,769][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:37,769][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:37,770][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:37,770][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:37,771][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:37,771][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:37,772][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:37,772][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:37,772][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:37,773][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:37,773][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:37,774][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:37,774][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:37,775][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 15:33:37,775][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 15:33:37,780][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 15:33:37,983][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 15:33:37,984][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 15:33:40,526][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 15:54:38,420][root][INFO] - 

[2024-10-21 15:54:38,421][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 15:54:38,421][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 15:54:38,421][root][INFO] - 

[2024-10-21 15:54:38,421][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 15:54:40,453][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:40,453][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:40,454][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:40,455][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:40,455][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:40,456][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:40,457][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:40,457][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:40,458][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:40,459][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:40,460][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:40,460][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:40,461][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:40,461][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:40,462][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:40,462][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:40,462][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:40,463][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:40,463][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:40,464][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:40,465][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:40,465][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:40,466][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:40,466][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:40,466][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 15:54:42,921][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 15:54:52,081][root][INFO] - 

[2024-10-21 15:54:52,081][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 15:54:52,081][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 15:54:52,081][root][INFO] - 

[2024-10-21 15:54:52,081][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 15:54:54,034][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:54,035][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:54,035][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:54,036][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:54,036][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:54,037][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:54,037][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:54,038][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:54,039][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:54,039][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:54,040][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:54,040][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:54,041][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:54,041][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:54,042][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:54,043][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:54,043][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:54,044][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:54,044][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:54,045][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:54,046][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:54,046][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:54,047][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 15:54:54,047][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 15:54:54,053][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 15:54:54,258][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 15:54:54,259][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 15:54:57,112][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 15:58:24,624][root][INFO] - 

[2024-10-21 15:58:24,624][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 15:58:24,624][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 15:58:24,624][root][INFO] - 

[2024-10-21 15:58:24,624][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 15:58:26,618][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:26,618][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:26,619][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:26,619][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:26,620][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:26,620][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:26,621][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:26,621][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:26,622][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:26,622][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:26,623][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:26,623][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:26,624][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:26,624][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:26,625][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:26,625][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:26,626][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:26,626][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:26,627][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:26,627][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:26,628][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:26,628][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:26,629][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:26,629][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:26,630][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 15:58:29,107][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 15:58:38,606][root][INFO] - 

[2024-10-21 15:58:38,606][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 15:58:38,606][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 15:58:38,606][root][INFO] - 

[2024-10-21 15:58:38,607][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 15:58:40,508][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:40,509][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:40,509][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:40,510][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:40,511][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:40,511][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:40,512][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:40,513][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:40,514][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:40,514][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:40,515][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:40,515][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:40,516][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:40,517][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:40,518][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:40,518][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:40,519][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:40,520][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:40,520][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:40,521][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:40,522][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:40,523][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:40,523][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 15:58:40,524][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 15:58:40,531][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 15:58:40,766][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 15:58:40,767][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 15:58:43,526][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 15:59:09,069][root][INFO] - 

[2024-10-21 15:59:09,069][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 15:59:09,069][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 15:59:09,069][root][INFO] - 

[2024-10-21 15:59:09,069][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 15:59:11,116][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:11,116][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:11,117][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:11,117][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:11,118][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:11,118][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:11,119][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:11,119][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:11,119][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:11,120][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:11,120][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:11,121][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:11,121][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:11,121][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:11,122][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:11,122][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:11,123][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:11,123][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:11,124][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:11,124][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:11,125][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:11,125][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:11,125][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:11,126][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:11,126][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 15:59:13,569][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 15:59:22,478][root][INFO] - 

[2024-10-21 15:59:22,479][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 15:59:22,479][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 15:59:22,479][root][INFO] - 

[2024-10-21 15:59:22,479][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 15:59:24,437][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:24,438][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:24,439][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:24,439][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:24,440][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:24,441][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:24,441][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:24,442][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:24,443][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:24,443][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:24,444][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:24,445][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:24,446][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:24,446][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:24,447][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:24,447][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:24,448][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:24,449][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:24,449][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:24,450][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:24,451][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:24,452][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:24,452][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 15:59:24,453][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 15:59:24,461][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 15:59:24,690][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 15:59:24,691][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 15:59:27,225][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 16:01:54,548][root][INFO] - 

[2024-10-21 16:01:54,548][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 16:01:54,548][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 16:01:54,548][root][INFO] - 

[2024-10-21 16:01:54,549][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 16:01:57,010][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 16:01:57,010][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 16:01:57,010][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 16:01:57,011][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 16:01:57,011][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 16:01:57,012][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 16:01:57,012][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 16:01:57,012][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 16:01:57,013][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 16:01:57,013][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 16:01:57,014][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 16:01:57,014][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 16:01:57,014][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 16:01:57,015][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 16:01:57,015][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 16:01:57,016][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 16:01:57,016][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 16:01:57,017][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 16:01:57,017][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 16:01:57,017][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 16:01:57,018][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 16:01:57,018][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 16:01:57,019][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 16:01:57,019][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 16:01:57,019][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 16:01:59,451][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 16:02:08,411][root][INFO] - 

[2024-10-21 16:02:08,411][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 16:02:08,411][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 16:02:08,411][root][INFO] - 

[2024-10-21 16:02:08,411][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 16:02:10,388][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 16:02:10,389][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 16:02:10,389][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 16:02:10,390][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 16:02:10,391][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 16:02:10,392][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 16:02:10,392][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 16:02:10,393][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 16:02:10,394][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 16:02:10,394][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 16:02:10,395][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 16:02:10,396][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 16:02:10,397][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 16:02:10,397][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 16:02:10,398][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 16:02:10,399][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 16:02:10,399][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 16:02:10,400][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 16:02:10,400][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 16:02:10,401][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 16:02:10,401][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 16:02:10,402][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 16:02:10,402][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 16:02:10,403][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 16:02:10,408][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 16:02:10,613][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 16:02:10,614][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 16:02:13,559][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 17:43:30,451][root][INFO] - 

[2024-10-21 17:43:30,451][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 17:43:30,451][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 17:43:30,451][root][INFO] - 

[2024-10-21 17:43:30,451][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 17:43:32,381][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 17:43:32,382][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 17:43:32,382][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 17:43:32,383][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 17:43:32,383][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 17:43:32,384][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 17:43:32,384][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 17:43:32,384][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 17:43:32,385][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 17:43:32,385][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 17:43:32,386][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 17:43:32,386][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 17:43:32,387][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 17:43:32,387][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 17:43:32,388][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 17:43:32,388][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 17:43:32,389][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 17:43:32,389][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 17:43:32,390][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 17:43:32,390][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 17:43:32,391][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 17:43:32,391][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 17:43:32,391][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 17:43:32,392][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 17:43:32,397][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 17:43:32,600][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 17:43:32,601][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 17:43:35,204][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 17:44:10,921][root][INFO] - 

[2024-10-21 17:44:10,921][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 17:44:10,922][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 17:44:10,922][root][INFO] - 

[2024-10-21 17:44:10,922][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 17:44:12,929][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 17:44:12,930][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 17:44:12,930][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 17:44:12,931][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 17:44:12,931][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 17:44:12,932][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 17:44:12,932][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 17:44:12,933][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 17:44:12,933][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 17:44:12,934][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 17:44:12,934][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 17:44:12,935][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 17:44:12,936][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 17:44:12,936][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 17:44:12,937][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 17:44:12,937][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 17:44:12,938][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 17:44:12,938][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 17:44:12,939][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 17:44:12,940][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 17:44:12,940][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 17:44:12,941][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 17:44:12,941][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 17:44:12,942][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 17:44:12,948][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 17:44:13,152][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 17:44:13,153][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 17:44:15,712][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 17:45:00,298][root][INFO] - 

[2024-10-21 17:45:00,299][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 17:45:00,299][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 17:45:00,299][root][INFO] - 

[2024-10-21 17:45:00,299][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 17:45:02,615][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:02,616][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:02,616][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:02,617][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:02,617][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:02,618][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:02,618][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:02,618][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:02,619][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:02,619][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:02,620][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:02,620][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:02,620][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:02,621][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:02,621][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:02,622][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:02,622][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:02,622][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:02,623][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:02,623][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:02,624][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:02,624][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:02,625][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:02,625][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:02,631][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 17:45:02,838][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 17:45:02,838][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 17:45:05,396][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 17:45:29,887][root][INFO] - 

[2024-10-21 17:45:29,888][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 17:45:29,888][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 17:45:29,888][root][INFO] - 

[2024-10-21 17:45:29,888][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 17:45:31,772][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:31,773][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:31,773][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:31,774][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:31,774][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:31,775][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:31,775][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:31,776][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:31,776][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:31,776][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:31,777][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:31,777][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:31,778][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:31,778][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:31,779][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:31,779][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:31,780][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:31,780][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:31,780][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:31,781][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:31,781][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:31,782][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:31,782][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 17:45:31,783][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 17:45:31,788][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 17:45:31,988][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 17:45:31,989][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 17:45:34,549][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 17:46:01,571][root][INFO] - 

[2024-10-21 17:46:01,571][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 17:46:01,571][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 17:46:01,571][root][INFO] - 

[2024-10-21 17:46:01,571][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 17:46:03,657][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 17:46:03,657][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 17:46:03,658][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 17:46:03,658][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 17:46:03,659][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 17:46:03,659][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 17:46:03,660][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 17:46:03,660][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 17:46:03,661][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 17:46:03,661][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 17:46:03,662][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 17:46:03,662][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 17:46:03,663][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 17:46:03,663][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 17:46:03,664][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 17:46:03,664][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 17:46:03,665][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 17:46:03,665][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 17:46:03,666][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 17:46:03,666][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 17:46:03,667][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 17:46:03,667][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 17:46:03,668][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 17:46:03,668][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 17:46:03,674][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 17:46:03,878][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 17:46:03,879][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 17:46:06,435][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 17:47:26,005][root][INFO] - 

[2024-10-21 17:47:26,005][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 17:47:26,005][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 17:47:26,005][root][INFO] - 

[2024-10-21 17:47:26,006][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 17:47:28,139][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 17:47:28,139][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 17:47:28,140][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 17:47:28,140][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 17:47:28,141][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 17:47:28,141][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 17:47:28,142][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 17:47:28,142][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 17:47:28,142][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 17:47:28,143][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 17:47:28,144][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 17:47:28,144][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 17:47:28,145][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 17:47:28,145][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 17:47:28,145][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 17:47:28,146][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 17:47:28,146][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 17:47:28,147][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 17:47:28,147][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 17:47:28,148][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 17:47:28,148][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 17:47:28,148][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 17:47:28,149][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 17:47:28,149][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 17:47:28,153][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 17:47:28,364][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 17:47:28,364][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 17:47:30,898][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 17:53:49,485][root][INFO] - 

[2024-10-21 17:53:49,485][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 17:53:49,485][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 17:53:49,485][root][INFO] - 

[2024-10-21 17:53:49,486][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 17:53:51,635][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 17:53:51,636][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 17:53:51,636][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 17:53:51,637][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 17:53:51,637][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 17:53:51,638][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 17:53:51,638][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 17:53:51,638][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 17:53:51,639][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 17:53:51,639][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 17:53:51,640][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 17:53:51,640][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 17:53:51,640][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 17:53:51,641][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 17:53:51,641][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 17:53:51,642][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 17:53:51,642][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 17:53:51,642][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 17:53:51,643][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 17:53:51,643][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 17:53:51,644][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 17:53:51,644][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 17:53:51,645][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 17:53:51,645][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 17:53:51,650][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 17:53:51,852][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 17:53:51,853][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 17:53:54,367][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 17:54:42,614][root][INFO] - 

[2024-10-21 17:54:42,614][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 17:54:42,614][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 17:54:42,615][root][INFO] - 

[2024-10-21 17:54:42,615][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 17:54:44,600][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 17:54:44,601][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 17:54:44,601][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 17:54:44,602][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 17:54:44,602][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 17:54:44,602][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 17:54:44,603][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 17:54:44,603][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 17:54:44,604][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 17:54:44,604][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 17:54:44,605][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 17:54:44,605][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 17:54:44,606][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 17:54:44,606][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 17:54:44,607][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 17:54:44,607][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 17:54:44,608][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 17:54:44,608][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 17:54:44,609][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 17:54:44,609][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 17:54:44,610][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 17:54:44,610][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 17:54:44,611][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 17:54:44,611][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 17:54:44,616][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 17:54:44,829][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 17:54:44,830][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 17:54:47,345][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 17:57:30,756][root][INFO] - 

[2024-10-21 17:57:30,756][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 17:57:30,756][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 17:57:30,756][root][INFO] - 

[2024-10-21 17:57:30,756][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 17:57:33,366][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 17:57:33,367][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 17:57:33,367][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 17:57:33,368][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 17:57:33,368][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 17:57:33,369][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 17:57:33,369][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 17:57:33,370][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 17:57:33,371][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 17:57:33,371][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 17:57:33,372][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 17:57:33,372][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 17:57:33,373][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 17:57:33,373][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 17:57:33,374][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 17:57:33,374][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 17:57:33,375][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 17:57:33,376][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 17:57:33,376][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 17:57:33,377][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 17:57:33,377][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 17:57:33,378][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 17:57:33,378][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 17:57:33,379][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 17:57:33,385][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 17:57:33,590][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 17:57:33,591][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 17:57:36,317][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 17:58:35,615][root][INFO] - 

[2024-10-21 17:58:35,615][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 17:58:35,615][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 17:58:35,615][root][INFO] - 

[2024-10-21 17:58:35,615][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 17:58:37,686][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 17:58:37,686][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 17:58:37,687][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 17:58:37,688][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 17:58:37,688][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 17:58:37,689][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 17:58:37,690][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 17:58:37,690][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 17:58:37,691][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 17:58:37,691][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 17:58:37,692][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 17:58:37,693][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 17:58:37,693][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 17:58:37,693][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 17:58:37,694][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 17:58:37,694][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 17:58:37,695][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 17:58:37,696][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 17:58:37,696][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 17:58:37,697][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 17:58:37,698][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 17:58:37,699][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 17:58:37,700][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 17:58:37,701][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 17:58:37,705][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 17:58:37,908][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 17:58:37,909][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 17:58:40,765][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 17:58:59,936][root][INFO] - 

[2024-10-21 17:58:59,936][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 17:58:59,936][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 17:58:59,936][root][INFO] - 

[2024-10-21 17:58:59,937][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 17:59:01,823][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:01,823][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:01,824][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:01,824][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:01,825][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:01,825][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:01,825][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:01,826][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:01,826][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:01,827][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:01,827][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:01,827][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:01,828][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:01,828][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:01,829][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:01,829][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:01,830][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:01,830][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:01,830][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:01,831][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:01,831][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:01,832][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:01,832][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:01,832][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:01,839][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 17:59:02,041][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 17:59:02,042][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 17:59:04,566][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 17:59:35,758][root][INFO] - 

[2024-10-21 17:59:35,758][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 17:59:35,758][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 17:59:35,758][root][INFO] - 

[2024-10-21 17:59:35,758][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 17:59:37,719][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:37,724][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:37,725][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:37,725][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:37,726][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:37,726][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:37,727][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:37,727][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:37,728][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:37,728][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:37,729][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:37,729][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:37,730][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:37,730][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:37,731][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:37,732][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:37,732][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:37,732][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:37,733][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:37,734][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:37,734][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:37,735][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:37,735][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 17:59:37,736][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 17:59:37,741][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 17:59:37,950][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 17:59:37,951][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 17:59:40,544][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 18:01:26,396][root][INFO] - 

[2024-10-21 18:01:26,396][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 18:01:26,396][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 18:01:26,396][root][INFO] - 

[2024-10-21 18:01:26,396][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 18:01:28,408][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 18:01:28,409][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 18:01:28,409][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 18:01:28,410][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 18:01:28,411][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 18:01:28,412][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 18:01:28,413][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 18:01:28,414][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 18:01:28,416][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 18:01:28,417][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 18:01:28,417][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 18:01:28,418][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 18:01:28,419][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 18:01:28,420][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 18:01:28,420][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 18:01:28,421][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 18:01:28,421][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 18:01:28,422][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 18:01:28,422][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 18:01:28,423][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 18:01:28,424][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 18:01:28,425][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 18:01:28,426][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 18:01:28,426][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 18:01:28,432][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 18:01:28,637][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 18:01:28,638][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 18:01:31,180][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 18:02:35,668][root][INFO] - 

[2024-10-21 18:02:35,668][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 18:02:35,668][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 18:02:35,668][root][INFO] - 

[2024-10-21 18:02:35,668][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 18:02:37,708][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 18:02:37,708][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 18:02:37,709][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 18:02:37,709][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 18:02:37,710][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 18:02:37,710][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 18:02:37,711][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 18:02:37,711][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 18:02:37,712][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 18:02:37,712][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 18:02:37,713][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 18:02:37,713][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 18:02:37,714][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 18:02:37,714][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 18:02:37,715][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 18:02:37,715][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 18:02:37,716][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 18:02:37,716][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 18:02:37,717][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 18:02:37,717][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 18:02:37,718][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 18:02:37,718][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 18:02:37,719][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 18:02:37,719][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 18:02:37,724][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 18:02:37,929][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 18:02:37,930][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 18:02:40,573][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 18:03:22,166][root][INFO] - 

[2024-10-21 18:03:22,166][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 18:03:22,167][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 18:03:22,167][root][INFO] - 

[2024-10-21 18:03:22,167][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 18:03:24,181][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 18:03:24,181][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 18:03:24,182][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 18:03:24,182][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 18:03:24,183][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 18:03:24,183][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 18:03:24,184][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 18:03:24,184][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 18:03:24,184][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 18:03:24,185][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 18:03:24,185][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 18:03:24,186][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 18:03:24,186][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 18:03:24,187][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 18:03:24,187][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 18:03:24,188][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 18:03:24,188][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 18:03:24,189][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 18:03:24,189][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 18:03:24,189][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 18:03:24,190][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 18:03:24,190][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 18:03:24,191][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 18:03:24,191][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 18:03:24,197][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 18:03:24,419][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 18:03:24,420][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 18:03:27,162][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 18:04:18,318][root][INFO] - 

[2024-10-21 18:04:18,318][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 18:04:18,318][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 18:04:18,318][root][INFO] - 

[2024-10-21 18:04:18,318][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 18:04:20,295][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 18:04:20,296][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 18:04:20,297][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 18:04:20,297][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 18:04:20,298][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 18:04:20,298][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 18:04:20,299][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 18:04:20,299][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 18:04:20,300][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 18:04:20,300][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 18:04:20,301][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 18:04:20,301][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 18:04:20,302][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 18:04:20,302][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 18:04:20,303][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 18:04:20,303][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 18:04:20,304][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 18:04:20,304][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 18:04:20,305][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 18:04:20,305][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 18:04:20,306][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 18:04:20,306][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 18:04:20,307][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 18:04:20,307][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 18:04:20,315][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 18:04:20,519][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 18:04:20,520][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 18:04:23,069][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 18:04:58,394][root][INFO] - 

[2024-10-21 18:04:58,394][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 18:04:58,394][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 18:04:58,394][root][INFO] - 

[2024-10-21 18:04:58,395][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 18:05:00,355][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:00,355][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:00,356][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:00,356][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:00,357][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:00,357][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:00,357][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:00,358][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:00,358][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:00,359][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:00,359][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:00,359][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:00,360][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:00,360][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:00,361][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:00,361][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:00,362][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:00,362][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:00,363][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:00,363][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:00,363][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:00,364][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:00,364][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:00,365][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:00,370][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 18:05:00,573][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 18:05:00,574][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 18:05:03,124][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 18:05:38,338][root][INFO] - 

[2024-10-21 18:05:38,338][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 18:05:38,338][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 18:05:38,338][root][INFO] - 

[2024-10-21 18:05:38,338][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 18:05:40,291][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:40,292][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:40,293][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:40,294][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:40,295][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:40,295][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:40,296][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:40,297][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:40,298][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:40,299][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:40,300][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:40,301][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:40,301][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:40,302][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:40,303][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:40,303][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:40,304][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:40,304][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:40,305][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:40,305][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:40,305][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:40,306][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:40,306][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 18:05:40,307][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 18:05:40,311][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 18:05:40,514][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 18:05:40,515][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 18:05:42,949][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 18:07:44,774][root][INFO] - 

[2024-10-21 18:07:44,774][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 18:07:44,774][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 18:07:44,774][root][INFO] - 

[2024-10-21 18:07:44,775][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 18:07:46,694][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 18:07:46,695][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 18:07:46,695][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 18:07:46,696][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 18:07:46,696][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 18:07:46,697][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 18:07:46,697][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 18:07:46,697][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 18:07:46,698][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 18:07:46,698][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 18:07:46,699][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 18:07:46,699][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 18:07:46,700][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 18:07:46,700][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 18:07:46,701][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 18:07:46,701][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 18:07:46,702][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 18:07:46,702][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 18:07:46,702][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 18:07:46,703][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 18:07:46,703][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 18:07:46,704][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 18:07:46,704][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 18:07:46,705][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 18:07:46,709][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 18:07:46,924][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 18:07:46,925][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 18:07:49,490][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 18:08:27,379][root][INFO] - 

[2024-10-21 18:08:27,379][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 18:08:27,379][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 18:08:27,379][root][INFO] - 

[2024-10-21 18:08:27,379][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 18:08:29,315][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:29,316][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:29,317][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:29,317][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:29,318][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:29,318][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:29,319][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:29,319][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:29,320][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:29,320][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:29,321][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:29,322][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:29,322][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:29,323][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:29,323][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:29,324][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:29,325][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:29,325][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:29,326][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:29,326][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:29,327][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:29,328][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:29,328][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:29,329][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:29,335][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 18:08:29,542][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 18:08:29,543][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 18:08:32,112][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 18:08:47,193][root][INFO] - 

[2024-10-21 18:08:47,193][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 18:08:47,193][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 18:08:47,193][root][INFO] - 

[2024-10-21 18:08:47,194][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 18:08:50,386][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:50,387][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:50,387][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:50,388][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:50,388][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:50,389][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:50,389][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:50,390][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:50,390][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:50,390][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:50,391][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:50,391][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:50,392][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:50,392][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:50,393][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:50,393][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:50,394][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:50,394][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:50,394][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:50,395][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:50,395][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:50,396][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:50,396][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 18:08:50,397][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 18:08:50,401][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 18:08:50,604][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 18:08:50,605][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 18:08:53,153][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 18:10:26,206][root][INFO] - 

[2024-10-21 18:10:26,206][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 18:10:26,206][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 18:10:26,206][root][INFO] - 

[2024-10-21 18:10:26,207][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 18:10:28,151][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 18:10:28,152][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 18:10:28,152][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 18:10:28,152][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 18:10:28,153][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 18:10:28,153][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 18:10:28,154][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 18:10:28,154][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 18:10:28,155][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 18:10:28,155][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 18:10:28,156][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 18:10:28,156][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 18:10:28,157][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 18:10:28,157][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 18:10:28,158][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 18:10:28,158][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 18:10:28,159][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 18:10:28,159][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 18:10:28,159][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 18:10:28,160][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 18:10:28,160][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 18:10:28,161][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 18:10:28,161][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 18:10:28,162][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 18:10:28,168][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 18:10:28,374][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 18:10:28,375][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 18:10:30,930][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 18:12:35,430][root][INFO] - 

[2024-10-21 18:12:35,430][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 18:12:35,430][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 18:12:35,430][root][INFO] - 

[2024-10-21 18:12:35,430][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 18:12:37,332][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 18:12:37,333][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 18:12:37,334][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 18:12:37,334][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 18:12:37,334][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 18:12:37,335][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 18:12:37,335][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 18:12:37,336][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 18:12:37,336][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 18:12:37,337][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 18:12:37,337][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 18:12:37,337][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 18:12:37,338][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 18:12:37,338][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 18:12:37,339][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 18:12:37,339][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 18:12:37,340][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 18:12:37,340][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 18:12:37,341][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 18:12:37,341][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 18:12:37,341][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 18:12:37,342][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 18:12:37,342][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 18:12:37,343][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 18:12:37,347][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 18:12:37,565][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 18:12:37,565][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 18:12:40,233][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 20:30:04,330][root][INFO] - 

[2024-10-21 20:30:04,330][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 20:30:04,330][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 20:30:04,330][root][INFO] - 

[2024-10-21 20:30:04,330][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 20:30:06,699][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 20:30:06,700][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 20:30:06,700][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 20:30:06,701][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 20:30:06,701][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 20:30:06,701][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 20:30:06,702][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 20:30:06,702][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 20:30:06,703][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 20:30:06,703][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 20:30:06,704][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 20:30:06,704][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 20:30:06,705][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 20:30:06,705][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 20:30:06,706][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 20:30:06,706][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 20:30:06,707][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 20:30:06,707][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 20:30:06,707][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 20:30:06,708][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 20:30:06,708][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 20:30:06,709][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 20:30:06,709][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 20:30:06,710][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 20:30:06,714][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 20:30:06,925][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 20:30:06,926][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 20:30:09,502][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 20:30:59,022][root][INFO] - 

[2024-10-21 20:30:59,022][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 20:30:59,022][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 20:30:59,022][root][INFO] - 

[2024-10-21 20:30:59,022][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 20:31:01,064][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:01,065][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:01,066][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:01,066][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:01,066][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:01,067][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:01,067][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:01,068][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:01,068][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:01,069][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:01,069][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:01,070][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:01,070][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:01,071][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:01,071][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:01,072][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:01,072][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:01,073][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:01,073][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:01,074][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:01,074][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:01,075][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:01,075][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:01,076][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:01,083][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 20:31:01,293][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 20:31:01,294][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 20:31:03,901][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 20:31:51,048][root][INFO] - 

[2024-10-21 20:31:51,048][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 20:31:51,048][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 20:31:51,048][root][INFO] - 

[2024-10-21 20:31:51,048][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 20:31:53,396][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:53,397][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:53,398][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:53,399][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:53,399][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:53,400][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:53,401][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:53,401][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:53,402][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:53,403][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:53,403][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:53,404][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:53,405][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:53,405][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:53,406][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:53,407][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:53,408][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:53,408][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:53,409][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:53,410][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:53,411][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:53,411][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:53,412][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 20:31:53,413][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 20:31:53,421][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 20:31:53,679][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 20:31:53,680][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 20:31:56,396][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 20:32:09,579][root][INFO] - 

[2024-10-21 20:32:09,579][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 20:32:09,579][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 20:32:09,579][root][INFO] - 

[2024-10-21 20:32:09,579][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 20:32:11,481][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 20:32:11,482][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 20:32:11,482][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 20:32:11,483][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 20:32:11,483][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 20:32:11,484][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 20:32:11,484][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 20:32:11,484][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 20:32:11,485][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 20:32:11,485][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 20:32:11,486][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 20:32:11,486][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 20:32:11,487][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 20:32:11,487][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 20:32:11,487][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 20:32:11,488][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 20:32:11,488][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 20:32:11,489][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 20:32:11,489][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 20:32:11,489][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 20:32:11,490][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 20:32:11,490][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 20:32:11,491][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 20:32:11,496][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 20:32:11,500][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 20:32:11,703][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 20:32:11,703][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 20:32:14,520][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 20:33:42,742][root][INFO] - 

[2024-10-21 20:33:42,742][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 20:33:42,742][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 20:33:42,743][root][INFO] - 

[2024-10-21 20:33:42,743][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 20:33:44,899][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:44,899][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:44,900][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:44,900][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:44,901][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:44,901][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:44,902][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:44,902][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:44,902][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:44,903][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:44,903][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:44,904][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:44,904][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:44,905][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:44,905][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:44,906][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:44,906][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:44,906][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:44,907][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:44,907][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:44,908][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:44,908][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:44,909][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:44,909][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:44,913][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 20:33:45,116][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 20:33:45,116][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 20:33:47,657][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 20:33:55,267][root][INFO] - 

[2024-10-21 20:33:55,267][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 20:33:55,267][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 20:33:55,267][root][INFO] - 

[2024-10-21 20:33:55,268][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 20:33:57,195][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:57,195][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:57,196][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:57,196][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:57,197][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:57,197][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:57,198][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:57,198][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:57,198][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:57,199][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:57,199][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:57,200][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:57,200][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:57,200][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:57,201][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:57,201][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:57,202][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:57,202][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:57,203][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:57,203][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:57,204][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:57,204][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:57,205][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 20:33:57,205][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 20:33:57,209][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 20:33:57,423][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 20:33:57,424][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 20:33:59,961][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 20:36:04,331][root][INFO] - 

[2024-10-21 20:36:04,331][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 20:36:04,331][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 20:36:04,331][root][INFO] - 

[2024-10-21 20:36:04,331][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 20:36:06,482][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 20:36:06,482][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 20:36:06,483][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 20:36:06,483][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 20:36:06,484][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 20:36:06,484][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 20:36:06,484][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 20:36:06,485][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 20:36:06,485][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 20:36:06,486][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 20:36:06,486][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 20:36:06,486][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 20:36:06,487][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 20:36:06,487][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 20:36:06,488][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 20:36:06,488][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 20:36:06,489][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 20:36:06,489][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 20:36:06,489][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 20:36:06,490][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 20:36:06,490][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 20:36:06,491][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 20:36:06,491][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 20:36:06,492][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 20:36:06,495][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 20:36:06,702][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 20:36:06,703][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 20:36:09,223][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 20:42:23,544][root][INFO] - 

[2024-10-21 20:42:23,544][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 20:42:23,544][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 20:42:23,544][root][INFO] - 

[2024-10-21 20:42:23,544][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 20:42:25,517][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:25,518][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:25,519][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:25,520][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:25,521][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:25,521][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:25,522][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:25,523][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:25,523][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:25,524][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:25,524][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:25,525][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:25,526][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:25,526][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:25,527][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:25,527][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:25,528][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:25,528][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:25,529][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:25,529][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:25,530][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:25,531][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:25,531][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:25,532][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:25,537][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 20:42:25,749][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 20:42:25,750][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 20:42:28,348][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 20:42:46,415][root][INFO] - 

[2024-10-21 20:42:46,415][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 20:42:46,415][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 20:42:46,415][root][INFO] - 

[2024-10-21 20:42:46,415][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 20:42:48,417][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:48,418][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:48,418][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:48,418][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:48,419][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:48,419][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:48,420][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:48,420][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:48,420][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:48,421][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:48,421][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:48,422][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:48,422][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:48,423][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:48,423][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:48,423][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:48,424][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:48,424][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:48,425][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:48,425][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:48,426][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:48,426][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:48,426][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 20:42:48,427][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 20:42:48,432][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 20:42:48,634][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 20:42:48,635][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 20:42:51,120][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 20:43:34,358][root][INFO] - 

[2024-10-21 20:43:34,358][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 20:43:34,358][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 20:43:34,358][root][INFO] - 

[2024-10-21 20:43:34,359][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 20:43:36,248][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 20:43:36,248][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 20:43:36,249][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 20:43:36,249][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 20:43:36,250][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 20:43:36,250][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 20:43:36,250][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 20:43:36,251][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 20:43:36,251][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 20:43:36,252][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 20:43:36,252][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 20:43:36,253][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 20:43:36,253][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 20:43:36,254][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 20:43:36,254][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 20:43:36,255][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 20:43:36,255][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 20:43:36,255][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 20:43:36,256][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 20:43:36,256][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 20:43:36,257][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 20:43:36,257][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 20:43:36,258][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 20:43:36,258][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 20:43:36,262][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 20:43:36,469][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 20:43:36,470][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 20:43:38,974][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 20:44:19,161][root][INFO] - 

[2024-10-21 20:44:19,161][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 20:44:19,161][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 20:44:19,161][root][INFO] - 

[2024-10-21 20:44:19,161][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 20:44:21,086][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 20:44:21,086][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 20:44:21,087][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 20:44:21,087][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 20:44:21,088][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 20:44:21,088][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 20:44:21,089][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 20:44:21,089][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 20:44:21,089][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 20:44:21,090][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 20:44:21,090][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 20:44:21,091][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 20:44:21,091][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 20:44:21,091][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 20:44:21,092][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 20:44:21,092][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 20:44:21,093][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 20:44:21,093][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 20:44:21,094][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 20:44:21,094][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 20:44:21,095][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 20:44:21,095][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 20:44:21,096][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 20:44:21,096][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 20:44:21,102][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 20:44:21,303][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 20:44:21,303][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 20:44:23,818][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 20:45:30,597][root][INFO] - 

[2024-10-21 20:45:30,597][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 20:45:30,597][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 20:45:30,597][root][INFO] - 

[2024-10-21 20:45:30,597][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 20:45:32,534][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 20:45:32,534][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 20:45:32,535][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 20:45:32,535][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 20:45:32,536][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 20:45:32,536][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 20:45:32,536][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 20:45:32,537][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 20:45:32,537][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 20:45:32,538][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 20:45:32,538][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 20:45:32,539][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 20:45:32,539][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 20:45:32,540][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 20:45:32,540][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 20:45:32,541][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 20:45:32,541][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 20:45:32,542][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 20:45:32,542][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 20:45:32,542][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 20:45:32,543][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 20:45:32,543][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 20:45:32,544][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 20:45:32,544][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 20:45:32,549][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 20:45:32,750][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 20:45:32,751][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 20:45:35,519][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 20:48:37,763][root][INFO] - 

[2024-10-21 20:48:37,763][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 20:48:37,763][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 20:48:37,763][root][INFO] - 

[2024-10-21 20:48:37,763][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 20:48:39,701][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 20:48:39,701][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 20:48:39,702][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 20:48:39,702][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 20:48:39,703][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 20:48:39,703][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 20:48:39,703][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 20:48:39,704][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 20:48:39,704][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 20:48:39,705][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 20:48:39,705][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 20:48:39,706][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 20:48:39,706][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 20:48:39,707][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 20:48:39,707][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 20:48:39,707][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 20:48:39,708][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 20:48:39,708][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 20:48:39,709][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 20:48:39,709][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 20:48:39,710][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 20:48:39,710][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 20:48:39,711][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 20:48:39,711][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 20:48:39,715][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 20:48:39,919][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 20:48:39,920][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 20:48:42,438][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 20:56:06,195][root][INFO] - 

[2024-10-21 20:56:06,196][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 20:56:06,196][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 20:56:06,196][root][INFO] - 

[2024-10-21 20:56:06,196][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 20:56:08,154][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 20:56:08,155][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 20:56:08,155][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 20:56:08,156][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 20:56:08,157][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 20:56:08,157][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 20:56:08,158][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 20:56:08,158][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 20:56:08,159][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 20:56:08,159][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 20:56:08,160][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 20:56:08,160][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 20:56:08,161][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 20:56:08,161][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 20:56:08,162][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 20:56:08,162][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 20:56:08,163][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 20:56:08,163][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 20:56:08,164][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 20:56:08,164][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 20:56:08,165][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 20:56:08,165][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 20:56:08,166][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 20:56:08,166][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 20:56:08,171][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 20:56:08,374][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 20:56:08,375][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 20:56:10,909][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 20:59:15,412][root][INFO] - 

[2024-10-21 20:59:15,412][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 20:59:15,413][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 20:59:15,413][root][INFO] - 

[2024-10-21 20:59:15,413][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 20:59:17,458][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 20:59:17,458][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 20:59:17,459][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 20:59:17,459][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 20:59:17,459][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 20:59:17,460][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 20:59:17,460][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 20:59:17,461][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 20:59:17,461][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 20:59:17,461][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 20:59:17,462][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 20:59:17,462][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 20:59:17,463][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 20:59:17,463][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 20:59:17,464][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 20:59:17,464][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 20:59:17,465][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 20:59:17,465][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 20:59:17,465][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 20:59:17,466][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 20:59:17,466][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 20:59:17,467][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 20:59:17,467][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 20:59:17,468][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 20:59:17,472][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 20:59:17,687][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 20:59:17,689][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 20:59:20,231][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:01:20,181][root][INFO] - 

[2024-10-21 21:01:20,181][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:01:20,181][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:01:20,181][root][INFO] - 

[2024-10-21 21:01:20,181][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:01:22,431][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:01:22,432][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:01:22,432][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:01:22,432][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:01:22,433][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:01:22,433][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:01:22,434][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:01:22,434][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:01:22,435][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:01:22,435][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:01:22,436][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:01:22,436][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:01:22,437][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:01:22,437][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:01:22,438][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:01:22,438][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:01:22,439][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:01:22,439][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:01:22,440][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:01:22,440][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:01:22,441][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:01:22,441][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:01:22,442][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:01:22,442][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:01:22,449][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:01:22,652][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:01:22,653][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:01:25,160][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:03:24,637][root][INFO] - 

[2024-10-21 21:03:24,637][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:03:24,637][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:03:24,637][root][INFO] - 

[2024-10-21 21:03:24,637][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:03:26,707][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:03:26,707][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:03:26,708][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:03:26,708][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:03:26,709][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:03:26,709][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:03:26,709][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:03:26,710][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:03:26,710][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:03:26,711][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:03:26,711][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:03:26,712][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:03:26,712][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:03:26,713][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:03:26,713][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:03:26,713][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:03:26,714][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:03:26,714][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:03:26,715][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:03:26,715][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:03:26,716][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:03:26,716][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:03:26,717][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:03:26,717][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:03:26,722][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:03:26,924][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:03:26,925][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:03:29,627][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:05:14,837][root][INFO] - 

[2024-10-21 21:05:14,837][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:05:14,837][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:05:14,837][root][INFO] - 

[2024-10-21 21:05:14,837][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:05:16,763][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:16,763][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:16,765][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:16,765][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:16,767][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:16,768][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:16,769][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:16,770][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:16,771][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:16,772][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:16,773][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:16,774][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:16,774][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:16,775][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:16,776][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:16,776][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:16,776][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:16,777][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:16,777][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:16,778][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:16,778][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:16,779][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:16,779][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:16,780][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:16,784][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:05:16,990][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:05:16,991][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:05:19,528][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:05:52,919][root][INFO] - 

[2024-10-21 21:05:52,919][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:05:52,919][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:05:52,919][root][INFO] - 

[2024-10-21 21:05:52,919][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:05:54,840][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:54,841][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:54,841][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:54,842][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:54,842][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:54,843][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:54,843][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:54,843][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:54,844][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:54,844][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:54,845][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:54,845][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:54,846][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:54,846][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:54,847][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:54,847][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:54,847][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:54,848][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:54,848][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:54,849][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:54,849][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:54,849][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:54,850][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:05:54,850][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:05:54,855][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:05:55,057][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:05:55,058][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:05:57,579][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:07:13,016][root][INFO] - 

[2024-10-21 21:07:13,016][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:07:13,016][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:07:13,016][root][INFO] - 

[2024-10-21 21:07:13,016][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:07:14,999][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:07:15,000][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:07:15,001][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:07:15,002][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:07:15,003][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:07:15,004][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:07:15,005][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:07:15,006][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:07:15,007][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:07:15,008][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:07:15,009][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:07:15,010][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:07:15,010][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:07:15,011][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:07:15,012][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:07:15,013][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:07:15,014][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:07:15,014][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:07:15,015][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:07:15,016][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:07:15,018][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:07:15,018][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:07:15,019][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:07:15,020][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:07:15,024][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:07:15,229][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:07:15,229][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:07:17,793][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:08:11,932][root][INFO] - 

[2024-10-21 21:08:11,932][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:08:11,932][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:08:11,932][root][INFO] - 

[2024-10-21 21:08:11,932][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:08:13,813][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:08:13,814][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:08:13,814][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:08:13,814][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:08:13,815][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:08:13,815][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:08:13,816][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:08:13,816][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:08:13,817][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:08:13,817][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:08:13,817][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:08:13,818][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:08:13,818][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:08:13,819][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:08:13,819][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:08:13,820][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:08:13,820][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:08:13,821][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:08:13,821][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:08:13,822][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:08:13,822][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:08:13,823][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:08:13,823][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:08:13,824][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:08:13,828][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:08:14,030][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:08:14,031][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:08:16,843][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:18:45,198][root][INFO] - 

[2024-10-21 21:18:45,198][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:18:45,198][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:18:45,199][root][INFO] - 

[2024-10-21 21:18:45,199][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:18:47,482][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:18:47,482][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:18:47,483][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:18:47,483][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:18:47,484][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:18:47,484][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:18:47,484][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:18:47,485][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:18:47,485][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:18:47,486][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:18:47,486][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:18:47,486][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:18:47,487][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:18:47,487][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:18:47,488][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:18:47,488][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:18:47,489][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:18:47,489][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:18:47,490][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:18:47,490][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:18:47,490][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:18:47,491][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:18:47,491][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:18:47,492][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:18:47,496][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:18:47,704][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:18:47,705][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:18:50,482][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:20:57,245][root][INFO] - 

[2024-10-21 21:20:57,245][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:20:57,245][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:20:57,245][root][INFO] - 

[2024-10-21 21:20:57,246][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:20:59,643][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:20:59,644][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:20:59,645][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:20:59,645][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:20:59,646][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:20:59,646][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:20:59,647][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:20:59,648][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:20:59,649][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:20:59,649][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:20:59,650][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:20:59,650][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:20:59,651][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:20:59,651][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:20:59,652][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:20:59,652][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:20:59,653][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:20:59,654][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:20:59,654][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:20:59,655][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:20:59,656][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:20:59,656][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:20:59,657][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:20:59,658][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:20:59,662][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:20:59,905][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:20:59,905][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:21:02,767][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:21:58,115][root][INFO] - 

[2024-10-21 21:21:58,115][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:21:58,115][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:21:58,115][root][INFO] - 

[2024-10-21 21:21:58,115][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:22:00,071][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:00,072][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:00,073][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:00,074][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:00,075][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:00,076][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:00,077][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:00,078][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:00,079][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:00,080][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:00,081][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:00,082][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:00,083][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:00,083][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:00,084][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:00,084][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:00,085][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:00,085][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:00,086][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:00,086][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:00,087][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:00,087][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:00,087][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:00,088][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:00,092][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:22:00,295][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:22:00,296][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:22:02,772][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:22:43,066][root][INFO] - 

[2024-10-21 21:22:43,067][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:22:43,067][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:22:43,067][root][INFO] - 

[2024-10-21 21:22:43,067][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:22:44,991][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:44,991][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:44,992][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:44,993][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:44,993][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:44,994][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:44,994][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:44,995][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:44,995][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:44,996][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:44,996][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:44,997][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:44,997][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:44,998][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:44,998][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:44,999][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:44,999][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:45,000][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:45,000][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:45,001][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:45,002][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:45,002][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:45,003][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:22:45,003][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:22:45,008][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:22:45,212][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:22:45,213][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:22:47,741][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:23:19,299][root][INFO] - 

[2024-10-21 21:23:19,299][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:23:19,300][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:23:19,300][root][INFO] - 

[2024-10-21 21:23:19,300][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:23:21,300][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:23:21,300][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:23:21,301][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:23:21,302][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:23:21,302][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:23:21,302][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:23:21,303][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:23:21,304][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:23:21,304][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:23:21,305][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:23:21,305][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:23:21,306][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:23:21,306][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:23:21,307][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:23:21,307][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:23:21,308][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:23:21,308][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:23:21,309][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:23:21,309][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:23:21,310][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:23:21,311][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:23:21,311][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:23:21,312][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:23:21,312][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:23:21,318][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:23:21,523][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:23:21,524][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:23:24,060][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:24:02,884][root][INFO] - 

[2024-10-21 21:24:02,884][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:24:02,884][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:24:02,884][root][INFO] - 

[2024-10-21 21:24:02,884][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:24:04,785][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:24:04,786][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:24:04,786][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:24:04,786][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:24:04,787][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:24:04,787][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:24:04,788][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:24:04,788][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:24:04,788][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:24:04,789][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:24:04,789][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:24:04,790][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:24:04,790][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:24:04,791][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:24:04,791][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:24:04,791][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:24:04,792][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:24:04,792][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:24:04,793][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:24:04,793][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:24:04,794][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:24:04,794][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:24:04,795][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:24:04,795][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:24:04,799][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:24:05,002][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:24:05,003][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:24:07,551][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:26:21,723][root][INFO] - 

[2024-10-21 21:26:21,724][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:26:21,724][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:26:21,724][root][INFO] - 

[2024-10-21 21:26:21,724][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:26:23,609][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:26:23,610][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:26:23,610][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:26:23,610][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:26:23,611][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:26:23,611][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:26:23,612][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:26:23,612][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:26:23,613][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:26:23,613][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:26:23,613][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:26:23,614][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:26:23,614][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:26:23,615][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:26:23,615][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:26:23,616][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:26:23,616][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:26:23,617][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:26:23,617][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:26:23,617][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:26:23,618][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:26:23,618][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:26:23,619][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:26:23,619][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:26:23,624][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:26:23,827][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:26:23,828][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:26:26,323][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:30:10,708][root][INFO] - 

[2024-10-21 21:30:10,709][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:30:10,709][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:30:10,709][root][INFO] - 

[2024-10-21 21:30:10,709][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:30:12,707][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:30:12,708][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:30:12,708][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:30:12,709][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:30:12,709][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:30:12,710][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:30:12,711][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:30:12,712][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:30:12,713][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:30:12,713][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:30:12,714][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:30:12,714][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:30:12,715][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:30:12,716][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:30:12,716][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:30:12,717][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:30:12,717][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:30:12,718][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:30:12,718][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:30:12,719][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:30:12,719][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:30:12,720][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:30:12,721][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:30:12,721][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:30:12,726][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:30:12,928][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:30:12,928][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:30:15,483][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:31:22,254][root][INFO] - 

[2024-10-21 21:31:22,254][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:31:22,254][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:31:22,254][root][INFO] - 

[2024-10-21 21:31:22,255][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:31:24,259][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:31:24,259][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:31:24,260][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:31:24,260][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:31:24,261][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:31:24,261][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:31:24,261][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:31:24,262][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:31:24,262][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:31:24,263][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:31:24,263][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:31:24,263][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:31:24,264][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:31:24,264][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:31:24,265][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:31:24,265][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:31:24,266][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:31:24,266][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:31:24,266][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:31:24,267][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:31:24,267][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:31:24,268][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:31:24,268][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:31:24,268][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:31:24,273][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:31:24,487][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:31:24,488][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:31:26,958][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:31:59,463][root][INFO] - 

[2024-10-21 21:31:59,463][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:31:59,463][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:31:59,463][root][INFO] - 

[2024-10-21 21:31:59,463][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:32:01,440][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:01,440][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:01,441][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:01,441][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:01,441][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:01,442][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:01,442][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:01,443][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:01,443][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:01,444][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:01,444][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:01,445][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:01,445][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:01,445][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:01,446][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:01,446][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:01,447][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:01,447][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:01,448][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:01,448][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:01,449][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:01,449][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:01,450][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:01,450][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:01,454][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:32:01,653][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:32:01,654][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:32:04,115][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:32:27,805][root][INFO] - 

[2024-10-21 21:32:27,805][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:32:27,805][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:32:27,805][root][INFO] - 

[2024-10-21 21:32:27,806][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:32:29,759][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:29,759][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:29,760][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:29,760][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:29,761][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:29,761][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:29,761][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:29,762][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:29,762][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:29,763][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:29,763][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:29,763][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:29,764][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:29,764][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:29,765][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:29,765][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:29,766][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:29,766][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:29,767][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:29,767][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:29,768][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:29,768][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:29,768][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:32:29,769][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:32:29,773][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:32:29,976][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:32:29,977][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:32:32,520][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:33:00,917][root][INFO] - 

[2024-10-21 21:33:00,918][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:33:00,918][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:33:00,918][root][INFO] - 

[2024-10-21 21:33:00,918][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:33:03,449][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:03,449][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:03,450][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:03,450][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:03,451][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:03,451][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:03,452][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:03,452][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:03,453][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:03,453][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:03,454][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:03,455][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:03,455][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:03,456][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:03,456][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:03,457][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:03,457][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:03,458][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:03,458][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:03,459][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:03,459][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:03,460][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:03,460][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:03,461][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:03,466][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:33:03,698][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:33:03,699][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:33:06,235][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:33:38,887][root][INFO] - 

[2024-10-21 21:33:38,887][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:33:38,887][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:33:38,887][root][INFO] - 

[2024-10-21 21:33:38,887][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:33:40,812][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:40,813][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:40,813][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:40,814][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:40,814][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:40,815][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:40,815][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:40,816][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:40,816][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:40,817][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:40,817][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:40,818][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:40,818][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:40,819][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:40,819][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:40,820][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:40,820][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:40,821][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:40,821][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:40,822][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:40,822][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:40,823][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:40,823][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:33:40,824][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:33:40,829][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:33:41,030][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:33:41,031][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:33:43,552][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:34:20,391][root][INFO] - 

[2024-10-21 21:34:20,391][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:34:20,392][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:34:20,392][root][INFO] - 

[2024-10-21 21:34:20,392][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:34:22,549][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:34:22,550][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:34:22,550][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:34:22,551][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:34:22,552][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:34:22,552][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:34:22,553][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:34:22,554][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:34:22,555][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:34:22,555][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:34:22,556][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:34:22,556][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:34:22,557][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:34:22,557][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:34:22,558][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:34:22,559][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:34:22,559][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:34:22,560][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:34:22,560][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:34:22,561][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:34:22,561][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:34:22,562][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:34:22,563][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:34:22,563][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:34:22,568][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:34:22,775][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:34:22,776][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:34:25,251][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:35:53,476][root][INFO] - 

[2024-10-21 21:35:53,476][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:35:53,477][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:35:53,477][root][INFO] - 

[2024-10-21 21:35:53,477][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:35:55,468][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:35:55,468][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:35:55,469][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:35:55,469][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:35:55,470][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:35:55,470][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:35:55,471][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:35:55,471][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:35:55,472][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:35:55,472][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:35:55,473][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:35:55,473][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:35:55,474][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:35:55,475][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:35:55,475][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:35:55,476][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:35:55,476][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:35:55,477][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:35:55,477][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:35:55,478][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:35:55,478][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:35:55,479][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:35:55,480][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:35:55,480][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:35:55,486][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:35:55,686][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:35:55,687][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:35:58,213][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:36:49,230][root][INFO] - 

[2024-10-21 21:36:49,230][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:36:49,230][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:36:49,230][root][INFO] - 

[2024-10-21 21:36:49,230][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:36:51,203][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:36:51,204][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:36:51,205][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:36:51,205][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:36:51,205][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:36:51,206][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:36:51,206][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:36:51,207][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:36:51,207][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:36:51,208][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:36:51,208][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:36:51,209][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:36:51,209][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:36:51,210][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:36:51,210][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:36:51,211][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:36:51,211][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:36:51,211][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:36:51,212][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:36:51,212][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:36:51,213][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:36:51,213][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:36:51,214][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:36:51,214][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:36:51,219][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:36:51,422][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:36:51,423][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:36:54,010][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:37:16,237][root][INFO] - 

[2024-10-21 21:37:16,238][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:37:16,238][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:37:16,238][root][INFO] - 

[2024-10-21 21:37:16,238][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:37:18,201][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:37:18,202][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:37:18,203][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:37:18,203][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:37:18,203][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:37:18,204][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:37:18,204][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:37:18,205][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:37:18,205][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:37:18,206][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:37:18,206][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:37:18,206][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:37:18,207][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:37:18,207][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:37:18,208][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:37:18,208][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:37:18,209][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:37:18,209][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:37:18,210][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:37:18,210][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:37:18,211][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:37:18,211][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:37:18,212][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:37:18,212][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:37:18,216][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:37:18,419][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:37:18,420][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:37:21,008][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:38:16,673][root][INFO] - 

[2024-10-21 21:38:16,674][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:38:16,674][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:38:16,674][root][INFO] - 

[2024-10-21 21:38:16,674][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:38:18,713][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:38:18,714][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:38:18,715][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:38:18,715][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:38:18,716][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:38:18,716][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:38:18,717][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:38:18,717][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:38:18,717][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:38:18,718][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:38:18,718][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:38:18,719][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:38:18,719][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:38:18,720][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:38:18,721][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:38:18,721][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:38:18,722][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:38:18,722][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:38:18,723][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:38:18,723][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:38:18,724][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:38:18,724][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:38:18,725][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:38:18,725][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:38:18,730][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:38:18,950][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:38:18,951][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:38:21,435][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:40:13,166][root][INFO] - 

[2024-10-21 21:40:13,166][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:40:13,166][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:40:13,166][root][INFO] - 

[2024-10-21 21:40:13,166][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:40:15,183][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:15,183][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:15,184][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:15,184][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:15,185][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:15,185][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:15,186][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:15,186][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:15,187][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:15,187][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:15,188][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:15,188][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:15,189][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:15,189][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:15,190][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:15,190][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:15,191][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:15,191][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:15,192][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:15,192][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:15,193][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:15,193][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:15,194][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:15,194][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:15,199][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:40:15,404][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:40:15,405][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:40:17,849][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:40:48,234][root][INFO] - 

[2024-10-21 21:40:48,234][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:40:48,234][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:40:48,234][root][INFO] - 

[2024-10-21 21:40:48,234][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:40:50,297][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:50,298][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:50,299][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:50,299][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:50,300][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:50,300][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:50,301][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:50,301][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:50,302][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:50,302][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:50,303][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:50,304][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:50,304][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:50,305][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:50,305][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:50,306][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:50,306][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:50,307][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:50,307][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:50,308][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:50,309][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:50,309][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:50,310][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:40:50,310][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:40:50,315][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:40:50,518][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:40:50,518][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:40:53,063][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:42:43,967][root][INFO] - 

[2024-10-21 21:42:43,967][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:42:43,967][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:42:43,967][root][INFO] - 

[2024-10-21 21:42:43,967][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:42:45,846][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:42:45,846][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:42:45,847][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:42:45,847][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:42:45,848][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:42:45,848][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:42:45,849][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:42:45,849][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:42:45,849][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:42:45,850][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:42:45,850][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:42:45,851][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:42:45,851][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:42:45,852][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:42:45,852][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:42:45,853][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:42:45,853][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:42:45,853][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:42:45,854][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:42:45,854][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:42:45,855][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:42:45,855][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:42:45,856][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:42:45,856][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:42:45,861][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:42:46,064][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:42:46,065][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:42:48,632][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:44:09,233][root][INFO] - 

[2024-10-21 21:44:09,233][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:44:09,233][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:44:09,233][root][INFO] - 

[2024-10-21 21:44:09,233][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:44:11,293][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:44:11,294][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:44:11,294][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:44:11,295][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:44:11,295][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:44:11,296][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:44:11,296][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:44:11,297][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:44:11,297][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:44:11,297][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:44:11,298][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:44:11,298][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:44:11,299][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:44:11,299][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:44:11,300][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:44:11,300][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:44:11,301][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:44:11,301][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:44:11,301][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:44:11,302][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:44:11,302][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:44:11,303][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:44:11,303][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:44:11,304][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:44:11,308][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:44:11,521][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:44:11,521][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:44:14,060][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:45:34,669][root][INFO] - 

[2024-10-21 21:45:34,669][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:45:34,669][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:45:34,669][root][INFO] - 

[2024-10-21 21:45:34,670][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:45:37,349][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:37,349][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:37,350][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:37,350][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:37,351][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:37,351][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:37,352][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:37,352][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:37,353][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:37,353][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:37,354][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:37,354][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:37,355][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:37,355][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:37,356][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:37,356][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:37,357][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:37,357][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:37,358][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:37,358][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:37,359][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:37,360][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:37,360][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:37,361][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:37,365][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:45:37,571][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:45:37,572][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:45:40,108][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:45:56,576][root][INFO] - 

[2024-10-21 21:45:56,577][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:45:56,577][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:45:56,577][root][INFO] - 

[2024-10-21 21:45:56,577][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:45:58,541][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:58,542][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:58,542][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:58,543][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:58,543][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:58,544][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:58,544][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:58,545][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:58,545][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:58,545][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:58,546][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:58,546][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:58,547][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:58,547][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:58,548][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:58,548][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:58,549][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:58,549][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:58,550][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:58,550][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:58,550][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:58,551][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:58,551][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:45:58,552][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:45:58,556][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:45:58,758][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:45:58,759][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:46:01,317][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:46:10,840][root][INFO] - 

[2024-10-21 21:46:10,840][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:46:10,840][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:46:10,840][root][INFO] - 

[2024-10-21 21:46:10,841][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:46:12,799][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:46:12,801][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:46:12,802][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:46:12,803][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:46:12,804][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:46:12,805][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:46:12,806][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:46:12,807][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:46:12,809][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:46:12,810][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:46:12,811][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:46:12,812][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:46:12,813][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:46:12,814][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:46:12,814][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:46:12,815][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:46:12,815][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:46:12,816][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:46:12,816][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:46:12,817][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:46:12,817][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:46:12,818][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:46:12,818][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:46:12,819][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:46:12,824][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:46:13,027][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:46:13,028][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:46:15,508][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:47:11,890][root][INFO] - 

[2024-10-21 21:47:11,891][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:47:11,891][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:47:11,891][root][INFO] - 

[2024-10-21 21:47:11,891][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:47:14,179][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:47:14,180][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:47:14,180][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:47:14,181][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:47:14,181][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:47:14,182][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:47:14,182][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:47:14,182][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:47:14,183][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:47:14,183][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:47:14,184][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:47:14,184][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:47:14,185][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:47:14,185][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:47:14,185][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:47:14,186][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:47:14,186][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:47:14,187][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:47:14,187][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:47:14,188][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:47:14,188][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:47:14,188][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:47:14,189][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:47:14,189][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:47:14,193][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:47:14,396][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:47:14,397][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:47:16,964][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:48:34,357][root][INFO] - 

[2024-10-21 21:48:34,358][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:48:34,358][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:48:34,358][root][INFO] - 

[2024-10-21 21:48:34,358][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:48:36,346][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:48:36,346][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:48:36,347][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:48:36,347][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:48:36,348][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:48:36,348][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:48:36,349][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:48:36,349][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:48:36,350][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:48:36,350][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:48:36,351][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:48:36,351][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:48:36,352][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:48:36,353][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:48:36,353][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:48:36,354][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:48:36,354][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:48:36,355][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:48:36,355][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:48:36,356][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:48:36,356][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:48:36,357][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:48:36,357][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:48:36,358][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:48:36,363][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:48:36,581][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:48:36,582][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:48:39,166][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:49:25,858][root][INFO] - 

[2024-10-21 21:49:25,858][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:49:25,858][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:49:25,858][root][INFO] - 

[2024-10-21 21:49:25,858][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:49:28,024][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:28,025][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:28,025][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:28,026][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:28,027][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:28,027][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:28,028][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:28,028][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:28,029][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:28,029][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:28,030][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:28,030][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:28,031][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:28,031][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:28,032][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:28,032][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:28,033][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:28,034][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:28,034][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:28,035][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:28,035][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:28,036][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:28,036][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:28,037][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:28,042][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:49:28,245][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:49:28,246][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:49:30,693][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:49:49,866][root][INFO] - 

[2024-10-21 21:49:49,866][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:49:49,866][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:49:49,866][root][INFO] - 

[2024-10-21 21:49:49,867][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:49:52,059][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:52,060][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:52,061][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:52,061][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:52,062][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:52,062][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:52,063][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:52,063][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:52,064][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:52,064][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:52,065][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:52,065][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:52,066][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:52,066][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:52,067][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:52,067][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:52,068][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:52,068][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:52,069][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:52,069][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:52,070][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:52,070][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:52,071][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:49:52,072][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:49:52,076][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:49:52,279][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:49:52,279][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:49:54,725][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:52:02,026][root][INFO] - 

[2024-10-21 21:52:02,027][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:52:02,027][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:52:02,027][root][INFO] - 

[2024-10-21 21:52:02,027][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:52:03,942][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:03,943][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:03,944][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:03,945][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:03,946][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:03,947][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:03,948][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:03,949][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:03,951][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:03,952][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:03,953][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:03,954][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:03,955][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:03,955][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:03,956][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:03,957][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:03,957][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:03,958][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:03,958][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:03,958][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:03,959][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:03,960][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:03,960][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:03,961][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:03,966][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:52:04,166][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:52:04,166][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:52:06,594][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:52:31,836][root][INFO] - 

[2024-10-21 21:52:31,836][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:52:31,836][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:52:31,836][root][INFO] - 

[2024-10-21 21:52:31,836][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:52:33,807][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:33,808][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:33,808][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:33,809][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:33,809][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:33,810][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:33,810][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:33,810][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:33,811][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:33,811][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:33,812][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:33,812][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:33,813][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:33,813][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:33,814][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:33,814][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:33,815][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:33,815][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:33,816][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:33,816][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:33,817][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:33,817][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:33,817][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:52:33,818][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:52:33,822][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:52:34,022][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:52:34,023][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:52:36,498][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:55:20,386][root][INFO] - 

[2024-10-21 21:55:20,386][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:55:20,386][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:55:20,387][root][INFO] - 

[2024-10-21 21:55:20,387][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:55:22,373][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:55:22,374][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:55:22,374][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:55:22,375][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:55:22,375][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:55:22,375][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:55:22,376][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:55:22,376][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:55:22,377][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:55:22,377][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:55:22,377][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:55:22,378][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:55:22,378][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:55:22,379][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:55:22,379][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:55:22,380][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:55:22,380][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:55:22,380][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:55:22,381][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:55:22,381][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:55:22,382][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:55:22,382][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:55:22,383][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:55:22,383][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:55:22,387][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:55:22,591][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:55:22,592][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:55:25,092][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 21:59:33,471][root][INFO] - 

[2024-10-21 21:59:33,471][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 21:59:33,471][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 21:59:33,471][root][INFO] - 

[2024-10-21 21:59:33,471][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 21:59:35,593][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 21:59:35,594][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 21:59:35,596][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 21:59:35,597][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 21:59:35,598][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 21:59:35,599][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 21:59:35,600][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 21:59:35,601][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 21:59:35,603][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 21:59:35,603][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 21:59:35,604][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 21:59:35,605][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 21:59:35,605][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 21:59:35,606][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 21:59:35,606][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 21:59:35,607][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 21:59:35,607][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 21:59:35,608][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 21:59:35,608][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 21:59:35,609][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 21:59:35,609][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 21:59:35,610][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 21:59:35,610][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 21:59:35,611][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 21:59:35,616][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 21:59:35,817][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 21:59:35,818][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 21:59:38,281][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 22:00:03,421][root][INFO] - 

[2024-10-21 22:00:03,421][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 22:00:03,421][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 22:00:03,421][root][INFO] - 

[2024-10-21 22:00:03,421][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 22:00:05,698][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 22:00:05,699][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 22:00:05,699][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 22:00:05,700][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 22:00:05,700][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 22:00:05,701][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 22:00:05,701][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 22:00:05,702][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 22:00:05,702][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 22:00:05,702][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 22:00:05,703][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 22:00:05,703][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 22:00:05,704][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 22:00:05,704][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 22:00:05,705][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 22:00:05,705][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 22:00:05,706][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 22:00:05,706][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 22:00:05,706][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 22:00:05,707][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 22:00:05,707][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 22:00:05,708][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 22:00:05,708][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 22:00:05,709][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 22:00:05,714][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 22:00:05,919][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 22:00:05,920][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 22:00:08,428][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 22:01:23,632][root][INFO] - 

[2024-10-21 22:01:23,632][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 22:01:23,632][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 22:01:23,632][root][INFO] - 

[2024-10-21 22:01:23,632][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 22:01:25,827][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:25,827][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:25,828][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:25,828][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:25,828][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:25,829][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:25,829][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:25,830][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:25,830][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:25,830][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:25,831][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:25,831][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:25,832][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:25,832][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:25,833][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:25,833][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:25,833][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:25,834][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:25,834][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:25,835][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:25,835][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:25,836][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:25,836][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:25,836][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:25,841][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 22:01:26,044][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 22:01:26,045][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 22:01:28,583][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 22:01:45,947][root][INFO] - 

[2024-10-21 22:01:45,947][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 22:01:45,948][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 22:01:45,948][root][INFO] - 

[2024-10-21 22:01:45,948][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 22:01:47,849][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:47,850][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:47,851][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:47,851][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:47,852][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:47,853][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:47,853][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:47,853][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:47,854][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:47,854][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:47,855][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:47,855][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:47,856][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:47,856][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:47,857][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:47,857][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:47,858][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:47,858][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:47,858][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:47,859][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:47,859][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:47,860][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:47,860][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 22:01:47,861][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 22:01:47,865][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 22:01:48,081][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 22:01:48,082][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 22:01:50,567][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 22:05:08,685][root][INFO] - 

[2024-10-21 22:05:08,685][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 22:05:08,685][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 22:05:08,685][root][INFO] - 

[2024-10-21 22:05:08,686][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 22:05:10,655][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:10,656][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:10,656][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:10,657][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:10,657][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:10,657][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:10,658][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:10,658][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:10,659][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:10,659][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:10,660][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:10,660][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:10,660][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:10,661][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:10,661][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:10,662][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:10,662][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:10,663][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:10,663][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:10,664][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:10,664][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:10,665][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:10,665][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:10,665][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:10,670][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 22:05:10,879][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 22:05:10,880][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 22:05:13,498][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 22:05:32,169][root][INFO] - 

[2024-10-21 22:05:32,170][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 22:05:32,170][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 22:05:32,170][root][INFO] - 

[2024-10-21 22:05:32,170][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 22:05:34,350][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:34,351][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:34,351][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:34,351][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:34,352][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:34,352][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:34,353][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:34,353][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:34,354][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:34,354][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:34,354][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:34,355][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:34,355][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:34,356][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:34,356][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:34,357][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:34,357][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:34,358][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:34,358][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:34,358][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:34,359][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:34,359][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:34,360][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 22:05:34,360][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 22:05:34,365][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 22:05:34,567][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 22:05:34,568][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 22:05:37,099][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 22:06:35,975][root][INFO] - 

[2024-10-21 22:06:35,976][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 22:06:35,976][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 22:06:35,976][root][INFO] - 

[2024-10-21 22:06:35,976][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 22:06:37,981][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 22:06:37,982][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 22:06:37,983][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 22:06:37,984][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 22:06:37,985][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 22:06:37,986][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 22:06:37,987][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 22:06:37,988][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 22:06:37,989][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 22:06:37,990][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 22:06:37,991][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 22:06:37,992][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 22:06:37,993][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 22:06:37,993][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 22:06:37,994][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 22:06:37,994][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 22:06:37,995][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 22:06:37,995][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 22:06:37,996][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 22:06:37,997][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 22:06:37,997][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 22:06:37,998][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 22:06:37,999][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 22:06:38,000][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 22:06:38,005][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 22:06:38,212][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 22:06:38,213][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 22:06:40,720][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 22:07:00,161][root][INFO] - 

[2024-10-21 22:07:00,161][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 22:07:00,161][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 22:07:00,161][root][INFO] - 

[2024-10-21 22:07:00,161][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 22:07:02,088][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 22:07:02,089][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 22:07:02,089][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 22:07:02,090][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 22:07:02,090][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 22:07:02,091][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 22:07:02,091][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 22:07:02,092][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 22:07:02,092][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 22:07:02,093][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 22:07:02,093][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 22:07:02,094][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 22:07:02,094][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 22:07:02,095][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 22:07:02,096][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 22:07:02,096][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 22:07:02,097][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 22:07:02,097][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 22:07:02,098][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 22:07:02,098][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 22:07:02,099][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 22:07:02,099][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 22:07:02,100][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 22:07:02,101][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 22:07:02,107][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 22:07:02,312][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 22:07:02,313][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 22:07:04,838][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 22:12:02,506][root][INFO] - 

[2024-10-21 22:12:02,506][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 22:12:02,506][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 22:12:02,506][root][INFO] - 

[2024-10-21 22:12:02,506][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 22:12:04,473][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 22:12:04,474][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 22:12:04,474][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 22:12:04,475][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 22:12:04,475][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 22:12:04,476][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 22:12:04,476][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 22:12:04,476][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 22:12:04,477][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 22:12:04,477][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 22:12:04,478][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 22:12:04,478][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 22:12:04,479][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 22:12:04,479][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 22:12:04,480][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 22:12:04,480][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 22:12:04,480][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 22:12:04,481][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 22:12:04,481][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 22:12:04,482][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 22:12:04,482][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 22:12:04,483][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 22:12:04,483][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 22:12:04,483][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 22:12:04,490][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 22:12:04,692][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 22:12:04,693][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 22:12:07,489][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 22:13:18,957][root][INFO] - 

[2024-10-21 22:13:18,957][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 22:13:18,957][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 22:13:18,957][root][INFO] - 

[2024-10-21 22:13:18,957][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 22:13:20,994][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 22:13:20,994][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 22:13:20,995][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 22:13:20,995][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 22:13:20,996][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 22:13:20,996][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 22:13:20,997][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 22:13:20,997][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 22:13:20,998][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 22:13:20,998][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 22:13:20,998][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 22:13:20,999][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 22:13:20,999][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 22:13:21,000][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 22:13:21,000][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 22:13:21,001][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 22:13:21,001][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 22:13:21,002][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 22:13:21,002][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 22:13:21,003][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 22:13:21,003][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 22:13:21,004][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 22:13:21,005][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 22:13:21,005][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 22:13:21,010][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 22:13:21,212][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 22:13:21,213][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 22:13:23,815][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 22:17:24,018][root][INFO] - 

[2024-10-21 22:17:24,018][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 22:17:24,018][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 22:17:24,018][root][INFO] - 

[2024-10-21 22:17:24,018][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 22:17:26,381][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 22:17:26,382][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 22:17:26,383][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 22:17:26,383][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 22:17:26,384][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 22:17:26,385][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 22:17:26,386][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 22:17:26,387][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 22:17:26,388][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 22:17:26,388][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 22:17:26,389][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 22:17:26,389][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 22:17:26,390][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 22:17:26,390][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 22:17:26,391][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 22:17:26,391][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 22:17:26,392][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 22:17:26,392][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 22:17:26,392][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 22:17:26,393][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 22:17:26,393][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 22:17:26,394][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 22:17:26,394][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 22:17:26,395][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 22:17:26,395][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 22:17:28,764][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 22:18:17,282][root][INFO] - 

[2024-10-21 22:18:17,282][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 22:18:17,282][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 22:18:17,282][root][INFO] - 

[2024-10-21 22:18:17,282][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 22:18:19,197][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 22:18:19,197][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 22:18:19,198][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 22:18:19,198][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 22:18:19,198][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 22:18:19,199][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 22:18:19,199][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 22:18:19,200][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 22:18:19,200][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 22:18:19,200][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 22:18:19,201][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 22:18:19,201][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 22:18:19,202][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 22:18:19,202][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 22:18:19,202][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 22:18:19,203][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 22:18:19,203][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 22:18:19,204][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 22:18:19,204][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 22:18:19,204][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 22:18:19,205][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 22:18:19,205][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 22:18:19,206][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 22:18:19,206][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 22:18:19,206][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 22:18:21,646][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 22:20:27,729][root][INFO] - 

[2024-10-21 22:20:27,730][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 22:20:27,730][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 22:20:27,730][root][INFO] - 

[2024-10-21 22:20:27,730][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 22:20:29,745][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 22:20:29,746][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 22:20:29,747][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 22:20:29,747][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 22:20:29,748][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 22:20:29,748][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 22:20:29,749][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 22:20:29,749][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 22:20:29,750][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 22:20:29,750][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 22:20:29,751][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 22:20:29,751][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 22:20:29,752][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 22:20:29,752][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 22:20:29,753][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 22:20:29,753][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 22:20:29,754][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 22:20:29,754][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 22:20:29,755][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 22:20:29,755][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 22:20:29,756][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 22:20:29,756][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 22:20:29,757][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 22:20:29,757][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 22:20:29,764][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 22:20:29,969][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 22:20:29,970][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 22:20:32,497][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 22:21:31,961][root][INFO] - 

[2024-10-21 22:21:31,961][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 22:21:31,961][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 22:21:31,961][root][INFO] - 

[2024-10-21 22:21:31,961][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 22:21:33,910][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 22:21:33,910][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 22:21:33,911][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 22:21:33,911][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 22:21:33,912][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 22:21:33,912][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 22:21:33,913][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 22:21:33,913][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 22:21:33,913][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 22:21:33,914][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 22:21:33,914][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 22:21:33,914][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 22:21:33,915][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 22:21:33,915][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 22:21:33,916][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 22:21:33,916][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 22:21:33,917][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 22:21:33,917][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 22:21:33,918][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 22:21:33,918][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 22:21:33,918][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 22:21:33,919][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 22:21:33,919][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 22:21:33,920][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 22:21:33,923][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 22:21:34,133][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 22:21:34,134][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 22:21:36,696][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 22:24:07,634][root][INFO] - 

[2024-10-21 22:24:07,634][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 22:24:07,634][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 22:24:07,634][root][INFO] - 

[2024-10-21 22:24:07,634][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 22:24:09,594][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 22:24:09,595][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 22:24:09,596][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 22:24:09,596][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 22:24:09,597][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 22:24:09,598][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 22:24:09,599][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 22:24:09,599][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 22:24:09,600][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 22:24:09,600][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 22:24:09,601][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 22:24:09,601][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 22:24:09,602][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 22:24:09,602][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 22:24:09,603][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 22:24:09,603][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 22:24:09,604][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 22:24:09,604][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 22:24:09,605][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 22:24:09,605][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 22:24:09,606][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 22:24:09,606][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 22:24:09,607][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 22:24:09,607][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 22:24:09,614][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 22:24:09,816][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 22:24:09,816][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 22:24:12,255][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 22:44:06,299][root][INFO] - 

[2024-10-21 22:44:06,299][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 22:44:06,299][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 22:44:06,299][root][INFO] - 

[2024-10-21 22:44:06,300][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 22:44:08,331][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:08,332][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:08,333][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:08,334][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:08,335][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:08,336][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:08,337][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:08,338][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:08,339][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:08,339][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:08,340][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:08,341][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:08,342][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:08,342][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:08,343][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:08,343][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:08,344][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:08,344][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:08,345][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:08,345][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:08,346][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:08,346][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:08,347][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:08,347][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:08,352][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 22:44:08,554][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 22:44:08,555][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 22:44:35,473][root][INFO] - 

[2024-10-21 22:44:35,473][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 22:44:35,473][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 22:44:35,473][root][INFO] - 

[2024-10-21 22:44:35,473][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 22:44:37,363][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:37,364][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:37,365][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:37,365][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:37,366][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:37,367][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:37,368][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:37,368][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:37,369][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:37,369][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:37,370][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:37,370][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:37,371][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:37,372][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:37,373][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:37,373][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:37,374][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:37,375][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:37,375][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:37,376][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:37,377][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:37,378][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:37,379][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 22:44:37,379][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 22:44:37,387][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 22:44:37,590][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 22:44:37,591][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 22:44:40,149][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 22:55:32,917][root][INFO] - 

[2024-10-21 22:55:32,918][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 22:55:32,918][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 22:55:32,918][root][INFO] - 

[2024-10-21 22:55:32,918][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 22:55:34,988][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 22:55:34,988][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 22:55:34,989][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 22:55:34,989][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 22:55:34,990][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 22:55:34,990][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 22:55:34,991][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 22:55:34,991][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 22:55:34,991][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 22:55:34,992][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 22:55:34,992][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 22:55:34,993][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 22:55:34,993][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 22:55:34,994][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 22:55:34,994][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 22:55:34,994][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 22:55:34,995][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 22:55:34,995][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 22:55:34,996][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 22:55:34,996][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 22:55:34,997][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 22:55:34,997][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 22:55:34,998][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 22:55:34,998][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 22:55:35,003][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 22:55:35,233][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 22:55:35,233][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 22:55:37,756][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 22:56:41,517][root][INFO] - 

[2024-10-21 22:56:41,517][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 22:56:41,517][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 22:56:41,517][root][INFO] - 

[2024-10-21 22:56:41,517][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 22:56:43,827][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 22:56:43,828][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 22:56:43,829][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 22:56:43,830][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 22:56:43,831][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 22:56:43,831][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 22:56:43,832][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 22:56:43,833][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 22:56:43,834][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 22:56:43,835][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 22:56:43,836][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 22:56:43,836][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 22:56:43,837][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 22:56:43,837][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 22:56:43,838][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 22:56:43,838][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 22:56:43,839][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 22:56:43,839][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 22:56:43,840][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 22:56:43,840][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 22:56:43,841][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 22:56:43,842][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 22:56:43,843][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 22:56:43,843][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 22:56:43,848][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 22:56:44,051][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 22:56:44,052][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 22:56:46,611][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 23:00:03,764][root][INFO] - 

[2024-10-21 23:00:03,764][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 23:00:03,764][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 23:00:03,764][root][INFO] - 

[2024-10-21 23:00:03,765][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 23:00:06,198][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 23:00:06,199][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 23:00:06,200][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 23:00:06,200][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 23:00:06,201][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 23:00:06,201][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 23:00:06,202][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 23:00:06,202][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 23:00:06,203][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 23:00:06,203][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 23:00:06,204][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 23:00:06,204][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 23:00:06,205][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 23:00:06,205][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 23:00:06,206][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 23:00:06,206][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 23:00:06,207][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 23:00:06,207][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 23:00:06,208][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 23:00:06,208][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 23:00:06,209][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 23:00:06,209][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 23:00:06,210][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 23:00:06,210][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 23:00:06,216][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 23:00:06,419][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 23:00:06,420][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 23:00:09,019][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 23:04:17,907][root][INFO] - 

[2024-10-21 23:04:17,907][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 23:04:17,907][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 23:04:17,907][root][INFO] - 

[2024-10-21 23:04:17,907][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 23:04:20,013][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 23:04:20,014][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 23:04:20,015][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 23:04:20,016][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 23:04:20,017][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 23:04:20,018][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 23:04:20,019][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 23:04:20,020][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 23:04:20,021][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 23:04:20,022][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 23:04:20,023][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 23:04:20,024][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 23:04:20,024][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 23:04:20,025][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 23:04:20,025][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 23:04:20,025][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 23:04:20,026][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 23:04:20,026][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 23:04:20,027][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 23:04:20,027][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 23:04:20,028][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 23:04:20,028][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 23:04:20,029][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 23:04:20,029][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 23:04:20,033][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 23:04:20,295][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 23:04:20,296][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 23:04:22,808][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 23:08:29,565][root][INFO] - 

[2024-10-21 23:08:29,566][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 23:08:29,566][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 23:08:29,566][root][INFO] - 

[2024-10-21 23:08:29,566][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 23:08:31,508][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 23:08:31,508][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 23:08:31,509][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 23:08:31,509][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 23:08:31,510][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 23:08:31,510][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 23:08:31,510][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 23:08:31,511][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 23:08:31,511][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 23:08:31,512][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 23:08:31,512][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 23:08:31,513][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 23:08:31,513][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 23:08:31,514][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 23:08:31,514][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 23:08:31,515][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 23:08:31,515][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 23:08:31,515][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 23:08:31,516][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 23:08:31,516][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 23:08:31,517][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 23:08:31,517][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 23:08:31,518][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 23:08:31,518][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 23:08:31,523][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 23:08:31,723][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 23:08:31,724][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 23:08:34,233][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 23:09:02,424][root][INFO] - 

[2024-10-21 23:09:02,424][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 23:09:02,424][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 23:09:02,424][root][INFO] - 

[2024-10-21 23:09:02,424][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 23:09:05,030][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:05,031][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:05,032][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:05,033][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:05,034][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:05,035][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:05,036][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:05,037][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:05,038][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:05,039][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:05,040][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:05,041][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:05,041][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:05,042][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:05,043][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:05,043][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:05,044][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:05,045][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:05,045][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:05,046][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:05,047][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:05,047][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:05,048][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:05,049][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:05,056][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 23:09:05,264][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 23:09:05,265][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 23:09:07,833][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 23:09:35,604][root][INFO] - 

[2024-10-21 23:09:35,604][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 23:09:35,604][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 23:09:35,604][root][INFO] - 

[2024-10-21 23:09:35,604][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 23:09:38,084][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:38,084][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:38,085][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:38,085][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:38,086][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:38,086][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:38,087][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:38,087][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:38,087][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:38,088][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:38,088][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:38,089][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:38,089][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:38,090][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:38,090][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:38,091][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:38,091][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:38,092][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:38,092][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:38,092][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:38,093][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:38,093][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:38,094][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 23:09:38,094][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 23:09:38,099][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 23:09:38,301][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 23:09:38,302][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 23:09:40,863][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 23:10:21,155][root][INFO] - 

[2024-10-21 23:10:21,155][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 23:10:21,155][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 23:10:21,155][root][INFO] - 

[2024-10-21 23:10:21,155][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 23:10:23,380][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 23:10:23,380][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 23:10:23,381][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 23:10:23,381][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 23:10:23,382][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 23:10:23,382][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 23:10:23,383][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 23:10:23,383][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 23:10:23,383][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 23:10:23,384][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 23:10:23,384][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 23:10:23,385][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 23:10:23,385][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 23:10:23,386][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 23:10:23,386][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 23:10:23,387][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 23:10:23,387][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 23:10:23,387][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 23:10:23,388][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 23:10:23,388][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 23:10:23,389][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 23:10:23,389][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 23:10:23,390][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 23:10:23,390][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 23:10:23,396][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 23:10:23,598][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 23:10:23,598][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 23:10:26,481][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 23:11:03,145][root][INFO] - 

[2024-10-21 23:11:03,145][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 23:11:03,145][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 23:11:03,145][root][INFO] - 

[2024-10-21 23:11:03,146][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 23:11:05,445][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 23:11:05,446][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 23:11:05,447][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 23:11:05,447][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 23:11:05,448][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 23:11:05,448][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 23:11:05,449][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 23:11:05,449][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 23:11:05,450][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 23:11:05,450][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 23:11:05,451][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 23:11:05,452][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 23:11:05,452][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 23:11:05,453][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 23:11:05,453][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 23:11:05,454][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 23:11:05,454][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 23:11:05,455][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 23:11:05,456][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 23:11:05,456][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 23:11:05,457][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 23:11:05,457][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 23:11:05,458][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 23:11:05,458][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 23:11:05,464][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 23:11:05,670][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 23:11:05,671][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 23:11:08,359][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 23:13:14,491][root][INFO] - 

[2024-10-21 23:13:14,491][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 23:13:14,491][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 23:13:14,491][root][INFO] - 

[2024-10-21 23:13:14,491][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 23:13:16,442][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 23:13:16,443][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 23:13:16,443][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 23:13:16,444][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 23:13:16,444][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 23:13:16,445][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 23:13:16,445][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 23:13:16,445][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 23:13:16,446][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 23:13:16,446][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 23:13:16,447][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 23:13:16,447][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 23:13:16,448][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 23:13:16,448][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 23:13:16,448][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 23:13:16,449][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 23:13:16,449][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 23:13:16,450][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 23:13:16,450][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 23:13:16,450][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 23:13:16,451][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 23:13:16,451][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 23:13:16,452][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 23:13:16,452][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 23:13:16,457][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 23:13:16,656][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 23:13:16,657][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 23:13:19,159][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 23:21:09,005][root][INFO] - 

[2024-10-21 23:21:09,006][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 23:21:09,006][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 23:21:09,006][root][INFO] - 

[2024-10-21 23:21:09,006][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 23:21:10,979][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 23:21:10,980][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 23:21:10,980][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 23:21:10,981][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 23:21:10,981][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 23:21:10,982][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 23:21:10,982][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 23:21:10,983][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 23:21:10,983][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 23:21:10,984][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 23:21:10,984][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 23:21:10,985][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 23:21:10,985][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 23:21:10,986][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 23:21:10,986][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 23:21:10,987][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 23:21:10,987][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 23:21:10,988][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 23:21:10,988][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 23:21:10,989][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 23:21:10,989][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 23:21:10,990][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 23:21:10,990][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 23:21:10,991][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 23:21:10,996][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 23:21:11,199][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 23:21:11,200][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 23:21:13,943][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 23:37:59,048][root][INFO] - 

[2024-10-21 23:37:59,048][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 23:37:59,048][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 23:37:59,048][root][INFO] - 

[2024-10-21 23:37:59,048][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 23:38:01,202][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 23:38:01,203][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 23:38:01,203][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 23:38:01,204][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 23:38:01,205][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 23:38:01,205][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 23:38:01,206][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 23:38:01,206][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 23:38:01,206][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 23:38:01,207][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 23:38:01,207][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 23:38:01,208][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 23:38:01,208][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 23:38:01,209][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 23:38:01,209][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 23:38:01,210][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 23:38:01,210][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 23:38:01,210][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 23:38:01,215][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 23:38:01,215][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 23:38:01,216][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 23:38:01,217][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 23:38:01,217][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 23:38:01,218][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 23:38:01,222][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 23:38:01,427][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 23:38:01,428][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 23:38:03,968][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-21 23:50:33,285][root][INFO] - 

[2024-10-21 23:50:33,285][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-21 23:50:33,285][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-21 23:50:33,285][root][INFO] - 

[2024-10-21 23:50:33,286][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/kombo/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/kombo/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-21 23:50:35,553][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-21 23:50:35,554][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-21 23:50:35,554][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-21 23:50:35,555][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-21 23:50:35,555][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-21 23:50:35,556][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-21 23:50:35,556][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-21 23:50:35,557][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-21 23:50:35,557][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-21 23:50:35,558][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-21 23:50:35,558][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-21 23:50:35,559][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-21 23:50:35,559][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-21 23:50:35,560][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-21 23:50:35,561][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-21 23:50:35,561][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-21 23:50:35,562][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-21 23:50:35,562][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-21 23:50:35,563][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-21 23:50:35,563][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-21 23:50:35,564][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-21 23:50:35,564][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-21 23:50:35,565][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-21 23:50:35,566][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-21 23:50:35,571][root][INFO] - Change the max_length to 2046 for the kombo_tokenizer's truncation.
[2024-10-21 23:50:35,778][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-21 23:50:35,779][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-21 23:50:38,313][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:07:22,290][root][INFO] - 

[2024-10-22 00:07:22,290][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:07:22,290][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs
[2024-10-22 00:07:22,290][root][INFO] - 

[2024-10-22 00:07:22,290][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/lora/nlu_tasks_lrs/KorSTS/ko_punc/256t_64b_1s_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 128, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KorSTS'}, 'optim': {'name': 'adamwscale', 'base_lr': 5e-05, 'base_lrs': '', 'batch_size': 64, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 8, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/lora/nlu_ft/KorSTS/ko_punc/128t_64b_8s_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-22 00:07:25,454][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:07:25,454][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:07:25,455][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:07:25,455][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:07:25,456][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:07:25,456][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:07:25,457][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:07:25,457][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:07:25,458][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:07:25,458][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:07:25,459][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:07:25,459][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:07:25,460][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:07:25,460][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:07:25,461][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:07:25,461][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:07:25,462][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:07:25,462][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:07:25,463][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:07:25,463][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:07:25,463][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:07:25,464][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:07:25,464][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:07:25,465][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:07:25,465][root][INFO] - 
Save directory: 256t_64b_1s_1rs
[2024-10-22 00:07:27,937][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:14:03,102][root][INFO] - 

[2024-10-22 00:14:03,102][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:14:03,102][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs
[2024-10-22 00:14:03,102][root][INFO] - 

[2024-10-22 00:14:03,102][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/KoCommonGen/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4096, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.02, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:14:05,302][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:14:05,303][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:14:05,304][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:14:05,304][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:14:05,305][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:14:05,306][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:14:05,307][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:14:05,307][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:14:05,308][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:14:05,309][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:14:05,309][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:14:05,310][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:14:05,310][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:14:05,311][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:14:05,311][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:14:05,312][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:14:05,312][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:14:05,313][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:14:05,313][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:14:05,314][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:14:05,315][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:14:05,315][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:14:05,316][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:14:05,316][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:14:05,316][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-22 00:16:00,301][root][INFO] - 

[2024-10-22 00:16:00,301][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:16:00,301][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs
[2024-10-22 00:16:00,301][root][INFO] - 

[2024-10-22 00:16:00,301][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4096, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.02, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:16:02,635][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:16:02,637][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:16:02,639][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:16:02,640][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:16:02,642][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:16:02,643][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:16:02,644][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:16:02,645][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:16:02,646][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:16:02,647][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:16:02,648][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:16:02,649][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:16:02,650][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:16:02,651][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:16:02,652][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:16:02,653][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:16:02,654][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:16:02,654][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:16:02,655][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:16:02,656][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:16:02,656][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:16:02,657][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:16:02,658][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:16:02,658][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:16:02,659][root][INFO] - 
Save directory: lora_kombo_comb-gru_128+30t_128b_1s_0.04lr_1rs
[2024-10-22 00:17:08,457][root][INFO] - 

[2024-10-22 00:17:08,457][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:17:08,457][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs
[2024-10-22 00:17:08,457][root][INFO] - 

[2024-10-22 00:17:08,457][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.01lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4096, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.02, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:17:11,352][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:11,354][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:11,355][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:11,355][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:11,356][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:11,357][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:11,358][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:11,358][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:11,359][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:11,359][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:11,360][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:11,361][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:11,362][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:11,362][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:11,363][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:11,364][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:11,364][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:11,365][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:11,366][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:11,366][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:11,367][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:11,368][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:11,368][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:11,369][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:11,369][root][INFO] - 
Save directory: lora_512+64t_128b_2s_0.01lr_1rs
[2024-10-22 00:17:14,161][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:17:20,947][root][INFO] - 

[2024-10-22 00:17:20,947][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:17:20,947][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-22 00:17:20,947][root][INFO] - 

[2024-10-22 00:17:20,947][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4067, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:17:23,215][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:23,216][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:23,217][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:23,217][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:23,218][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:23,219][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:23,219][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:23,220][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:23,220][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:23,221][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:23,221][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:23,222][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:23,223][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:23,223][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:23,224][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:23,224][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:23,225][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:23,226][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:23,226][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:23,227][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:23,228][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:23,228][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:23,229][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:17:23,229][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:17:23,236][root][INFO] - Change the max_length to 4065 for the kombo_tokenizer's truncation.
[2024-10-22 00:17:23,450][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-22 00:17:23,451][root][INFO] - 
Save directory: lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-22 00:17:26,029][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:21:28,741][root][INFO] - 

[2024-10-22 00:21:28,741][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:21:28,741][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs
[2024-10-22 00:21:28,741][root][INFO] - 

[2024-10-22 00:21:28,741][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.01lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4096, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.02, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:21:30,866][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:30,867][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:30,868][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:30,868][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:30,869][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:30,870][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:30,870][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:30,871][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:30,871][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:30,872][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:30,873][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:30,873][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:30,874][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:30,875][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:30,875][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:30,876][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:30,877][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:30,877][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:30,878][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:30,878][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:30,879][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:30,880][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:30,880][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:30,881][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:30,881][root][INFO] - 
Save directory: lora_512+64t_128b_2s_0.01lr_1rs
[2024-10-22 00:21:33,392][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:21:39,189][root][INFO] - 

[2024-10-22 00:21:39,189][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:21:39,189][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-22 00:21:39,189][root][INFO] - 

[2024-10-22 00:21:39,190][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4067, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:21:41,305][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:41,306][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:41,307][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:41,307][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:41,308][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:41,309][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:41,309][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:41,310][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:41,310][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:41,311][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:41,311][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:41,312][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:41,312][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:41,313][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:41,314][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:41,314][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:41,315][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:41,315][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:41,316][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:41,316][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:41,317][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:41,318][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:41,318][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:21:41,319][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:21:41,323][root][INFO] - Change the max_length to 4065 for the kombo_tokenizer's truncation.
[2024-10-22 00:21:41,529][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-22 00:21:41,530][root][INFO] - 
Save directory: lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-22 00:21:44,103][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:26:00,389][root][INFO] - 

[2024-10-22 00:26:00,389][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:26:00,389][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs
[2024-10-22 00:26:00,389][root][INFO] - 

[2024-10-22 00:26:00,389][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.01lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4096, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.02, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:26:02,547][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:02,548][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:02,549][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:02,550][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:02,551][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:02,551][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:02,552][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:02,553][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:02,553][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:02,554][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:02,554][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:02,555][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:02,556][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:02,556][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:02,557][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:02,558][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:02,558][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:02,559][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:02,560][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:02,560][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:02,561][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:02,562][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:02,562][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:02,563][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:02,563][root][INFO] - 
Save directory: lora_512+64t_128b_2s_0.01lr_1rs
[2024-10-22 00:26:05,107][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:26:10,919][root][INFO] - 

[2024-10-22 00:26:10,919][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:26:10,919][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-22 00:26:10,920][root][INFO] - 

[2024-10-22 00:26:10,920][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4067, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:26:13,062][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:13,063][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:13,063][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:13,064][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:13,064][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:13,065][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:13,065][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:13,066][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:13,067][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:13,067][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:13,068][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:13,068][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:13,069][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:13,069][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:13,070][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:13,071][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:13,071][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:13,072][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:13,072][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:13,072][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:13,073][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:13,073][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:13,074][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:26:13,074][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:26:13,078][root][INFO] - Change the max_length to 4065 for the kombo_tokenizer's truncation.
[2024-10-22 00:26:13,301][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-22 00:26:13,302][root][INFO] - 
Save directory: lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-22 00:26:15,904][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:26:59,872][root][INFO] - 

[2024-10-22 00:26:59,873][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:26:59,873][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs
[2024-10-22 00:26:59,873][root][INFO] - 

[2024-10-22 00:26:59,873][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.01lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4096, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.02, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:27:02,234][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:02,235][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:02,236][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:02,236][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:02,237][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:02,238][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:02,238][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:02,239][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:02,239][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:02,240][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:02,240][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:02,241][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:02,242][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:02,242][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:02,243][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:02,244][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:02,244][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:02,245][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:02,245][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:02,246][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:02,246][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:02,247][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:02,247][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:02,248][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:02,248][root][INFO] - 
Save directory: lora_512+64t_128b_2s_0.01lr_1rs
[2024-10-22 00:27:04,737][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:27:10,766][root][INFO] - 

[2024-10-22 00:27:10,766][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:27:10,766][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-22 00:27:10,766][root][INFO] - 

[2024-10-22 00:27:10,766][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4067, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:27:13,719][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:13,720][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:13,721][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:13,721][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:13,722][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:13,723][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:13,724][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:13,724][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:13,725][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:13,725][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:13,725][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:13,726][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:13,727][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:13,727][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:13,728][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:13,728][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:13,729][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:13,729][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:13,730][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:13,730][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:13,731][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:13,731][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:13,732][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:27:13,732][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:27:13,739][root][INFO] - Change the max_length to 4065 for the kombo_tokenizer's truncation.
[2024-10-22 00:27:13,987][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-22 00:27:13,988][root][INFO] - 
Save directory: lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-22 00:27:16,460][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:28:06,220][root][INFO] - 

[2024-10-22 00:28:06,220][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:28:06,220][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs
[2024-10-22 00:28:06,220][root][INFO] - 

[2024-10-22 00:28:06,221][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.01lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4096, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.02, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:28:08,455][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:28:08,456][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:28:08,457][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:28:08,457][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:28:08,458][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:28:08,458][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:28:08,459][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:28:08,460][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:28:08,460][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:28:08,461][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:28:08,462][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:28:08,462][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:28:08,463][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:28:08,463][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:28:08,464][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:28:08,464][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:28:08,465][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:28:08,465][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:28:08,466][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:28:08,466][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:28:08,467][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:28:08,467][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:28:08,468][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:28:08,468][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:28:08,469][root][INFO] - 
Save directory: lora_512+64t_128b_2s_0.01lr_1rs
[2024-10-22 00:28:10,960][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:28:17,152][root][INFO] - 

[2024-10-22 00:28:17,152][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:28:17,152][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-22 00:28:17,152][root][INFO] - 

[2024-10-22 00:28:17,153][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4067, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:29:26,312][root][INFO] - 

[2024-10-22 00:29:26,312][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:29:26,312][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs
[2024-10-22 00:29:26,312][root][INFO] - 

[2024-10-22 00:29:26,313][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.01lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4096, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.02, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:29:28,844][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:28,846][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:28,848][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:28,850][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:28,851][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:28,852][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:28,853][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:28,855][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:28,856][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:28,857][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:28,859][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:28,859][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:28,860][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:28,860][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:28,861][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:28,861][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:28,862][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:28,863][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:28,864][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:28,865][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:28,866][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:28,867][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:28,868][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:28,869][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:28,870][root][INFO] - 
Save directory: lora_512+64t_128b_2s_0.01lr_1rs
[2024-10-22 00:29:31,721][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:29:37,812][root][INFO] - 

[2024-10-22 00:29:37,812][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:29:37,812][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-22 00:29:37,812][root][INFO] - 

[2024-10-22 00:29:37,812][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4067, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:29:40,858][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:40,859][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:40,861][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:40,861][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:40,863][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:40,863][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:40,864][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:40,865][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:40,866][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:40,867][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:40,868][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:40,869][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:40,870][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:40,870][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:40,871][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:40,872][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:40,872][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:40,873][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:40,874][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:40,875][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:40,875][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:40,876][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:40,877][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:29:40,878][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:29:40,884][root][INFO] - Change the max_length to 4065 for the kombo_tokenizer's truncation.
[2024-10-22 00:29:41,089][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-22 00:29:41,090][root][INFO] - 
Save directory: lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-22 00:29:43,663][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:30:20,750][root][INFO] - 

[2024-10-22 00:30:20,750][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:30:20,750][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs
[2024-10-22 00:30:20,750][root][INFO] - 

[2024-10-22 00:30:20,750][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.01lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4096, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.02, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:30:23,013][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:23,014][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:23,015][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:23,015][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:23,016][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:23,016][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:23,017][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:23,017][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:23,018][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:23,018][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:23,019][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:23,019][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:23,020][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:23,020][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:23,021][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:23,021][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:23,022][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:23,022][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:23,023][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:23,024][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:23,024][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:23,025][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:23,025][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:23,026][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:23,026][root][INFO] - 
Save directory: lora_512+64t_128b_2s_0.01lr_1rs
[2024-10-22 00:30:25,535][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:30:31,557][root][INFO] - 

[2024-10-22 00:30:31,557][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:30:31,557][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-22 00:30:31,557][root][INFO] - 

[2024-10-22 00:30:31,557][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4067, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:30:33,660][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:33,661][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:33,661][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:33,662][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:33,663][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:33,663][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:33,664][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:33,664][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:33,665][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:33,666][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:33,666][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:33,667][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:33,667][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:33,668][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:33,669][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:33,669][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:33,670][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:33,671][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:33,671][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:33,672][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:33,672][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:33,673][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:33,673][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:33,674][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:33,679][root][INFO] - Change the max_length to 4065 for the kombo_tokenizer's truncation.
[2024-10-22 00:30:33,883][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-22 00:30:33,883][root][INFO] - 
Save directory: lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-22 00:30:36,456][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:30:57,334][root][INFO] - 

[2024-10-22 00:30:57,334][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:30:57,334][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs
[2024-10-22 00:30:57,334][root][INFO] - 

[2024-10-22 00:30:57,334][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.01lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4096, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.02, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:30:59,817][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:59,818][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:59,819][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:59,820][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:59,822][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:59,823][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:59,824][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:59,825][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:59,826][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:59,827][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:59,828][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:59,828][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:59,829][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:59,830][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:59,831][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:59,832][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:59,833][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:59,834][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:59,835][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:59,836][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:59,836][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:59,837][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:59,838][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:30:59,838][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:30:59,839][root][INFO] - 
Save directory: lora_512+64t_128b_2s_0.01lr_1rs
[2024-10-22 00:31:02,687][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:31:08,785][root][INFO] - 

[2024-10-22 00:31:08,786][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:31:08,786][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-22 00:31:08,786][root][INFO] - 

[2024-10-22 00:31:08,786][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4067, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:31:11,259][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:31:11,260][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:31:11,261][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:31:11,262][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:31:11,262][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:31:11,263][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:31:11,264][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:31:11,264][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:31:11,265][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:31:11,266][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:31:11,266][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:31:11,266][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:31:11,267][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:31:11,268][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:31:11,268][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:31:11,269][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:31:11,269][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:31:11,270][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:31:11,270][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:31:11,271][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:31:11,271][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:31:11,272][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:31:11,272][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:31:11,273][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:31:11,278][root][INFO] - Change the max_length to 4065 for the kombo_tokenizer's truncation.
[2024-10-22 00:31:11,479][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-22 00:31:11,480][root][INFO] - 
Save directory: lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-22 00:31:14,088][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:37:04,372][root][INFO] - 

[2024-10-22 00:37:04,372][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:37:04,372][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs
[2024-10-22 00:37:04,372][root][INFO] - 

[2024-10-22 00:37:04,372][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.01lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4096, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.02, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:37:06,548][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:06,549][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:06,550][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:06,551][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:06,552][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:06,553][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:06,554][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:06,555][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:06,556][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:06,557][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:06,558][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:06,559][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:06,560][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:06,561][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:06,562][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:06,563][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:06,564][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:06,565][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:06,566][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:06,567][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:06,568][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:06,569][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:06,569][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:06,570][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:06,571][root][INFO] - 
Save directory: lora_512+64t_128b_2s_0.01lr_1rs
[2024-10-22 00:37:09,072][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:37:14,988][root][INFO] - 

[2024-10-22 00:37:14,988][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:37:14,989][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-22 00:37:14,989][root][INFO] - 

[2024-10-22 00:37:14,989][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4067, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:37:17,139][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:17,140][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:17,141][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:17,142][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:17,142][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:17,143][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:17,144][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:17,145][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:17,145][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:17,146][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:17,146][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:17,147][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:17,148][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:17,148][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:17,149][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:17,149][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:17,150][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:17,151][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:17,151][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:17,152][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:17,153][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:17,153][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:17,154][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:37:17,154][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:37:17,160][root][INFO] - Change the max_length to 4065 for the kombo_tokenizer's truncation.
[2024-10-22 00:37:17,363][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-22 00:37:17,364][root][INFO] - 
Save directory: lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-22 00:37:19,941][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:37:59,806][root][INFO] - 

[2024-10-22 00:37:59,807][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:37:59,807][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs
[2024-10-22 00:37:59,807][root][INFO] - 

[2024-10-22 00:37:59,807][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.01lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4096, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.02, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:38:02,023][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:02,024][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:02,025][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:02,025][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:02,026][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:02,026][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:02,027][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:02,027][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:02,028][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:02,028][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:02,029][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:02,030][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:02,030][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:02,031][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:02,031][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:02,032][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:02,033][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:02,033][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:02,033][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:02,034][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:02,034][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:02,035][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:02,040][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:02,040][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:02,041][root][INFO] - 
Save directory: lora_512+64t_128b_2s_0.01lr_1rs
[2024-10-22 00:38:04,597][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:38:10,482][root][INFO] - 

[2024-10-22 00:38:10,482][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:38:10,482][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-22 00:38:10,482][root][INFO] - 

[2024-10-22 00:38:10,482][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4067, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:38:13,048][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:13,049][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:13,049][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:13,050][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:13,051][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:13,052][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:13,052][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:13,053][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:13,054][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:13,054][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:13,055][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:13,055][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:13,056][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:13,056][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:13,057][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:13,057][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:13,058][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:13,059][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:13,059][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:13,060][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:13,060][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:13,061][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:13,061][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:38:13,062][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:38:13,067][root][INFO] - Change the max_length to 4065 for the kombo_tokenizer's truncation.
[2024-10-22 00:38:13,269][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-22 00:38:13,270][root][INFO] - 
Save directory: lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-22 00:38:15,858][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:39:43,115][root][INFO] - 

[2024-10-22 00:39:43,116][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:39:43,116][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs
[2024-10-22 00:39:43,116][root][INFO] - 

[2024-10-22 00:39:43,116][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.01lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4096, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.02, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:39:45,972][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:45,973][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:45,974][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:45,975][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:45,975][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:45,976][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:45,976][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:45,977][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:45,978][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:45,978][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:45,978][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:45,979][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:45,980][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:45,980][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:45,981][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:45,981][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:45,982][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:45,982][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:45,983][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:45,983][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:45,984][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:45,984][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:45,985][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:45,985][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:45,986][root][INFO] - 
Save directory: lora_512+64t_128b_2s_0.01lr_1rs
[2024-10-22 00:39:48,664][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:39:54,510][root][INFO] - 

[2024-10-22 00:39:54,510][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:39:54,510][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-22 00:39:54,510][root][INFO] - 

[2024-10-22 00:39:54,510][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4067, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:39:56,620][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:56,621][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:56,622][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:56,623][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:56,623][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:56,624][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:56,625][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:56,625][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:56,626][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:56,626][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:56,627][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:56,628][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:56,628][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:56,629][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:56,630][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:56,630][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:56,632][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:56,632][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:56,633][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:56,634][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:56,634][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:56,635][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:56,636][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:39:56,636][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:39:56,646][root][INFO] - Change the max_length to 4065 for the kombo_tokenizer's truncation.
[2024-10-22 00:39:56,858][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-22 00:39:56,859][root][INFO] - 
Save directory: lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-22 00:39:59,700][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:40:19,513][root][INFO] - 

[2024-10-22 00:40:19,513][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:40:19,513][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs
[2024-10-22 00:40:19,513][root][INFO] - 

[2024-10-22 00:40:19,513][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.01lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': False, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4096, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.02, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_512+64t_128b_2s_0.02lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:40:21,604][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:21,604][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:21,605][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:21,606][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:21,607][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:21,607][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:21,608][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:21,608][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:21,609][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:21,609][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:21,610][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:21,610][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:21,611][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:21,611][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:21,612][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:21,612][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:21,613][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:21,613][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:21,614][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:21,614][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:21,615][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:21,615][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:21,616][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:21,616][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:21,617][root][INFO] - 
Save directory: lora_512+64t_128b_2s_0.01lr_1rs
[2024-10-22 00:40:24,100][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
[2024-10-22 00:40:30,092][root][INFO] - 

[2024-10-22 00:40:30,092][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-22 00:40:30,093][root][INFO] - Save the parser information to analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-22 00:40:30,093][root][INFO] - 

[2024-10-22 00:40:30,093][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '/data3/user21/KOMBO_Generation/logs/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs/ckpt', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_kombo': True, 'kombo': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'kombo_max_length': 4067, 'do_combination': True, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': False, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}, 'generation_config': {'min_length': 64, 'max_length': 512, 'max_new_tokens': 64, 'no_repeat_ngram_size': 3, 'do_sample': False, 'num_beams': 4, 'pad_token_id': None, 'bos_token_id': None, 'eos_token_id': None}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 581, 'num_workers': 4, 'task_name': 'XL_Sum'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.04, 'base_lrs': '', 'batch_size': 128, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 2, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1, 'train_batch_size': 16, 'eval_batch_size': 8}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 10000, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'analysis/embedding/skt_kogpt2-base-v2/nlg_tasks/XL_Sum/lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs', 'tb_dir': '${logging.log_dir}/tb', 'save_dir': '${logging.log_dir}/ckpt'}, 'mode': 'nlg_ft'}

[2024-10-22 00:40:32,258][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:32,259][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:32,260][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:32,261][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:32,262][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:32,263][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:32,264][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:32,265][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:32,266][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:32,267][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:32,268][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:32,268][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:32,269][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:32,269][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:32,270][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:32,270][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:32,271][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:32,271][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:32,271][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:32,272][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:32,272][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:32,273][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:32,273][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-22 00:40:32,274][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-22 00:40:32,281][root][INFO] - Change the max_length to 4065 for the kombo_tokenizer's truncation.
[2024-10-22 00:40:32,498][root][INFO] - Replaced transformer.wte with KOMBO_LoRA_Layer
[2024-10-22 00:40:32,499][root][INFO] - 
Save directory: lora_kombo_comb-gru_512+64t_128b_2s_0.04lr_1rs
[2024-10-22 00:40:35,045][root][INFO] - Complete to reload the checkpoint of the model from above save directory.
