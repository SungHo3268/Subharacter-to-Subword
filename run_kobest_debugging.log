[2024-10-31 16:15:24,269][root][INFO] - 

[2024-10-31 16:15:24,269][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-31 16:15:24,270][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_256_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 16:15:24,270][root][INFO] - 

[2024-10-31 16:15:24,270][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 256, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_256_red-linear_pool_256t_8b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_256_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_256_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-31 16:15:31,793][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-31 16:15:31,794][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-31 16:15:31,795][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-31 16:15:31,795][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-31 16:15:31,796][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-31 16:15:31,796][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-31 16:15:31,797][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-31 16:15:31,797][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-31 16:15:31,797][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-31 16:15:31,798][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-31 16:15:31,798][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-31 16:15:31,799][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-31 16:15:31,799][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-31 16:15:31,800][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-31 16:15:31,800][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-31 16:15:31,800][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-31 16:15:31,801][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-31 16:15:31,801][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-31 16:15:31,802][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-31 16:15:31,802][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-31 16:15:31,803][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-31 16:15:31,803][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-31 16:15:31,804][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-31 16:15:31,804][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-31 16:15:31,806][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-10-31 16:15:31,810][root][INFO] - Change the max_length to 255 for the sub2_tokenizer's truncation.
[2024-10-31 16:15:31,950][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-10-31 16:15:31,952][root][INFO] - Trainable params: 11397120 || all params: 136562688 || trainable: 8.35 %
[2024-10-31 16:15:32,124][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-10-31 16:15:34,160][root][INFO] - 

[2024-10-31 16:15:34,160][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-10-31 16:15:34,160][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-10-31 16:15:34,168][root][INFO] - vocab size              : 51200
[2024-10-31 16:15:34,169][root][INFO] - device                  : gpu
[2024-10-31 16:15:34,169][root][INFO] - random seed             : 1
[2024-10-31 16:15:34,169][root][INFO] - train data size         : 3672
[2024-10-31 16:15:34,169][root][INFO] - max epochs              : 10
[2024-10-31 16:15:34,169][root][INFO] - total steps             : 4590
[2024-10-31 16:15:34,169][root][INFO] - warmup steps            : 459
[2024-10-31 16:15:34,169][root][INFO] - batch size              : 8
[2024-10-31 16:15:34,169][root][INFO] - accumulation steps      : 1
[2024-10-31 16:15:34,169][root][INFO] - optimizer               : adamwscale
[2024-10-31 16:15:34,169][root][INFO] - lr_scheduler            : cosine
[2024-10-31 16:15:34,169][root][INFO] - learning rate           : 0.01
[2024-10-31 16:15:34,169][root][INFO] - max length              : 256

[2024-10-31 16:15:34,170][root][INFO] - LoRA Configuration
[2024-10-31 16:15:34,170][root][INFO] - ㄴ r                    : 32
[2024-10-31 16:15:34,170][root][INFO] - ㄴ alpha                : 128
[2024-10-31 16:15:34,170][root][INFO] - ㄴ dropout              : 0.03

[2024-10-31 16:15:34,170][root][INFO] - SUB2 Configuration
[2024-10-31 16:15:34,170][root][INFO] - ㄴ tok_type             : jamo_var
[2024-10-31 16:15:34,170][root][INFO] - ㄴ hidden_dim           : 768
[2024-10-31 16:15:34,170][root][INFO] - ㄴ sub2_max_length     : 255
[2024-10-31 16:15:34,170][root][INFO] - ㄴ embedding_norm       : False
[2024-10-31 16:15:34,170][root][INFO] - ㄴ do_combination       : False
[2024-10-31 16:15:34,171][root][INFO] - ㄴ reducer              : linear_pool

[2024-10-31 16:15:34,171][root][INFO] - LoRA in SUB2 Configuration
[2024-10-31 16:15:34,171][root][INFO] - ㄴ r                : 32
[2024-10-31 16:15:34,171][root][INFO] - ㄴ alpha            : 128
[2024-10-31 16:15:34,171][root][INFO] - ㄴ dropout          : 0.03

[2024-10-31 16:15:34,171][root][INFO] - 

[2024-10-31 16:15:34,171][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_256_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 16:15:34,171][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_256_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt
[2024-10-31 16:15:34,171][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_256_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb
[2024-10-31 16:15:34,171][root][INFO] - * tb interval   : 100

[2024-10-31 16:15:34,171][root][INFO] - 

[2024-10-31 16:15:34,171][root][INFO] - Start the Training !
[2024-10-31 16:15:34,175][root][INFO] - 
[1/ 10 Epoch]
[2024-10-31 16:17:16,929][root][INFO] - 

[2024-10-31 16:17:16,929][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-31 16:17:16,929][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_256_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 16:17:16,929][root][INFO] - 

[2024-10-31 16:17:16,929][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 256, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_256_red-linear_pool_256t_8b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_256_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_256_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-31 16:17:24,452][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-31 16:17:24,453][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-31 16:17:24,454][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-31 16:17:24,454][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-31 16:17:24,454][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-31 16:17:24,455][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-31 16:17:24,455][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-31 16:17:24,456][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-31 16:17:24,456][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-31 16:17:24,457][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-31 16:17:24,457][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-31 16:17:24,458][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-31 16:17:24,458][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-31 16:17:24,458][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-31 16:17:24,459][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-31 16:17:24,459][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-31 16:17:24,460][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-31 16:17:24,460][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-31 16:17:24,461][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-31 16:17:24,461][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-31 16:17:24,462][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-31 16:17:24,462][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-31 16:17:24,462][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-31 16:17:24,463][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-31 16:17:24,464][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-10-31 16:17:24,469][root][INFO] - Change the max_length to 255 for the sub2_tokenizer's truncation.
[2024-10-31 16:17:24,578][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-10-31 16:17:24,580][root][INFO] - Trainable params: 11397120 || all params: 136562688 || trainable: 8.35 %
[2024-10-31 16:17:24,760][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-10-31 16:17:27,735][root][INFO] - 

[2024-10-31 16:17:27,735][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-10-31 16:17:27,736][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-10-31 16:17:27,745][root][INFO] - vocab size              : 51200
[2024-10-31 16:17:27,745][root][INFO] - device                  : gpu
[2024-10-31 16:17:27,745][root][INFO] - random seed             : 1
[2024-10-31 16:17:27,745][root][INFO] - train data size         : 3672
[2024-10-31 16:17:27,746][root][INFO] - max epochs              : 10
[2024-10-31 16:17:27,746][root][INFO] - total steps             : 4590
[2024-10-31 16:17:27,746][root][INFO] - warmup steps            : 459
[2024-10-31 16:17:27,746][root][INFO] - batch size              : 8
[2024-10-31 16:17:27,746][root][INFO] - accumulation steps      : 1
[2024-10-31 16:17:27,746][root][INFO] - optimizer               : adamwscale
[2024-10-31 16:17:27,746][root][INFO] - lr_scheduler            : cosine
[2024-10-31 16:17:27,746][root][INFO] - learning rate           : 0.01
[2024-10-31 16:17:27,746][root][INFO] - max length              : 256

[2024-10-31 16:17:27,746][root][INFO] - LoRA Configuration
[2024-10-31 16:17:27,746][root][INFO] - ㄴ r                    : 32
[2024-10-31 16:17:27,746][root][INFO] - ㄴ alpha                : 128
[2024-10-31 16:17:27,747][root][INFO] - ㄴ dropout              : 0.03

[2024-10-31 16:17:27,747][root][INFO] - SUB2 Configuration
[2024-10-31 16:17:27,747][root][INFO] - ㄴ tok_type             : jamo_var
[2024-10-31 16:17:27,747][root][INFO] - ㄴ hidden_dim           : 768
[2024-10-31 16:17:27,747][root][INFO] - ㄴ sub2_max_length     : 255
[2024-10-31 16:17:27,747][root][INFO] - ㄴ embedding_norm       : False
[2024-10-31 16:17:27,747][root][INFO] - ㄴ do_combination       : False
[2024-10-31 16:17:27,748][root][INFO] - ㄴ reducer              : linear_pool

[2024-10-31 16:17:27,748][root][INFO] - LoRA in SUB2 Configuration
[2024-10-31 16:17:27,748][root][INFO] - ㄴ r                : 32
[2024-10-31 16:17:27,748][root][INFO] - ㄴ alpha            : 128
[2024-10-31 16:17:27,748][root][INFO] - ㄴ dropout          : 0.03

[2024-10-31 16:17:27,748][root][INFO] - 

[2024-10-31 16:17:27,748][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_256_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 16:17:27,748][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_256_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt
[2024-10-31 16:17:27,748][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_256_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb
[2024-10-31 16:17:27,748][root][INFO] - * tb interval   : 100

[2024-10-31 16:17:27,748][root][INFO] - 

[2024-10-31 16:17:27,748][root][INFO] - Start the Training !
[2024-10-31 16:17:27,752][root][INFO] - 
[1/ 10 Epoch]
[2024-10-31 17:01:46,990][root][INFO] - 

[2024-10-31 17:01:46,991][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-31 17:01:46,991][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 17:01:46,991][root][INFO] - 

[2024-10-31 17:01:46,991][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-31 17:01:57,110][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-31 17:01:57,111][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-31 17:01:57,111][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-31 17:01:57,112][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-31 17:01:57,112][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-31 17:01:57,113][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-31 17:01:57,113][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-31 17:01:57,114][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-31 17:01:57,115][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-31 17:01:57,116][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-31 17:01:57,116][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-31 17:01:57,117][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-31 17:01:57,118][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-31 17:01:57,119][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-31 17:01:57,120][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-31 17:01:57,121][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-31 17:01:57,121][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-31 17:01:57,122][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-31 17:01:57,123][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-31 17:01:57,124][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-31 17:01:57,125][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-31 17:01:57,126][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-31 17:01:57,127][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-31 17:01:57,127][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-31 17:01:57,130][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-10-31 17:01:57,137][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-10-31 17:01:57,323][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-10-31 17:01:57,325][root][INFO] - Trainable params: 15525888 || all params: 140691456 || trainable: 11.04 %
[2024-10-31 17:01:57,513][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-10-31 17:02:00,298][root][INFO] - 

[2024-10-31 17:02:00,298][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-10-31 17:02:00,299][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-10-31 17:02:00,309][root][INFO] - vocab size              : 51200
[2024-10-31 17:02:00,310][root][INFO] - device                  : gpu
[2024-10-31 17:02:00,310][root][INFO] - random seed             : 1
[2024-10-31 17:02:00,310][root][INFO] - train data size         : 3672
[2024-10-31 17:02:00,310][root][INFO] - max epochs              : 10
[2024-10-31 17:02:00,310][root][INFO] - total steps             : 4590
[2024-10-31 17:02:00,310][root][INFO] - warmup steps            : 459
[2024-10-31 17:02:00,310][root][INFO] - batch size              : 8
[2024-10-31 17:02:00,310][root][INFO] - accumulation steps      : 1
[2024-10-31 17:02:00,310][root][INFO] - optimizer               : adamwscale
[2024-10-31 17:02:00,311][root][INFO] - lr_scheduler            : cosine
[2024-10-31 17:02:00,311][root][INFO] - learning rate           : 0.01
[2024-10-31 17:02:00,311][root][INFO] - max length              : 256

[2024-10-31 17:02:00,311][root][INFO] - LoRA Configuration
[2024-10-31 17:02:00,311][root][INFO] - ㄴ r                    : 32
[2024-10-31 17:02:00,311][root][INFO] - ㄴ alpha                : 128
[2024-10-31 17:02:00,311][root][INFO] - ㄴ dropout              : 0.03

[2024-10-31 17:02:00,311][root][INFO] - SUB2 Configuration
[2024-10-31 17:02:00,311][root][INFO] - ㄴ tok_type             : jamo_var
[2024-10-31 17:02:00,311][root][INFO] - ㄴ hidden_dim           : 768
[2024-10-31 17:02:00,311][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-10-31 17:02:00,312][root][INFO] - ㄴ embedding_norm       : False
[2024-10-31 17:02:00,312][root][INFO] - ㄴ do_combination       : False
[2024-10-31 17:02:00,312][root][INFO] - ㄴ reducer              : linear_pool

[2024-10-31 17:02:00,312][root][INFO] - LoRA in SUB2 Configuration
[2024-10-31 17:02:00,312][root][INFO] - ㄴ r                : 32
[2024-10-31 17:02:00,312][root][INFO] - ㄴ alpha            : 128
[2024-10-31 17:02:00,312][root][INFO] - ㄴ dropout          : 0.03

[2024-10-31 17:02:00,312][root][INFO] - 

[2024-10-31 17:02:00,312][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 17:02:00,312][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt
[2024-10-31 17:02:00,312][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb
[2024-10-31 17:02:00,313][root][INFO] - * tb interval   : 100

[2024-10-31 17:02:00,313][root][INFO] - 

[2024-10-31 17:02:00,313][root][INFO] - Start the Training !
[2024-10-31 17:02:00,316][root][INFO] - 
[1/ 10 Epoch]
[2024-10-31 17:03:12,752][root][INFO] - 

[2024-10-31 17:03:12,752][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-31 17:03:12,752][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_COPA/lora_sub2_jamo_var_2048_red-linear_pool_256t_16b_1s_0.01lr_1rs
[2024-10-31 17:03:12,753][root][INFO] - 

[2024-10-31 17:03:12,753][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_COPA'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 16, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 40, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_COPA/lora_sub2_jamo_var_2048_red-linear_pool_256t_16b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_COPA/lora_sub2_jamo_var_2048_red-linear_pool_256t_16b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_COPA/lora_sub2_jamo_var_2048_red-linear_pool_256t_16b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-31 17:03:22,115][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-31 17:03:22,115][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-31 17:03:22,116][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-31 17:03:22,117][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-31 17:03:22,118][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-31 17:03:22,119][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-31 17:03:22,119][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-31 17:03:22,120][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-31 17:03:22,120][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-31 17:03:22,121][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-31 17:03:22,121][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-31 17:03:22,122][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-31 17:03:22,122][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-31 17:03:22,122][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-31 17:03:22,123][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-31 17:03:22,124][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-31 17:03:22,125][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-31 17:03:22,125][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-31 17:03:22,126][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-31 17:03:22,127][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-31 17:03:22,128][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-31 17:03:22,129][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-31 17:03:22,129][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-31 17:03:22,130][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-31 17:03:22,132][root][INFO] - Trainable params: 1769472 || all params: 126935041 || trainable: 1.39 %
[2024-10-31 17:03:22,138][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-10-31 17:03:22,138][srcs.gpt2_tokenizers][INFO] - Assigning [CLS] to the cls_token key of the tokenizer
[2024-10-31 17:03:22,310][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-10-31 17:03:22,312][root][INFO] - Trainable params: 15526656 || all params: 140692225 || trainable: 11.04 %
[2024-10-31 17:03:22,495][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-10-31 17:03:24,807][root][INFO] - 

[2024-10-31 17:03:24,807][root][INFO] - ========== Fine-tuning on KB_COPA ==========
[2024-10-31 17:03:24,807][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-10-31 17:03:24,819][root][INFO] - vocab size              : 51201
[2024-10-31 17:03:24,819][root][INFO] - device                  : gpu
[2024-10-31 17:03:24,819][root][INFO] - random seed             : 1
[2024-10-31 17:03:24,819][root][INFO] - train data size         : 3088
[2024-10-31 17:03:24,819][root][INFO] - max epochs              : 15
[2024-10-31 17:03:24,819][root][INFO] - total steps             : 2895
[2024-10-31 17:03:24,819][root][INFO] - warmup steps            : 290
[2024-10-31 17:03:24,820][root][INFO] - batch size              : 16
[2024-10-31 17:03:24,820][root][INFO] - accumulation steps      : 1
[2024-10-31 17:03:24,820][root][INFO] - optimizer               : adamwscale
[2024-10-31 17:03:24,820][root][INFO] - lr_scheduler            : cosine
[2024-10-31 17:03:24,820][root][INFO] - learning rate           : 0.01
[2024-10-31 17:03:24,820][root][INFO] - max length              : 256

[2024-10-31 17:03:24,820][root][INFO] - LoRA Configuration
[2024-10-31 17:03:24,820][root][INFO] - ㄴ r                    : 32
[2024-10-31 17:03:24,820][root][INFO] - ㄴ alpha                : 128
[2024-10-31 17:03:24,820][root][INFO] - ㄴ dropout              : 0.03

[2024-10-31 17:03:24,820][root][INFO] - SUB2 Configuration
[2024-10-31 17:03:24,820][root][INFO] - ㄴ tok_type             : jamo_var
[2024-10-31 17:03:24,821][root][INFO] - ㄴ hidden_dim           : 768
[2024-10-31 17:03:24,821][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-10-31 17:03:24,821][root][INFO] - ㄴ embedding_norm       : False
[2024-10-31 17:03:24,821][root][INFO] - ㄴ do_combination       : False
[2024-10-31 17:03:24,821][root][INFO] - ㄴ reducer              : linear_pool

[2024-10-31 17:03:24,821][root][INFO] - LoRA in SUB2 Configuration
[2024-10-31 17:03:24,821][root][INFO] - ㄴ r                : 32
[2024-10-31 17:03:24,821][root][INFO] - ㄴ alpha            : 128
[2024-10-31 17:03:24,821][root][INFO] - ㄴ dropout          : 0.03

[2024-10-31 17:03:24,821][root][INFO] - 

[2024-10-31 17:03:24,822][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_COPA/lora_sub2_jamo_var_2048_red-linear_pool_256t_16b_1s_0.01lr_1rs
[2024-10-31 17:03:24,822][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_COPA/lora_sub2_jamo_var_2048_red-linear_pool_256t_16b_1s_0.01lr_1rs/ckpt
[2024-10-31 17:03:24,822][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_COPA/lora_sub2_jamo_var_2048_red-linear_pool_256t_16b_1s_0.01lr_1rs/tb
[2024-10-31 17:03:24,822][root][INFO] - * tb interval   : 40

[2024-10-31 17:03:24,822][root][INFO] - 

[2024-10-31 17:03:24,822][root][INFO] - Start the Training !
[2024-10-31 17:03:24,825][root][INFO] - 
[1/ 15 Epoch]
[2024-10-31 17:03:59,774][root][INFO] - 

[2024-10-31 17:03:59,774][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-31 17:03:59,774][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_COPA/lora_sub2_jamo_var_2048_red-linear_pool_256t_16b_1s_0.01lr_1rs
[2024-10-31 17:03:59,774][root][INFO] - 

[2024-10-31 17:03:59,774][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': None, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_COPA'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 16, 'total_steps': -1, 'epochs': 15, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 40, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_COPA/lora_sub2_jamo_var_2048_red-linear_pool_256t_16b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_COPA/lora_sub2_jamo_var_2048_red-linear_pool_256t_16b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_COPA/lora_sub2_jamo_var_2048_red-linear_pool_256t_16b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-31 17:04:07,783][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:07,784][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:07,785][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:07,786][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:07,787][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:07,788][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:07,789][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:07,790][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:07,791][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:07,791][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:07,792][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:07,793][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:07,794][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:07,795][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:07,796][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:07,797][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:07,798][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:07,799][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:07,800][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:07,801][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:07,802][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:07,803][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:07,804][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:07,805][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:07,807][root][INFO] - Trainable params: 1769472 || all params: 126935041 || trainable: 1.39 %
[2024-10-31 17:04:07,812][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-10-31 17:04:07,813][srcs.gpt2_tokenizers][INFO] - Assigning [CLS] to the cls_token key of the tokenizer
[2024-10-31 17:04:07,988][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-10-31 17:04:07,991][root][INFO] - Trainable params: 15526656 || all params: 140692225 || trainable: 11.04 %
[2024-10-31 17:04:08,176][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-10-31 17:04:10,941][root][INFO] - 

[2024-10-31 17:04:10,941][root][INFO] - ========== Fine-tuning on KB_COPA ==========
[2024-10-31 17:04:10,941][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-10-31 17:04:10,953][root][INFO] - vocab size              : 51201
[2024-10-31 17:04:10,953][root][INFO] - device                  : gpu
[2024-10-31 17:04:10,953][root][INFO] - random seed             : 1
[2024-10-31 17:04:10,954][root][INFO] - train data size         : 3088
[2024-10-31 17:04:10,954][root][INFO] - max epochs              : 15
[2024-10-31 17:04:10,954][root][INFO] - total steps             : 2895
[2024-10-31 17:04:10,954][root][INFO] - warmup steps            : 290
[2024-10-31 17:04:10,954][root][INFO] - batch size              : 16
[2024-10-31 17:04:10,954][root][INFO] - accumulation steps      : 1
[2024-10-31 17:04:10,954][root][INFO] - optimizer               : adamwscale
[2024-10-31 17:04:10,954][root][INFO] - lr_scheduler            : cosine
[2024-10-31 17:04:10,954][root][INFO] - learning rate           : 0.01
[2024-10-31 17:04:10,954][root][INFO] - max length              : 256

[2024-10-31 17:04:10,954][root][INFO] - LoRA Configuration
[2024-10-31 17:04:10,955][root][INFO] - ㄴ r                    : 32
[2024-10-31 17:04:10,955][root][INFO] - ㄴ alpha                : 128
[2024-10-31 17:04:10,955][root][INFO] - ㄴ dropout              : 0.03

[2024-10-31 17:04:10,955][root][INFO] - SUB2 Configuration
[2024-10-31 17:04:10,955][root][INFO] - ㄴ tok_type             : jamo_var
[2024-10-31 17:04:10,955][root][INFO] - ㄴ hidden_dim           : 768
[2024-10-31 17:04:10,955][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-10-31 17:04:10,955][root][INFO] - ㄴ embedding_norm       : False
[2024-10-31 17:04:10,955][root][INFO] - ㄴ do_combination       : False
[2024-10-31 17:04:10,955][root][INFO] - ㄴ reducer              : linear_pool

[2024-10-31 17:04:10,956][root][INFO] - LoRA in SUB2 Configuration
[2024-10-31 17:04:10,956][root][INFO] - ㄴ r                : 32
[2024-10-31 17:04:10,956][root][INFO] - ㄴ alpha            : 128
[2024-10-31 17:04:10,956][root][INFO] - ㄴ dropout          : 0.03

[2024-10-31 17:04:10,956][root][INFO] - 

[2024-10-31 17:04:10,956][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_COPA/lora_sub2_jamo_var_2048_red-linear_pool_256t_16b_1s_0.01lr_1rs
[2024-10-31 17:04:10,956][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_COPA/lora_sub2_jamo_var_2048_red-linear_pool_256t_16b_1s_0.01lr_1rs/ckpt
[2024-10-31 17:04:10,956][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_COPA/lora_sub2_jamo_var_2048_red-linear_pool_256t_16b_1s_0.01lr_1rs/tb
[2024-10-31 17:04:10,956][root][INFO] - * tb interval   : 40

[2024-10-31 17:04:10,956][root][INFO] - 

[2024-10-31 17:04:10,956][root][INFO] - Start the Training !
[2024-10-31 17:04:10,960][root][INFO] - 
[1/ 15 Epoch]
[2024-10-31 17:04:39,482][root][INFO] - 

[2024-10-31 17:04:39,482][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-31 17:04:39,482][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 17:04:39,482][root][INFO] - 

[2024-10-31 17:04:39,482][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-31 17:04:47,085][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:47,086][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:47,087][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:47,088][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:47,088][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:47,089][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:47,089][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:47,090][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:47,090][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:47,091][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:47,092][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:47,092][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:47,093][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:47,093][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:47,094][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:47,094][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:47,095][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:47,095][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:47,096][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:47,097][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:47,097][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:47,098][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:47,098][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-31 17:04:47,099][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-31 17:04:47,101][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-10-31 17:04:47,106][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-10-31 17:04:47,308][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-10-31 17:04:47,310][root][INFO] - Trainable params: 15525888 || all params: 140691456 || trainable: 11.04 %
[2024-10-31 17:04:47,495][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-10-31 17:04:49,651][root][INFO] - 

[2024-10-31 17:04:49,651][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-10-31 17:04:49,651][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-10-31 17:04:49,659][root][INFO] - vocab size              : 51200
[2024-10-31 17:04:49,659][root][INFO] - device                  : gpu
[2024-10-31 17:04:49,659][root][INFO] - random seed             : 1
[2024-10-31 17:04:49,659][root][INFO] - train data size         : 3672
[2024-10-31 17:04:49,659][root][INFO] - max epochs              : 10
[2024-10-31 17:04:49,659][root][INFO] - total steps             : 4590
[2024-10-31 17:04:49,659][root][INFO] - warmup steps            : 459
[2024-10-31 17:04:49,660][root][INFO] - batch size              : 8
[2024-10-31 17:04:49,660][root][INFO] - accumulation steps      : 1
[2024-10-31 17:04:49,660][root][INFO] - optimizer               : adamwscale
[2024-10-31 17:04:49,660][root][INFO] - lr_scheduler            : cosine
[2024-10-31 17:04:49,660][root][INFO] - learning rate           : 0.01
[2024-10-31 17:04:49,660][root][INFO] - max length              : 256

[2024-10-31 17:04:49,660][root][INFO] - LoRA Configuration
[2024-10-31 17:04:49,660][root][INFO] - ㄴ r                    : 32
[2024-10-31 17:04:49,660][root][INFO] - ㄴ alpha                : 128
[2024-10-31 17:04:49,660][root][INFO] - ㄴ dropout              : 0.03

[2024-10-31 17:04:49,660][root][INFO] - SUB2 Configuration
[2024-10-31 17:04:49,661][root][INFO] - ㄴ tok_type             : jamo_var
[2024-10-31 17:04:49,661][root][INFO] - ㄴ hidden_dim           : 768
[2024-10-31 17:04:49,661][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-10-31 17:04:49,661][root][INFO] - ㄴ embedding_norm       : False
[2024-10-31 17:04:49,661][root][INFO] - ㄴ do_combination       : False
[2024-10-31 17:04:49,661][root][INFO] - ㄴ reducer              : linear_pool

[2024-10-31 17:04:49,661][root][INFO] - LoRA in SUB2 Configuration
[2024-10-31 17:04:49,661][root][INFO] - ㄴ r                : 32
[2024-10-31 17:04:49,661][root][INFO] - ㄴ alpha            : 128
[2024-10-31 17:04:49,661][root][INFO] - ㄴ dropout          : 0.03

[2024-10-31 17:04:49,662][root][INFO] - 

[2024-10-31 17:04:49,662][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 17:04:49,662][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt
[2024-10-31 17:04:49,662][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb
[2024-10-31 17:04:49,662][root][INFO] - * tb interval   : 100

[2024-10-31 17:04:49,662][root][INFO] - 

[2024-10-31 17:04:49,662][root][INFO] - Start the Training !
[2024-10-31 17:04:49,665][root][INFO] - 
[1/ 10 Epoch]
[2024-10-31 17:05:08,506][root][INFO] - Step: 100/4590  |  Loss: 0.7650  |  Score: 49.25 [%]  |  Seq Length: 256.0
[2024-10-31 17:05:23,069][root][INFO] - Step: 200/4590  |  Loss: 0.7869  |  Score: 51.88 [%]  |  Seq Length: 256.0
[2024-10-31 17:05:38,791][root][INFO] - Step: 300/4590  |  Loss: 0.8217  |  Score: 54.62 [%]  |  Seq Length: 256.0
[2024-10-31 17:05:54,202][root][INFO] - Step: 400/4590  |  Loss: 0.7379  |  Score: 47.00 [%]  |  Seq Length: 256.0
[2024-10-31 17:06:03,711][root][INFO] - Step: 459/4590  |  Loss: 0.7151  |  Score: 49.15 [%]  |  Seq Length: 256.0
[2024-10-31 17:06:08,867][root][INFO] - ########################  DEV REPORT #EP1  ########################
[2024-10-31 17:06:08,868][root][INFO] - Score: 53.27 [%]  |  Evaluation Time: 5.15 [s]
[2024-10-31 17:06:21,141][root][INFO] - ########################  TEST REPORT #EP1  ########################
[2024-10-31 17:06:21,141][root][INFO] - Score: 50.14 [%]  |  Evaluation Time: 12.27 [s]
[2024-10-31 17:06:21,143][root][INFO] - 
Save new Best Score (Epoch: 1)
[2024-10-31 17:06:21,144][root][INFO] - 
[2/ 10 Epoch]
[2024-10-31 17:06:27,480][root][INFO] - Step: 500/4590  |  Loss: 0.7168  |  Score: 44.82 [%]  |  Seq Length: 256.0
[2024-10-31 17:06:42,803][root][INFO] - Step: 600/4590  |  Loss: 0.7057  |  Score: 49.75 [%]  |  Seq Length: 256.0
[2024-10-31 17:06:57,994][root][INFO] - Step: 700/4590  |  Loss: 0.7068  |  Score: 50.00 [%]  |  Seq Length: 256.0
[2024-10-31 17:07:14,325][root][INFO] - Step: 800/4590  |  Loss: 0.7115  |  Score: 49.25 [%]  |  Seq Length: 256.0
[2024-10-31 17:07:28,264][root][INFO] - Step: 900/4590  |  Loss: 0.7373  |  Score: 48.00 [%]  |  Seq Length: 256.0
[2024-10-31 17:07:31,142][root][INFO] - Step: 918/4590  |  Loss: 0.7054  |  Score: 50.00 [%]  |  Seq Length: 256.0
[2024-10-31 17:07:37,541][root][INFO] - ########################  DEV REPORT #EP2  ########################
[2024-10-31 17:07:37,541][root][INFO] - Score: 47.02 [%]  |  Evaluation Time: 6.40 [s]
[2024-10-31 17:07:48,580][root][INFO] - ########################  TEST REPORT #EP2  ########################
[2024-10-31 17:07:48,580][root][INFO] - Score: 48.65 [%]  |  Evaluation Time: 11.04 [s]
[2024-10-31 17:07:48,583][root][INFO] - 
[3/ 10 Epoch]
[2024-10-31 17:08:02,126][root][INFO] - Step: 1000/4590  |  Loss: 0.7197  |  Score: 51.52 [%]  |  Seq Length: 256.0
[2024-10-31 17:08:16,948][root][INFO] - Step: 1100/4590  |  Loss: 0.6974  |  Score: 51.62 [%]  |  Seq Length: 256.0
[2024-10-31 17:08:33,886][root][INFO] - Step: 1200/4590  |  Loss: 0.7174  |  Score: 48.75 [%]  |  Seq Length: 256.0
[2024-10-31 17:08:48,825][root][INFO] - Step: 1300/4590  |  Loss: 0.7080  |  Score: 48.38 [%]  |  Seq Length: 256.0
[2024-10-31 17:09:02,393][root][INFO] - Step: 1377/4590  |  Loss: 0.7038  |  Score: 49.03 [%]  |  Seq Length: 256.0
[2024-10-31 17:09:07,480][root][INFO] - ########################  DEV REPORT #EP3  ########################
[2024-10-31 17:09:07,480][root][INFO] - Score: 46.59 [%]  |  Evaluation Time: 5.08 [s]
[2024-10-31 17:09:18,627][root][INFO] - ########################  TEST REPORT #EP3  ########################
[2024-10-31 17:09:18,627][root][INFO] - Score: 49.29 [%]  |  Evaluation Time: 11.14 [s]
[2024-10-31 17:09:18,630][root][INFO] - 
[4/ 10 Epoch]
[2024-10-31 17:09:23,114][root][INFO] - Step: 1400/4590  |  Loss: 0.7129  |  Score: 46.74 [%]  |  Seq Length: 256.0
[2024-10-31 17:09:37,101][root][INFO] - Step: 1500/4590  |  Loss: 0.7058  |  Score: 50.12 [%]  |  Seq Length: 256.0
[2024-10-31 17:09:54,221][root][INFO] - Step: 1600/4590  |  Loss: 0.7138  |  Score: 49.50 [%]  |  Seq Length: 256.0
[2024-10-31 17:10:08,001][root][INFO] - Step: 1700/4590  |  Loss: 0.6988  |  Score: 52.12 [%]  |  Seq Length: 256.0
[2024-10-31 17:10:24,363][root][INFO] - Step: 1800/4590  |  Loss: 0.7020  |  Score: 50.62 [%]  |  Seq Length: 256.0
[2024-10-31 17:10:29,310][root][INFO] - Step: 1836/4590  |  Loss: 0.6980  |  Score: 49.65 [%]  |  Seq Length: 256.0
[2024-10-31 17:10:35,073][root][INFO] - ########################  DEV REPORT #EP4  ########################
[2024-10-31 17:10:35,073][root][INFO] - Score: 53.55 [%]  |  Evaluation Time: 5.76 [s]
[2024-10-31 17:10:47,186][root][INFO] - ########################  TEST REPORT #EP4  ########################
[2024-10-31 17:10:47,187][root][INFO] - Score: 48.93 [%]  |  Evaluation Time: 12.11 [s]
[2024-10-31 17:10:47,189][root][INFO] - 
[5/ 10 Epoch]
[2024-10-31 17:10:55,655][root][INFO] - Step: 1900/4590  |  Loss: 0.7243  |  Score: 51.37 [%]  |  Seq Length: 256.0
[2024-10-31 17:11:12,754][root][INFO] - Step: 2000/4590  |  Loss: 0.7180  |  Score: 52.00 [%]  |  Seq Length: 256.0
[2024-10-31 17:11:26,502][root][INFO] - Step: 2100/4590  |  Loss: 0.7297  |  Score: 51.00 [%]  |  Seq Length: 256.0
[2024-10-31 17:11:42,786][root][INFO] - Step: 2200/4590  |  Loss: 0.7047  |  Score: 52.12 [%]  |  Seq Length: 256.0
[2024-10-31 17:11:56,522][root][INFO] - Step: 2295/4590  |  Loss: 0.6960  |  Score: 51.97 [%]  |  Seq Length: 256.0
[2024-10-31 17:12:02,946][root][INFO] - ########################  DEV REPORT #EP5  ########################
[2024-10-31 17:12:02,946][root][INFO] - Score: 53.41 [%]  |  Evaluation Time: 6.42 [s]
[2024-10-31 17:12:13,064][root][INFO] - ########################  TEST REPORT #EP5  ########################
[2024-10-31 17:12:13,064][root][INFO] - Score: 49.79 [%]  |  Evaluation Time: 10.11 [s]
[2024-10-31 17:12:13,067][root][INFO] - 
[6/ 10 Epoch]
[2024-10-31 17:12:14,070][root][INFO] - Step: 2300/4590  |  Loss: 0.7352  |  Score: 47.50 [%]  |  Seq Length: 256.0
[2024-10-31 17:12:30,988][root][INFO] - Step: 2400/4590  |  Loss: 0.7043  |  Score: 50.75 [%]  |  Seq Length: 256.0
[2024-10-31 17:12:43,554][root][INFO] - Step: 2500/4590  |  Loss: 0.6985  |  Score: 50.62 [%]  |  Seq Length: 256.0
[2024-10-31 17:12:56,730][root][INFO] - Step: 2600/4590  |  Loss: 0.6963  |  Score: 48.62 [%]  |  Seq Length: 256.0
[2024-10-31 17:13:11,162][root][INFO] - Step: 2700/4590  |  Loss: 0.7090  |  Score: 50.38 [%]  |  Seq Length: 256.0
[2024-10-31 17:13:20,451][root][INFO] - Step: 2754/4590  |  Loss: 0.7074  |  Score: 51.85 [%]  |  Seq Length: 256.0
[2024-10-31 17:13:25,437][root][INFO] - ########################  DEV REPORT #EP6  ########################
[2024-10-31 17:13:25,437][root][INFO] - Score: 46.59 [%]  |  Evaluation Time: 4.98 [s]
[2024-10-31 17:13:36,380][root][INFO] - ########################  TEST REPORT #EP6  ########################
[2024-10-31 17:13:36,380][root][INFO] - Score: 50.14 [%]  |  Evaluation Time: 10.94 [s]
[2024-10-31 17:13:36,383][root][INFO] - 
[7/ 10 Epoch]
[2024-10-31 17:13:44,580][root][INFO] - Step: 2800/4590  |  Loss: 0.7070  |  Score: 44.02 [%]  |  Seq Length: 256.0
[2024-10-31 17:13:57,574][root][INFO] - Step: 2900/4590  |  Loss: 0.7025  |  Score: 48.50 [%]  |  Seq Length: 256.0
[2024-10-31 17:14:14,031][root][INFO] - Step: 3000/4590  |  Loss: 0.6937  |  Score: 52.88 [%]  |  Seq Length: 256.0
[2024-10-31 17:15:47,964][root][INFO] - 

[2024-10-31 17:15:47,964][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-31 17:15:47,964][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_256t_8b_1s_0.01lr_1rs
[2024-10-31 17:15:47,964][root][INFO] - 

[2024-10-31 17:15:47,965][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_256t_8b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_256t_8b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_256t_8b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-31 17:15:55,332][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-31 17:15:55,333][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-31 17:15:55,334][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-31 17:15:55,335][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-31 17:15:55,335][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-31 17:15:55,336][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-31 17:15:55,336][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-31 17:15:55,337][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-31 17:15:55,337][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-31 17:15:55,338][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-31 17:15:55,338][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-31 17:15:55,339][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-31 17:15:55,340][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-31 17:15:55,340][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-31 17:15:55,341][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-31 17:15:55,341][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-31 17:15:55,342][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-31 17:15:55,342][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-31 17:15:55,343][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-31 17:15:55,343][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-31 17:15:55,344][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-31 17:15:55,345][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-31 17:15:55,345][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-31 17:15:55,346][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-31 17:15:55,348][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-10-31 17:15:55,352][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-10-31 17:15:55,463][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-10-31 17:15:55,466][root][INFO] - Trainable params: 11331072 || all params: 136496640 || trainable: 8.30 %
[2024-10-31 17:15:55,644][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-10-31 17:15:58,175][root][INFO] - 

[2024-10-31 17:15:58,175][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-10-31 17:15:58,175][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-10-31 17:15:58,182][root][INFO] - vocab size              : 51200
[2024-10-31 17:15:58,182][root][INFO] - device                  : gpu
[2024-10-31 17:15:58,183][root][INFO] - random seed             : 1
[2024-10-31 17:15:58,183][root][INFO] - train data size         : 3672
[2024-10-31 17:15:58,183][root][INFO] - max epochs              : 10
[2024-10-31 17:15:58,183][root][INFO] - total steps             : 4590
[2024-10-31 17:15:58,183][root][INFO] - warmup steps            : 459
[2024-10-31 17:15:58,183][root][INFO] - batch size              : 8
[2024-10-31 17:15:58,183][root][INFO] - accumulation steps      : 1
[2024-10-31 17:15:58,183][root][INFO] - optimizer               : adamwscale
[2024-10-31 17:15:58,183][root][INFO] - lr_scheduler            : cosine
[2024-10-31 17:15:58,183][root][INFO] - learning rate           : 0.01
[2024-10-31 17:15:58,183][root][INFO] - max length              : 256

[2024-10-31 17:15:58,184][root][INFO] - LoRA Configuration
[2024-10-31 17:15:58,184][root][INFO] - ㄴ r                    : 32
[2024-10-31 17:15:58,184][root][INFO] - ㄴ alpha                : 128
[2024-10-31 17:15:58,184][root][INFO] - ㄴ dropout              : 0.03

[2024-10-31 17:15:58,184][root][INFO] - SUB2 Configuration
[2024-10-31 17:15:58,184][root][INFO] - ㄴ tok_type             : jamo_var
[2024-10-31 17:15:58,184][root][INFO] - ㄴ hidden_dim           : 768
[2024-10-31 17:15:58,184][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-10-31 17:15:58,184][root][INFO] - ㄴ embedding_norm       : False
[2024-10-31 17:15:58,184][root][INFO] - ㄴ do_combination       : False
[2024-10-31 17:15:58,184][root][INFO] - ㄴ reducer              : linear

[2024-10-31 17:15:58,185][root][INFO] - LoRA in SUB2 Configuration
[2024-10-31 17:15:58,185][root][INFO] - ㄴ r                : 32
[2024-10-31 17:15:58,185][root][INFO] - ㄴ alpha            : 128
[2024-10-31 17:15:58,185][root][INFO] - ㄴ dropout          : 0.03

[2024-10-31 17:15:58,185][root][INFO] - 

[2024-10-31 17:15:58,185][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_256t_8b_1s_0.01lr_1rs
[2024-10-31 17:15:58,185][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_256t_8b_1s_0.01lr_1rs/ckpt
[2024-10-31 17:15:58,185][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_256t_8b_1s_0.01lr_1rs/tb
[2024-10-31 17:15:58,185][root][INFO] - * tb interval   : 100

[2024-10-31 17:15:58,185][root][INFO] - 

[2024-10-31 17:15:58,185][root][INFO] - Start the Training !
[2024-10-31 17:15:58,188][root][INFO] - 
[1/ 10 Epoch]
[2024-10-31 17:16:13,465][root][INFO] - Step: 100/4590  |  Loss: 0.7517  |  Score: 52.62 [%]  |  Seq Length: 256.0
[2024-10-31 17:16:30,058][root][INFO] - Step: 200/4590  |  Loss: 0.7210  |  Score: 54.00 [%]  |  Seq Length: 256.0
[2024-10-31 17:16:43,183][root][INFO] - Step: 300/4590  |  Loss: 0.7081  |  Score: 53.37 [%]  |  Seq Length: 256.0
[2024-10-31 17:16:59,279][root][INFO] - Step: 400/4590  |  Loss: 0.7128  |  Score: 52.25 [%]  |  Seq Length: 256.0
[2024-10-31 17:17:06,825][root][INFO] - Step: 459/4590  |  Loss: 0.7250  |  Score: 51.27 [%]  |  Seq Length: 256.0
[2024-10-31 17:17:12,424][root][INFO] - ########################  DEV REPORT #EP1  ########################
[2024-10-31 17:17:12,424][root][INFO] - Score: 54.26 [%]  |  Evaluation Time: 5.60 [s]
[2024-10-31 17:17:24,201][root][INFO] - ########################  TEST REPORT #EP1  ########################
[2024-10-31 17:17:24,201][root][INFO] - Score: 50.36 [%]  |  Evaluation Time: 11.77 [s]
[2024-10-31 17:17:24,202][root][INFO] - 
Save new Best Score (Epoch: 1)
[2024-10-31 17:17:24,203][root][INFO] - 
[2/ 10 Epoch]
[2024-10-31 17:17:29,777][root][INFO] - Step: 500/4590  |  Loss: 0.6730  |  Score: 57.32 [%]  |  Seq Length: 256.0
[2024-10-31 17:17:44,985][root][INFO] - Step: 600/4590  |  Loss: 0.6979  |  Score: 57.38 [%]  |  Seq Length: 256.0
[2024-10-31 17:17:58,867][root][INFO] - Step: 700/4590  |  Loss: 0.7165  |  Score: 51.38 [%]  |  Seq Length: 256.0
[2024-10-31 17:18:14,427][root][INFO] - Step: 800/4590  |  Loss: 0.7242  |  Score: 49.38 [%]  |  Seq Length: 256.0
[2024-10-31 17:18:28,058][root][INFO] - Step: 900/4590  |  Loss: 0.7157  |  Score: 51.00 [%]  |  Seq Length: 256.0
[2024-10-31 17:18:30,143][root][INFO] - Step: 918/4590  |  Loss: 0.7094  |  Score: 46.53 [%]  |  Seq Length: 256.0
[2024-10-31 17:18:36,271][root][INFO] - ########################  DEV REPORT #EP2  ########################
[2024-10-31 17:18:36,271][root][INFO] - Score: 46.45 [%]  |  Evaluation Time: 6.13 [s]
[2024-10-31 17:18:47,428][root][INFO] - ########################  TEST REPORT #EP2  ########################
[2024-10-31 17:18:47,428][root][INFO] - Score: 50.21 [%]  |  Evaluation Time: 11.15 [s]
[2024-10-31 17:18:47,430][root][INFO] - 
[3/ 10 Epoch]
[2024-10-31 17:18:59,354][root][INFO] - Step: 1000/4590  |  Loss: 0.7260  |  Score: 48.93 [%]  |  Seq Length: 256.0
[2024-10-31 17:19:14,872][root][INFO] - Step: 1100/4590  |  Loss: 0.7098  |  Score: 50.50 [%]  |  Seq Length: 256.0
[2024-10-31 17:19:28,983][root][INFO] - Step: 1200/4590  |  Loss: 0.7070  |  Score: 48.75 [%]  |  Seq Length: 256.0
[2024-10-31 17:19:43,936][root][INFO] - Step: 1300/4590  |  Loss: 0.7016  |  Score: 49.62 [%]  |  Seq Length: 256.0
[2024-10-31 17:19:54,598][root][INFO] - Step: 1377/4590  |  Loss: 0.6974  |  Score: 53.41 [%]  |  Seq Length: 256.0
[2024-10-31 17:20:00,643][root][INFO] - ########################  DEV REPORT #EP3  ########################
[2024-10-31 17:20:00,643][root][INFO] - Score: 53.55 [%]  |  Evaluation Time: 6.04 [s]
[2024-10-31 17:20:11,016][root][INFO] - ########################  TEST REPORT #EP3  ########################
[2024-10-31 17:20:11,016][root][INFO] - Score: 49.86 [%]  |  Evaluation Time: 10.37 [s]
[2024-10-31 17:20:11,018][root][INFO] - 
[4/ 10 Epoch]
[2024-10-31 17:20:13,834][root][INFO] - Step: 1400/4590  |  Loss: 0.7203  |  Score: 52.17 [%]  |  Seq Length: 256.0
[2024-10-31 17:20:30,611][root][INFO] - Step: 1500/4590  |  Loss: 0.7013  |  Score: 47.75 [%]  |  Seq Length: 256.0
[2024-10-31 17:20:43,384][root][INFO] - Step: 1600/4590  |  Loss: 0.7004  |  Score: 49.38 [%]  |  Seq Length: 256.0
[2024-10-31 17:21:00,045][root][INFO] - Step: 1700/4590  |  Loss: 0.6986  |  Score: 53.62 [%]  |  Seq Length: 256.0
[2024-10-31 17:21:13,593][root][INFO] - Step: 1800/4590  |  Loss: 0.7157  |  Score: 51.25 [%]  |  Seq Length: 256.0
[2024-10-31 17:21:19,769][root][INFO] - Step: 1836/4590  |  Loss: 0.7007  |  Score: 47.92 [%]  |  Seq Length: 256.0
[2024-10-31 17:21:25,789][root][INFO] - ########################  DEV REPORT #EP4  ########################
[2024-10-31 17:21:25,789][root][INFO] - Score: 46.59 [%]  |  Evaluation Time: 6.02 [s]
[2024-10-31 17:21:35,012][root][INFO] - ########################  TEST REPORT #EP4  ########################
[2024-10-31 17:21:35,012][root][INFO] - Score: 50.43 [%]  |  Evaluation Time: 9.22 [s]
[2024-10-31 17:21:35,014][root][INFO] - 
[5/ 10 Epoch]
[2024-10-31 17:21:46,437][root][INFO] - Step: 1900/4590  |  Loss: 0.7066  |  Score: 52.34 [%]  |  Seq Length: 256.0
[2024-10-31 17:22:00,423][root][INFO] - Step: 2000/4590  |  Loss: 0.7061  |  Score: 49.12 [%]  |  Seq Length: 256.0
[2024-10-31 17:22:17,372][root][INFO] - Step: 2100/4590  |  Loss: 0.6926  |  Score: 54.12 [%]  |  Seq Length: 256.0
[2024-10-31 17:22:30,051][root][INFO] - Step: 2200/4590  |  Loss: 0.7003  |  Score: 53.12 [%]  |  Seq Length: 256.0
[2024-10-31 17:22:46,198][root][INFO] - Step: 2295/4590  |  Loss: 0.6960  |  Score: 52.63 [%]  |  Seq Length: 256.0
[2024-10-31 17:22:51,112][root][INFO] - ########################  DEV REPORT #EP5  ########################
[2024-10-31 17:22:51,112][root][INFO] - Score: 49.01 [%]  |  Evaluation Time: 4.91 [s]
[2024-10-31 17:23:02,606][root][INFO] - ########################  TEST REPORT #EP5  ########################
[2024-10-31 17:23:02,607][root][INFO] - Score: 51.14 [%]  |  Evaluation Time: 11.49 [s]
[2024-10-31 17:23:02,609][root][INFO] - 
[6/ 10 Epoch]
[2024-10-31 17:23:03,754][root][INFO] - Step: 2300/4590  |  Loss: 0.6489  |  Score: 65.00 [%]  |  Seq Length: 256.0
[2024-10-31 17:23:18,059][root][INFO] - Step: 2400/4590  |  Loss: 0.6957  |  Score: 53.87 [%]  |  Seq Length: 256.0
[2024-10-31 17:23:33,838][root][INFO] - Step: 2500/4590  |  Loss: 0.6969  |  Score: 52.62 [%]  |  Seq Length: 256.0
[2024-10-31 17:23:47,309][root][INFO] - Step: 2600/4590  |  Loss: 0.6961  |  Score: 51.00 [%]  |  Seq Length: 256.0
[2024-10-31 17:24:04,265][root][INFO] - Step: 2700/4590  |  Loss: 0.7044  |  Score: 49.62 [%]  |  Seq Length: 256.0
[2024-10-31 17:24:11,329][root][INFO] - Step: 2754/4590  |  Loss: 0.6948  |  Score: 54.17 [%]  |  Seq Length: 256.0
[2024-10-31 17:24:16,154][root][INFO] - ########################  DEV REPORT #EP6  ########################
[2024-10-31 17:24:16,154][root][INFO] - Score: 50.14 [%]  |  Evaluation Time: 4.82 [s]
[2024-10-31 17:24:28,029][root][INFO] - ########################  TEST REPORT #EP6  ########################
[2024-10-31 17:24:28,030][root][INFO] - Score: 53.62 [%]  |  Evaluation Time: 11.87 [s]
[2024-10-31 17:24:28,034][root][INFO] - 
[7/ 10 Epoch]
[2024-10-31 17:24:34,678][root][INFO] - Step: 2800/4590  |  Loss: 0.6672  |  Score: 60.05 [%]  |  Seq Length: 256.0
[2024-10-31 17:24:49,352][root][INFO] - Step: 2900/4590  |  Loss: 0.6901  |  Score: 55.88 [%]  |  Seq Length: 256.0
[2024-10-31 17:25:03,877][root][INFO] - Step: 3000/4590  |  Loss: 0.7144  |  Score: 52.38 [%]  |  Seq Length: 256.0
[2024-10-31 17:25:18,661][root][INFO] - Step: 3100/4590  |  Loss: 0.6774  |  Score: 60.12 [%]  |  Seq Length: 256.0
[2024-10-31 17:25:32,979][root][INFO] - Step: 3200/4590  |  Loss: 0.6846  |  Score: 58.00 [%]  |  Seq Length: 256.0
[2024-10-31 17:25:34,730][root][INFO] - Step: 3213/4590  |  Loss: 0.7299  |  Score: 47.12 [%]  |  Seq Length: 256.0
[2024-10-31 17:25:40,071][root][INFO] - ########################  DEV REPORT #EP7  ########################
[2024-10-31 17:25:40,071][root][INFO] - Score: 50.43 [%]  |  Evaluation Time: 5.34 [s]
[2024-10-31 17:26:29,695][root][INFO] - 

[2024-10-31 17:26:29,696][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-31 17:26:29,696][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 17:26:29,696][root][INFO] - 

[2024-10-31 17:26:29,696][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-31 17:26:36,848][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-31 17:26:36,849][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-31 17:26:36,849][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-31 17:26:36,850][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-31 17:26:36,850][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-31 17:26:36,851][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-31 17:26:36,851][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-31 17:26:36,852][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-31 17:26:36,852][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-31 17:26:36,853][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-31 17:26:36,853][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-31 17:26:36,854][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-31 17:26:36,855][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-31 17:26:36,855][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-31 17:26:36,856][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-31 17:26:36,856][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-31 17:26:36,857][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-31 17:26:36,857][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-31 17:26:36,858][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-31 17:26:36,858][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-31 17:26:36,859][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-31 17:26:36,860][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-31 17:26:36,860][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-31 17:26:36,861][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-31 17:26:36,863][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-10-31 17:26:36,869][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-10-31 17:26:37,036][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-10-31 17:28:34,564][root][INFO] - 

[2024-10-31 17:28:34,564][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-31 17:28:34,564][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 17:28:34,564][root][INFO] - 

[2024-10-31 17:28:34,564][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-31 17:28:42,041][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-31 17:28:42,042][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-31 17:28:42,042][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-31 17:28:42,043][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-31 17:28:42,043][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-31 17:28:42,044][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-31 17:28:42,044][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-31 17:28:42,045][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-31 17:28:42,045][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-31 17:28:42,045][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-31 17:28:42,046][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-31 17:28:42,046][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-31 17:28:42,047][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-31 17:28:42,047][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-31 17:28:42,048][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-31 17:28:42,048][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-31 17:28:42,049][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-31 17:28:42,049][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-31 17:28:42,050][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-31 17:28:42,050][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-31 17:28:42,051][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-31 17:28:42,051][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-31 17:28:42,052][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-31 17:28:42,052][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-31 17:28:42,054][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-10-31 17:28:42,058][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-10-31 17:28:42,225][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-10-31 17:38:52,853][root][INFO] - 

[2024-10-31 17:38:52,853][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-31 17:38:52,853][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 17:38:52,853][root][INFO] - 

[2024-10-31 17:38:52,853][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-31 17:39:00,355][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:00,356][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:00,357][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:00,357][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:00,357][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:00,358][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:00,358][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:00,359][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:00,359][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:00,360][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:00,360][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:00,361][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:00,361][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:00,362][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:00,362][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:00,363][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:00,363][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:00,363][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:00,364][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:00,364][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:00,365][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:00,365][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:00,366][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:00,366][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:00,368][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-10-31 17:39:00,372][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-10-31 17:39:00,537][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-10-31 17:39:00,541][root][INFO] - Trainable params: 15525888 || all params: 140691456 || trainable: 11.04 %
[2024-10-31 17:39:00,715][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-10-31 17:39:03,325][root][INFO] - 

[2024-10-31 17:39:03,325][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-10-31 17:39:03,325][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-10-31 17:39:03,332][root][INFO] - vocab size              : 51200
[2024-10-31 17:39:03,332][root][INFO] - device                  : gpu
[2024-10-31 17:39:03,333][root][INFO] - random seed             : 1
[2024-10-31 17:39:03,333][root][INFO] - train data size         : 3672
[2024-10-31 17:39:03,333][root][INFO] - max epochs              : 10
[2024-10-31 17:39:03,333][root][INFO] - total steps             : 4590
[2024-10-31 17:39:03,333][root][INFO] - warmup steps            : 459
[2024-10-31 17:39:03,333][root][INFO] - batch size              : 8
[2024-10-31 17:39:03,333][root][INFO] - accumulation steps      : 1
[2024-10-31 17:39:03,333][root][INFO] - optimizer               : adamwscale
[2024-10-31 17:39:03,333][root][INFO] - lr_scheduler            : cosine
[2024-10-31 17:39:03,333][root][INFO] - learning rate           : 0.01
[2024-10-31 17:39:03,333][root][INFO] - max length              : 256

[2024-10-31 17:39:03,333][root][INFO] - LoRA Configuration
[2024-10-31 17:39:03,334][root][INFO] - ㄴ r                    : 32
[2024-10-31 17:39:03,334][root][INFO] - ㄴ alpha                : 128
[2024-10-31 17:39:03,334][root][INFO] - ㄴ dropout              : 0.03

[2024-10-31 17:39:03,334][root][INFO] - SUB2 Configuration
[2024-10-31 17:39:03,334][root][INFO] - ㄴ tok_type             : jamo_var
[2024-10-31 17:39:03,334][root][INFO] - ㄴ hidden_dim           : 768
[2024-10-31 17:39:03,334][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-10-31 17:39:03,334][root][INFO] - ㄴ embedding_norm       : False
[2024-10-31 17:39:03,334][root][INFO] - ㄴ do_combination       : False
[2024-10-31 17:39:03,334][root][INFO] - ㄴ reducer              : linear_pool

[2024-10-31 17:39:03,335][root][INFO] - LoRA in SUB2 Configuration
[2024-10-31 17:39:03,335][root][INFO] - ㄴ r                : 32
[2024-10-31 17:39:03,335][root][INFO] - ㄴ alpha            : 128
[2024-10-31 17:39:03,335][root][INFO] - ㄴ dropout          : 0.03

[2024-10-31 17:39:03,335][root][INFO] - 

[2024-10-31 17:39:03,335][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 17:39:03,335][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt
[2024-10-31 17:39:03,335][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb
[2024-10-31 17:39:03,335][root][INFO] - * tb interval   : 100

[2024-10-31 17:39:03,335][root][INFO] - 

[2024-10-31 17:39:03,335][root][INFO] - Start the Training !
[2024-10-31 17:39:03,338][root][INFO] - 
[1/ 10 Epoch]
[2024-10-31 17:39:42,419][root][INFO] - 

[2024-10-31 17:39:42,419][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-31 17:39:42,419][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 17:39:42,419][root][INFO] - 

[2024-10-31 17:39:42,419][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-31 17:39:49,868][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:49,869][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:49,869][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:49,869][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:49,870][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:49,870][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:49,871][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:49,871][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:49,872][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:49,872][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:49,872][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:49,873][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:49,873][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:49,874][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:49,874][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:49,875][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:49,875][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:49,875][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:49,876][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:49,876][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:49,877][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:49,877][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:49,878][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-31 17:39:49,878][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-31 17:39:49,880][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-10-31 17:39:49,884][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-10-31 17:39:50,059][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-10-31 17:39:50,064][root][INFO] - Trainable params: 15525888 || all params: 140691456 || trainable: 11.04 %
[2024-10-31 17:39:50,246][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-10-31 17:39:52,909][root][INFO] - 

[2024-10-31 17:39:52,909][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-10-31 17:39:52,909][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-10-31 17:39:52,917][root][INFO] - vocab size              : 51200
[2024-10-31 17:39:52,917][root][INFO] - device                  : gpu
[2024-10-31 17:39:52,917][root][INFO] - random seed             : 1
[2024-10-31 17:39:52,918][root][INFO] - train data size         : 3672
[2024-10-31 17:39:52,918][root][INFO] - max epochs              : 10
[2024-10-31 17:39:52,918][root][INFO] - total steps             : 4590
[2024-10-31 17:39:52,918][root][INFO] - warmup steps            : 459
[2024-10-31 17:39:52,918][root][INFO] - batch size              : 8
[2024-10-31 17:39:52,918][root][INFO] - accumulation steps      : 1
[2024-10-31 17:39:52,918][root][INFO] - optimizer               : adamwscale
[2024-10-31 17:39:52,918][root][INFO] - lr_scheduler            : cosine
[2024-10-31 17:39:52,918][root][INFO] - learning rate           : 0.01
[2024-10-31 17:39:52,918][root][INFO] - max length              : 256

[2024-10-31 17:39:52,918][root][INFO] - LoRA Configuration
[2024-10-31 17:39:52,918][root][INFO] - ㄴ r                    : 32
[2024-10-31 17:39:52,919][root][INFO] - ㄴ alpha                : 128
[2024-10-31 17:39:52,919][root][INFO] - ㄴ dropout              : 0.03

[2024-10-31 17:39:52,919][root][INFO] - SUB2 Configuration
[2024-10-31 17:39:52,919][root][INFO] - ㄴ tok_type             : jamo_var
[2024-10-31 17:39:52,919][root][INFO] - ㄴ hidden_dim           : 768
[2024-10-31 17:39:52,919][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-10-31 17:39:52,919][root][INFO] - ㄴ embedding_norm       : False
[2024-10-31 17:39:52,919][root][INFO] - ㄴ do_combination       : False
[2024-10-31 17:39:52,919][root][INFO] - ㄴ reducer              : linear_pool

[2024-10-31 17:39:52,919][root][INFO] - LoRA in SUB2 Configuration
[2024-10-31 17:39:52,920][root][INFO] - ㄴ r                : 32
[2024-10-31 17:39:52,920][root][INFO] - ㄴ alpha            : 128
[2024-10-31 17:39:52,920][root][INFO] - ㄴ dropout          : 0.03

[2024-10-31 17:39:52,920][root][INFO] - 

[2024-10-31 17:39:52,920][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 17:39:52,920][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt
[2024-10-31 17:39:52,920][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb
[2024-10-31 17:39:52,920][root][INFO] - * tb interval   : 100

[2024-10-31 17:39:52,920][root][INFO] - 

[2024-10-31 17:39:52,920][root][INFO] - Start the Training !
[2024-10-31 17:39:52,923][root][INFO] - 
[1/ 10 Epoch]
[2024-10-31 17:40:11,183][root][INFO] - 

[2024-10-31 17:40:11,183][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-31 17:40:11,183][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 17:40:11,183][root][INFO] - 

[2024-10-31 17:40:11,184][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-31 17:40:18,512][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-31 17:40:18,513][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-31 17:40:18,514][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-31 17:40:18,514][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-31 17:40:18,515][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-31 17:40:18,515][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-31 17:40:18,516][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-31 17:40:18,516][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-31 17:40:18,517][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-31 17:40:18,517][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-31 17:40:18,517][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-31 17:40:18,518][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-31 17:40:18,518][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-31 17:40:18,519][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-31 17:40:18,519][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-31 17:40:18,520][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-31 17:40:18,520][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-31 17:40:18,521][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-31 17:40:18,521][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-31 17:40:18,522][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-31 17:40:18,522][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-31 17:40:18,523][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-31 17:40:18,523][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-31 17:40:18,524][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-31 17:40:18,526][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-10-31 17:40:18,530][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-10-31 17:40:18,699][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-10-31 17:40:18,703][root][INFO] - Trainable params: 15525888 || all params: 140691456 || trainable: 11.04 %
[2024-10-31 17:40:18,898][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-10-31 17:40:21,384][root][INFO] - 

[2024-10-31 17:40:21,385][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-10-31 17:40:21,385][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-10-31 17:40:21,392][root][INFO] - vocab size              : 51200
[2024-10-31 17:40:21,392][root][INFO] - device                  : gpu
[2024-10-31 17:40:21,392][root][INFO] - random seed             : 1
[2024-10-31 17:40:21,393][root][INFO] - train data size         : 3672
[2024-10-31 17:40:21,393][root][INFO] - max epochs              : 10
[2024-10-31 17:40:21,393][root][INFO] - total steps             : 4590
[2024-10-31 17:40:21,393][root][INFO] - warmup steps            : 459
[2024-10-31 17:40:21,393][root][INFO] - batch size              : 8
[2024-10-31 17:40:21,393][root][INFO] - accumulation steps      : 1
[2024-10-31 17:40:21,393][root][INFO] - optimizer               : adamwscale
[2024-10-31 17:40:21,393][root][INFO] - lr_scheduler            : cosine
[2024-10-31 17:40:21,393][root][INFO] - learning rate           : 0.01
[2024-10-31 17:40:21,393][root][INFO] - max length              : 256

[2024-10-31 17:40:21,394][root][INFO] - LoRA Configuration
[2024-10-31 17:40:21,394][root][INFO] - ㄴ r                    : 32
[2024-10-31 17:40:21,394][root][INFO] - ㄴ alpha                : 128
[2024-10-31 17:40:21,394][root][INFO] - ㄴ dropout              : 0.03

[2024-10-31 17:40:21,394][root][INFO] - SUB2 Configuration
[2024-10-31 17:40:21,394][root][INFO] - ㄴ tok_type             : jamo_var
[2024-10-31 17:40:21,394][root][INFO] - ㄴ hidden_dim           : 768
[2024-10-31 17:40:21,394][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-10-31 17:40:21,394][root][INFO] - ㄴ embedding_norm       : False
[2024-10-31 17:40:21,394][root][INFO] - ㄴ do_combination       : False
[2024-10-31 17:40:21,395][root][INFO] - ㄴ reducer              : linear_pool

[2024-10-31 17:40:21,395][root][INFO] - LoRA in SUB2 Configuration
[2024-10-31 17:40:21,395][root][INFO] - ㄴ r                : 32
[2024-10-31 17:40:21,395][root][INFO] - ㄴ alpha            : 128
[2024-10-31 17:40:21,395][root][INFO] - ㄴ dropout          : 0.03

[2024-10-31 17:40:21,395][root][INFO] - 

[2024-10-31 17:40:21,395][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 17:40:21,395][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt
[2024-10-31 17:40:21,395][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb
[2024-10-31 17:40:21,395][root][INFO] - * tb interval   : 100

[2024-10-31 17:40:21,396][root][INFO] - 

[2024-10-31 17:40:21,396][root][INFO] - Start the Training !
[2024-10-31 17:40:21,398][root][INFO] - 
[1/ 10 Epoch]
[2024-10-31 17:41:40,954][root][INFO] - 

[2024-10-31 17:41:40,954][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-31 17:41:40,955][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 17:41:40,955][root][INFO] - 

[2024-10-31 17:41:40,955][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-31 17:41:48,256][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-31 17:41:48,256][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-31 17:41:48,257][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-31 17:41:48,258][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-31 17:41:48,258][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-31 17:41:48,258][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-31 17:41:48,259][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-31 17:41:48,259][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-31 17:41:48,260][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-31 17:41:48,260][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-31 17:41:48,261][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-31 17:41:48,261][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-31 17:41:48,261][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-31 17:41:48,262][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-31 17:41:48,262][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-31 17:41:48,263][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-31 17:41:48,263][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-31 17:41:48,263][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-31 17:41:48,264][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-31 17:41:48,264][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-31 17:41:48,265][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-31 17:41:48,265][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-31 17:41:48,266][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-31 17:41:48,266][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-31 17:41:48,267][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-10-31 17:41:48,272][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-10-31 17:41:48,437][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-10-31 17:41:48,441][root][INFO] - Trainable params: 15525888 || all params: 140691456 || trainable: 11.04 %
[2024-10-31 17:41:48,626][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-10-31 17:41:50,713][root][INFO] - 

[2024-10-31 17:41:50,713][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-10-31 17:41:50,713][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-10-31 17:41:50,721][root][INFO] - vocab size              : 51200
[2024-10-31 17:41:50,721][root][INFO] - device                  : gpu
[2024-10-31 17:41:50,721][root][INFO] - random seed             : 1
[2024-10-31 17:41:50,721][root][INFO] - train data size         : 3672
[2024-10-31 17:41:50,721][root][INFO] - max epochs              : 10
[2024-10-31 17:41:50,721][root][INFO] - total steps             : 4590
[2024-10-31 17:41:50,722][root][INFO] - warmup steps            : 459
[2024-10-31 17:41:50,722][root][INFO] - batch size              : 8
[2024-10-31 17:41:50,722][root][INFO] - accumulation steps      : 1
[2024-10-31 17:41:50,722][root][INFO] - optimizer               : adamwscale
[2024-10-31 17:41:50,722][root][INFO] - lr_scheduler            : cosine
[2024-10-31 17:41:50,722][root][INFO] - learning rate           : 0.01
[2024-10-31 17:41:50,722][root][INFO] - max length              : 256

[2024-10-31 17:41:50,722][root][INFO] - LoRA Configuration
[2024-10-31 17:41:50,722][root][INFO] - ㄴ r                    : 32
[2024-10-31 17:41:50,722][root][INFO] - ㄴ alpha                : 128
[2024-10-31 17:41:50,722][root][INFO] - ㄴ dropout              : 0.03

[2024-10-31 17:41:50,723][root][INFO] - SUB2 Configuration
[2024-10-31 17:41:50,723][root][INFO] - ㄴ tok_type             : jamo_var
[2024-10-31 17:41:50,723][root][INFO] - ㄴ hidden_dim           : 768
[2024-10-31 17:41:50,723][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-10-31 17:41:50,723][root][INFO] - ㄴ embedding_norm       : False
[2024-10-31 17:41:50,723][root][INFO] - ㄴ do_combination       : False
[2024-10-31 17:41:50,723][root][INFO] - ㄴ reducer              : linear_pool

[2024-10-31 17:41:50,723][root][INFO] - LoRA in SUB2 Configuration
[2024-10-31 17:41:50,723][root][INFO] - ㄴ r                : 32
[2024-10-31 17:41:50,724][root][INFO] - ㄴ alpha            : 128
[2024-10-31 17:41:50,724][root][INFO] - ㄴ dropout          : 0.03

[2024-10-31 17:41:50,724][root][INFO] - 

[2024-10-31 17:41:50,724][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 17:41:50,724][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt
[2024-10-31 17:41:50,724][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb
[2024-10-31 17:41:50,724][root][INFO] - * tb interval   : 100

[2024-10-31 17:41:50,724][root][INFO] - 

[2024-10-31 17:41:50,724][root][INFO] - Start the Training !
[2024-10-31 17:41:50,727][root][INFO] - 
[1/ 10 Epoch]
[2024-10-31 17:42:03,437][root][INFO] - 

[2024-10-31 17:42:03,437][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-31 17:42:03,437][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 17:42:03,438][root][INFO] - 

[2024-10-31 17:42:03,438][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-31 17:42:10,899][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-31 17:42:10,899][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-31 17:42:10,900][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-31 17:42:10,900][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-31 17:42:10,901][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-31 17:42:10,901][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-31 17:42:10,902][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-31 17:42:10,902][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-31 17:42:10,903][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-31 17:42:10,903][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-31 17:42:10,904][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-31 17:42:10,904][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-31 17:42:10,905][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-31 17:42:10,905][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-31 17:42:10,906][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-31 17:42:10,906][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-31 17:42:10,906][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-31 17:42:10,907][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-31 17:42:10,907][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-31 17:42:10,908][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-31 17:42:10,908][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-31 17:42:10,909][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-31 17:42:10,909][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-31 17:42:10,910][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-31 17:42:10,911][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-10-31 17:42:10,916][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-10-31 17:42:11,081][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-10-31 17:42:11,085][root][INFO] - Trainable params: 15525888 || all params: 140691456 || trainable: 11.04 %
[2024-10-31 17:42:11,409][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-10-31 17:42:13,293][root][INFO] - 

[2024-10-31 17:42:13,293][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-10-31 17:42:13,293][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-10-31 17:42:13,301][root][INFO] - vocab size              : 51200
[2024-10-31 17:42:13,301][root][INFO] - device                  : gpu
[2024-10-31 17:42:13,301][root][INFO] - random seed             : 1
[2024-10-31 17:42:13,301][root][INFO] - train data size         : 3672
[2024-10-31 17:42:13,301][root][INFO] - max epochs              : 10
[2024-10-31 17:42:13,301][root][INFO] - total steps             : 4590
[2024-10-31 17:42:13,301][root][INFO] - warmup steps            : 459
[2024-10-31 17:42:13,301][root][INFO] - batch size              : 8
[2024-10-31 17:42:13,302][root][INFO] - accumulation steps      : 1
[2024-10-31 17:42:13,302][root][INFO] - optimizer               : adamwscale
[2024-10-31 17:42:13,302][root][INFO] - lr_scheduler            : cosine
[2024-10-31 17:42:13,302][root][INFO] - learning rate           : 0.01
[2024-10-31 17:42:13,302][root][INFO] - max length              : 256

[2024-10-31 17:42:13,302][root][INFO] - LoRA Configuration
[2024-10-31 17:42:13,302][root][INFO] - ㄴ r                    : 32
[2024-10-31 17:42:13,302][root][INFO] - ㄴ alpha                : 128
[2024-10-31 17:42:13,302][root][INFO] - ㄴ dropout              : 0.03

[2024-10-31 17:42:13,302][root][INFO] - SUB2 Configuration
[2024-10-31 17:42:13,302][root][INFO] - ㄴ tok_type             : jamo_var
[2024-10-31 17:42:13,303][root][INFO] - ㄴ hidden_dim           : 768
[2024-10-31 17:42:13,303][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-10-31 17:42:13,303][root][INFO] - ㄴ embedding_norm       : False
[2024-10-31 17:42:13,303][root][INFO] - ㄴ do_combination       : False
[2024-10-31 17:42:13,303][root][INFO] - ㄴ reducer              : linear_pool

[2024-10-31 17:42:13,303][root][INFO] - LoRA in SUB2 Configuration
[2024-10-31 17:42:13,303][root][INFO] - ㄴ r                : 32
[2024-10-31 17:42:13,303][root][INFO] - ㄴ alpha            : 128
[2024-10-31 17:42:13,303][root][INFO] - ㄴ dropout          : 0.03

[2024-10-31 17:42:13,303][root][INFO] - 

[2024-10-31 17:42:13,304][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 17:42:13,304][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt
[2024-10-31 17:42:13,304][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb
[2024-10-31 17:42:13,304][root][INFO] - * tb interval   : 100

[2024-10-31 17:42:13,304][root][INFO] - 

[2024-10-31 17:42:13,304][root][INFO] - Start the Training !
[2024-10-31 17:42:13,307][root][INFO] - 
[1/ 10 Epoch]
[2024-10-31 17:50:34,429][root][INFO] - 

[2024-10-31 17:50:34,429][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-31 17:50:34,429][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs
[2024-10-31 17:50:34,429][root][INFO] - 

[2024-10-31 17:50:34,429][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.0001, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-31 17:50:41,919][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-31 17:50:41,920][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-31 17:50:41,920][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-31 17:50:41,921][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-31 17:50:41,921][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-31 17:50:41,922][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-31 17:50:41,922][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-31 17:50:41,923][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-31 17:50:41,923][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-31 17:50:41,924][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-31 17:50:41,924][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-31 17:50:41,925][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-31 17:50:41,925][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-31 17:50:41,926][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-31 17:50:41,926][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-31 17:50:41,927][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-31 17:50:41,927][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-31 17:50:41,928][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-31 17:50:41,928][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-31 17:50:41,929][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-31 17:50:41,930][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-31 17:50:41,930][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-31 17:50:41,931][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-31 17:50:41,931][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-31 17:50:41,933][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-10-31 17:50:41,937][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-10-31 17:50:42,101][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-10-31 17:50:42,103][root][INFO] - Trainable params: 15525888 || all params: 140691456 || trainable: 11.04 %
[2024-10-31 17:50:42,285][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-10-31 17:50:44,412][root][INFO] - 

[2024-10-31 17:50:44,412][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-10-31 17:50:44,412][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-10-31 17:50:44,420][root][INFO] - vocab size              : 51200
[2024-10-31 17:50:44,420][root][INFO] - device                  : gpu
[2024-10-31 17:50:44,420][root][INFO] - random seed             : 1
[2024-10-31 17:50:44,420][root][INFO] - train data size         : 3672
[2024-10-31 17:50:44,420][root][INFO] - max epochs              : 10
[2024-10-31 17:50:44,420][root][INFO] - total steps             : 4590
[2024-10-31 17:50:44,420][root][INFO] - warmup steps            : 459
[2024-10-31 17:50:44,420][root][INFO] - batch size              : 8
[2024-10-31 17:50:44,420][root][INFO] - accumulation steps      : 1
[2024-10-31 17:50:44,421][root][INFO] - optimizer               : adamwscale
[2024-10-31 17:50:44,421][root][INFO] - lr_scheduler            : cosine
[2024-10-31 17:50:44,421][root][INFO] - learning rate           : 0.0001
[2024-10-31 17:50:44,421][root][INFO] - max length              : 256

[2024-10-31 17:50:44,421][root][INFO] - LoRA Configuration
[2024-10-31 17:50:44,421][root][INFO] - ㄴ r                    : 32
[2024-10-31 17:50:44,421][root][INFO] - ㄴ alpha                : 128
[2024-10-31 17:50:44,421][root][INFO] - ㄴ dropout              : 0.03

[2024-10-31 17:50:44,421][root][INFO] - SUB2 Configuration
[2024-10-31 17:50:44,421][root][INFO] - ㄴ tok_type             : jamo_var
[2024-10-31 17:50:44,421][root][INFO] - ㄴ hidden_dim           : 768
[2024-10-31 17:50:44,422][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-10-31 17:50:44,422][root][INFO] - ㄴ embedding_norm       : False
[2024-10-31 17:50:44,422][root][INFO] - ㄴ do_combination       : False
[2024-10-31 17:50:44,422][root][INFO] - ㄴ reducer              : linear_pool

[2024-10-31 17:50:44,422][root][INFO] - LoRA in SUB2 Configuration
[2024-10-31 17:50:44,422][root][INFO] - ㄴ r                : 32
[2024-10-31 17:50:44,422][root][INFO] - ㄴ alpha            : 128
[2024-10-31 17:50:44,422][root][INFO] - ㄴ dropout          : 0.03

[2024-10-31 17:50:44,422][root][INFO] - 

[2024-10-31 17:50:44,422][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs
[2024-10-31 17:50:44,422][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/ckpt
[2024-10-31 17:50:44,423][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/tb
[2024-10-31 17:50:44,423][root][INFO] - * tb interval   : 100

[2024-10-31 17:50:44,423][root][INFO] - 

[2024-10-31 17:50:44,423][root][INFO] - Start the Training !
[2024-10-31 17:50:44,429][root][INFO] - 
[1/ 10 Epoch]
[2024-10-31 17:50:56,662][root][INFO] - Step: 100/4590  |  Loss: 0.7706  |  Score: 51.12 [%]  |  Seq Length: 256.0
[2024-10-31 17:51:07,670][root][INFO] - Step: 200/4590  |  Loss: 0.7825  |  Score: 50.62 [%]  |  Seq Length: 256.0
[2024-10-31 17:51:18,427][root][INFO] - Step: 300/4590  |  Loss: 0.7673  |  Score: 51.62 [%]  |  Seq Length: 256.0
[2024-10-31 17:51:28,205][root][INFO] - Step: 400/4590  |  Loss: 0.8204  |  Score: 47.38 [%]  |  Seq Length: 256.0
[2024-10-31 17:51:34,010][root][INFO] - Step: 459/4590  |  Loss: 0.7632  |  Score: 48.94 [%]  |  Seq Length: 256.0
[2024-10-31 17:52:24,286][root][INFO] - 

[2024-10-31 17:52:24,286][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-31 17:52:24,286][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 17:52:24,286][root][INFO] - 

[2024-10-31 17:52:24,286][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0.1, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-31 17:52:31,848][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-31 17:52:31,848][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-31 17:52:31,849][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-31 17:52:31,849][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-31 17:52:31,850][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-31 17:52:31,851][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-31 17:52:31,851][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-31 17:52:31,852][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-31 17:52:31,852][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-31 17:52:31,853][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-31 17:52:31,853][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-31 17:52:31,854][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-31 17:52:31,854][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-31 17:52:31,855][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-31 17:52:31,855][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-31 17:52:31,856][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-31 17:52:31,856][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-31 17:52:31,857][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-31 17:52:31,857][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-31 17:52:31,858][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-31 17:52:31,858][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-31 17:52:31,859][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-31 17:52:31,859][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-31 17:52:31,860][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-31 17:52:31,862][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-10-31 17:52:31,866][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-10-31 17:52:32,030][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-10-31 17:52:32,032][root][INFO] - Trainable params: 15525888 || all params: 140691456 || trainable: 11.04 %
[2024-10-31 17:52:32,212][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-10-31 17:52:34,411][root][INFO] - 

[2024-10-31 17:52:34,412][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-10-31 17:52:34,412][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-10-31 17:52:34,421][root][INFO] - vocab size              : 51200
[2024-10-31 17:52:34,421][root][INFO] - device                  : gpu
[2024-10-31 17:52:34,421][root][INFO] - random seed             : 1
[2024-10-31 17:52:34,421][root][INFO] - train data size         : 3672
[2024-10-31 17:52:34,421][root][INFO] - max epochs              : 10
[2024-10-31 17:52:34,421][root][INFO] - total steps             : 4590
[2024-10-31 17:52:34,422][root][INFO] - warmup steps            : 459
[2024-10-31 17:52:34,422][root][INFO] - batch size              : 8
[2024-10-31 17:52:34,422][root][INFO] - accumulation steps      : 1
[2024-10-31 17:52:34,422][root][INFO] - optimizer               : adamwscale
[2024-10-31 17:52:34,422][root][INFO] - lr_scheduler            : cosine
[2024-10-31 17:52:34,422][root][INFO] - learning rate           : 0.01
[2024-10-31 17:52:34,422][root][INFO] - max length              : 256

[2024-10-31 17:52:34,422][root][INFO] - LoRA Configuration
[2024-10-31 17:52:34,422][root][INFO] - ㄴ r                    : 32
[2024-10-31 17:52:34,422][root][INFO] - ㄴ alpha                : 128
[2024-10-31 17:52:34,422][root][INFO] - ㄴ dropout              : 0.03

[2024-10-31 17:52:34,423][root][INFO] - SUB2 Configuration
[2024-10-31 17:52:34,423][root][INFO] - ㄴ tok_type             : jamo_var
[2024-10-31 17:52:34,423][root][INFO] - ㄴ hidden_dim           : 768
[2024-10-31 17:52:34,423][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-10-31 17:52:34,423][root][INFO] - ㄴ embedding_norm       : False
[2024-10-31 17:52:34,423][root][INFO] - ㄴ do_combination       : False
[2024-10-31 17:52:34,423][root][INFO] - ㄴ reducer              : linear_pool

[2024-10-31 17:52:34,423][root][INFO] - LoRA in SUB2 Configuration
[2024-10-31 17:52:34,423][root][INFO] - ㄴ r                : 32
[2024-10-31 17:52:34,423][root][INFO] - ㄴ alpha            : 128
[2024-10-31 17:52:34,424][root][INFO] - ㄴ dropout          : 0.03

[2024-10-31 17:52:34,424][root][INFO] - 

[2024-10-31 17:52:34,424][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 17:52:34,424][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt
[2024-10-31 17:52:34,424][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb
[2024-10-31 17:52:34,424][root][INFO] - * tb interval   : 100

[2024-10-31 17:52:34,424][root][INFO] - 

[2024-10-31 17:52:34,424][root][INFO] - Start the Training !
[2024-10-31 17:52:34,427][root][INFO] - 
[1/ 10 Epoch]
[2024-10-31 17:52:46,551][root][INFO] - Step: 100/4590  |  Loss: 0.7677  |  Score: 49.62 [%]  |  Seq Length: 256.0
[2024-10-31 17:52:57,664][root][INFO] - Step: 200/4590  |  Loss: 0.7327  |  Score: 50.62 [%]  |  Seq Length: 256.0
[2024-10-31 17:53:08,662][root][INFO] - Step: 300/4590  |  Loss: 0.7401  |  Score: 51.50 [%]  |  Seq Length: 256.0
[2024-10-31 17:53:19,752][root][INFO] - Step: 400/4590  |  Loss: 0.7256  |  Score: 51.25 [%]  |  Seq Length: 256.0
[2024-10-31 17:53:26,466][root][INFO] - Step: 459/4590  |  Loss: 0.7082  |  Score: 51.27 [%]  |  Seq Length: 256.0
[2024-10-31 17:53:30,850][root][INFO] - ########################  DEV REPORT #EP1  ########################
[2024-10-31 17:53:30,850][root][INFO] - Score: 59.52 [%]  |  Evaluation Time: 4.38 [s]
[2024-10-31 17:53:39,244][root][INFO] - ########################  TEST REPORT #EP1  ########################
[2024-10-31 17:53:39,244][root][INFO] - Score: 58.66 [%]  |  Evaluation Time: 8.39 [s]
[2024-10-31 17:53:39,246][root][INFO] - 
Save new Best Score (Epoch: 1)
[2024-10-31 17:53:39,247][root][INFO] - 
[2/ 10 Epoch]
[2024-10-31 17:53:43,829][root][INFO] - Step: 500/4590  |  Loss: 0.6808  |  Score: 60.06 [%]  |  Seq Length: 256.0
[2024-10-31 17:53:54,378][root][INFO] - Step: 600/4590  |  Loss: 0.6734  |  Score: 58.13 [%]  |  Seq Length: 256.0
[2024-10-31 17:54:04,940][root][INFO] - Step: 700/4590  |  Loss: 0.6694  |  Score: 61.75 [%]  |  Seq Length: 256.0
[2024-10-31 17:54:15,700][root][INFO] - Step: 800/4590  |  Loss: 0.6786  |  Score: 58.25 [%]  |  Seq Length: 256.0
[2024-10-31 17:54:26,361][root][INFO] - Step: 900/4590  |  Loss: 0.6643  |  Score: 62.38 [%]  |  Seq Length: 256.0
[2024-10-31 17:54:28,374][root][INFO] - Step: 918/4590  |  Loss: 0.7474  |  Score: 58.33 [%]  |  Seq Length: 256.0
[2024-10-31 17:54:32,786][root][INFO] - ########################  DEV REPORT #EP2  ########################
[2024-10-31 17:54:32,786][root][INFO] - Score: 60.37 [%]  |  Evaluation Time: 4.41 [s]
[2024-10-31 17:54:41,227][root][INFO] - ########################  TEST REPORT #EP2  ########################
[2024-10-31 17:54:41,227][root][INFO] - Score: 63.28 [%]  |  Evaluation Time: 8.44 [s]
[2024-10-31 17:54:41,228][root][INFO] - 
Save new Best Score (Epoch: 2)
[2024-10-31 17:54:41,229][root][INFO] - 
[3/ 10 Epoch]
[2024-10-31 17:54:50,198][root][INFO] - Step: 1000/4590  |  Loss: 0.5754  |  Score: 71.80 [%]  |  Seq Length: 256.0
[2024-10-31 17:55:00,616][root][INFO] - Step: 1100/4590  |  Loss: 0.5593  |  Score: 72.00 [%]  |  Seq Length: 256.0
[2024-10-31 21:49:59,704][root][INFO] - 

[2024-10-31 21:49:59,704][root][INFO] - This train_log.txt inform the Running Progress.

[2024-10-31 21:49:59,704][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 21:49:59,704][root][INFO] - 

[2024-10-31 21:49:59,704][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-10-31 21:50:07,113][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-10-31 21:50:07,114][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-10-31 21:50:07,114][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-10-31 21:50:07,115][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-10-31 21:50:07,115][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-10-31 21:50:07,116][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-10-31 21:50:07,116][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-10-31 21:50:07,117][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-10-31 21:50:07,117][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-10-31 21:50:07,118][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-10-31 21:50:07,118][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-10-31 21:50:07,119][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-10-31 21:50:07,119][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-10-31 21:50:07,120][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-10-31 21:50:07,120][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-10-31 21:50:07,121][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-10-31 21:50:07,121][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-10-31 21:50:07,122][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-10-31 21:50:07,122][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-10-31 21:50:07,123][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-10-31 21:50:07,124][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-10-31 21:50:07,124][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-10-31 21:50:07,125][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-10-31 21:50:07,125][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-10-31 21:50:07,128][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-10-31 21:50:07,132][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-10-31 21:50:07,296][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-10-31 21:50:07,298][root][INFO] - Trainable params: 15525888 || all params: 140691456 || trainable: 11.04 %
[2024-10-31 21:50:07,472][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-10-31 21:50:09,380][root][INFO] - 

[2024-10-31 21:50:09,380][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-10-31 21:50:09,380][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-10-31 21:50:09,388][root][INFO] - vocab size              : 51200
[2024-10-31 21:50:09,388][root][INFO] - device                  : gpu
[2024-10-31 21:50:09,388][root][INFO] - random seed             : 1
[2024-10-31 21:50:09,388][root][INFO] - train data size         : 3672
[2024-10-31 21:50:09,388][root][INFO] - max epochs              : 10
[2024-10-31 21:50:09,388][root][INFO] - total steps             : 4590
[2024-10-31 21:50:09,388][root][INFO] - warmup steps            : 0
[2024-10-31 21:50:09,388][root][INFO] - batch size              : 8
[2024-10-31 21:50:09,389][root][INFO] - accumulation steps      : 1
[2024-10-31 21:50:09,389][root][INFO] - optimizer               : adamwscale
[2024-10-31 21:50:09,389][root][INFO] - lr_scheduler            : cosine
[2024-10-31 21:50:09,389][root][INFO] - learning rate           : 0.01
[2024-10-31 21:50:09,389][root][INFO] - max length              : 256

[2024-10-31 21:50:09,389][root][INFO] - LoRA Configuration
[2024-10-31 21:50:09,389][root][INFO] - ㄴ r                    : 32
[2024-10-31 21:50:09,389][root][INFO] - ㄴ alpha                : 128
[2024-10-31 21:50:09,389][root][INFO] - ㄴ dropout              : 0.03

[2024-10-31 21:50:09,389][root][INFO] - SUB2 Configuration
[2024-10-31 21:50:09,390][root][INFO] - ㄴ tok_type             : jamo_var
[2024-10-31 21:50:09,390][root][INFO] - ㄴ hidden_dim           : 768
[2024-10-31 21:50:09,390][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-10-31 21:50:09,390][root][INFO] - ㄴ embedding_norm       : False
[2024-10-31 21:50:09,390][root][INFO] - ㄴ do_combination       : False
[2024-10-31 21:50:09,390][root][INFO] - ㄴ reducer              : linear_pool

[2024-10-31 21:50:09,390][root][INFO] - LoRA in SUB2 Configuration
[2024-10-31 21:50:09,390][root][INFO] - ㄴ r                : 32
[2024-10-31 21:50:09,391][root][INFO] - ㄴ alpha            : 128
[2024-10-31 21:50:09,391][root][INFO] - ㄴ dropout          : 0.03

[2024-10-31 21:50:09,391][root][INFO] - 

[2024-10-31 21:50:09,391][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-10-31 21:50:09,391][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt
[2024-10-31 21:50:09,391][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb
[2024-10-31 21:50:09,391][root][INFO] - * tb interval   : 100

[2024-10-31 21:50:09,391][root][INFO] - 

[2024-10-31 21:50:09,391][root][INFO] - Start the Training !
[2024-10-31 21:50:09,394][root][INFO] - 
[1/ 10 Epoch]
[2024-10-31 21:50:23,017][root][INFO] - Step: 100/4590  |  Loss: 0.7652  |  Score: 49.50 [%]  |  Seq Length: 256.0
[2024-10-31 21:50:34,536][root][INFO] - Step: 200/4590  |  Loss: 0.7385  |  Score: 50.75 [%]  |  Seq Length: 256.0
[2024-10-31 21:50:45,787][root][INFO] - Step: 300/4590  |  Loss: 0.8331  |  Score: 49.75 [%]  |  Seq Length: 256.0
[2024-10-31 21:50:56,784][root][INFO] - Step: 400/4590  |  Loss: 0.7313  |  Score: 46.75 [%]  |  Seq Length: 256.0
[2024-10-31 21:51:03,605][root][INFO] - Step: 459/4590  |  Loss: 0.7121  |  Score: 51.27 [%]  |  Seq Length: 256.0
[2024-10-31 21:51:07,912][root][INFO] - ########################  DEV REPORT #EP1  ########################
[2024-10-31 21:51:07,913][root][INFO] - Score: 46.59 [%]  |  Evaluation Time: 4.30 [s]
[2024-10-31 21:51:16,674][root][INFO] - ########################  TEST REPORT #EP1  ########################
[2024-10-31 21:51:16,675][root][INFO] - Score: 50.28 [%]  |  Evaluation Time: 8.76 [s]
[2024-10-31 21:51:16,676][root][INFO] - 
Save new Best Score (Epoch: 1)
[2024-10-31 21:51:16,677][root][INFO] - 
[2/ 10 Epoch]
[2024-10-31 21:51:21,864][root][INFO] - Step: 500/4590  |  Loss: 0.7151  |  Score: 43.29 [%]  |  Seq Length: 256.0
[2024-10-31 21:51:33,842][root][INFO] - Step: 600/4590  |  Loss: 0.7057  |  Score: 49.62 [%]  |  Seq Length: 256.0
[2024-10-31 21:51:45,757][root][INFO] - Step: 700/4590  |  Loss: 0.7041  |  Score: 51.38 [%]  |  Seq Length: 256.0
[2024-10-31 21:51:57,240][root][INFO] - Step: 800/4590  |  Loss: 0.7078  |  Score: 48.88 [%]  |  Seq Length: 256.0
[2024-10-31 21:52:07,951][root][INFO] - Step: 900/4590  |  Loss: 0.7221  |  Score: 48.62 [%]  |  Seq Length: 256.0
[2024-10-31 21:52:09,961][root][INFO] - Step: 918/4590  |  Loss: 0.7075  |  Score: 44.44 [%]  |  Seq Length: 256.0
[2024-10-31 21:52:14,217][root][INFO] - ########################  DEV REPORT #EP2  ########################
[2024-10-31 21:52:14,218][root][INFO] - Score: 46.73 [%]  |  Evaluation Time: 4.25 [s]
[2024-10-31 21:52:23,094][root][INFO] - ########################  TEST REPORT #EP2  ########################
[2024-10-31 21:52:23,094][root][INFO] - Score: 50.14 [%]  |  Evaluation Time: 8.87 [s]
[2024-10-31 21:52:23,095][root][INFO] - 
Save new Best Score (Epoch: 2)
[2024-10-31 21:52:23,096][root][INFO] - 
[3/ 10 Epoch]
[2024-10-31 21:52:32,239][root][INFO] - Step: 1000/4590  |  Loss: 0.7023  |  Score: 48.63 [%]  |  Seq Length: 256.0
[2024-10-31 21:52:42,938][root][INFO] - Step: 1100/4590  |  Loss: 0.6915  |  Score: 52.50 [%]  |  Seq Length: 256.0
[2024-10-31 21:52:53,637][root][INFO] - Step: 1200/4590  |  Loss: 0.7092  |  Score: 49.00 [%]  |  Seq Length: 256.0
[2024-10-31 21:53:04,705][root][INFO] - Step: 1300/4590  |  Loss: 0.7018  |  Score: 47.75 [%]  |  Seq Length: 256.0
[2024-10-31 21:53:13,123][root][INFO] - Step: 1377/4590  |  Loss: 0.6998  |  Score: 50.81 [%]  |  Seq Length: 256.0
[2024-10-31 21:53:17,861][root][INFO] - ########################  DEV REPORT #EP3  ########################
[2024-10-31 21:53:17,862][root][INFO] - Score: 53.84 [%]  |  Evaluation Time: 4.74 [s]
[2024-10-31 21:53:26,877][root][INFO] - ########################  TEST REPORT #EP3  ########################
[2024-10-31 21:53:26,877][root][INFO] - Score: 49.93 [%]  |  Evaluation Time: 9.01 [s]
[2024-10-31 21:53:26,878][root][INFO] - 
Save new Best Score (Epoch: 3)
[2024-10-31 21:53:26,879][root][INFO] - 
[4/ 10 Epoch]
[2024-10-31 21:53:29,691][root][INFO] - Step: 1400/4590  |  Loss: 0.7011  |  Score: 47.83 [%]  |  Seq Length: 256.0
[2024-10-31 21:53:40,116][root][INFO] - Step: 1500/4590  |  Loss: 0.7001  |  Score: 51.12 [%]  |  Seq Length: 256.0
[2024-10-31 21:53:50,658][root][INFO] - Step: 1600/4590  |  Loss: 0.6994  |  Score: 52.25 [%]  |  Seq Length: 256.0
[2024-10-31 21:54:02,390][root][INFO] - Step: 1700/4590  |  Loss: 0.6951  |  Score: 54.12 [%]  |  Seq Length: 256.0
[2024-10-31 21:54:14,171][root][INFO] - Step: 1800/4590  |  Loss: 0.7003  |  Score: 51.38 [%]  |  Seq Length: 256.0
[2024-10-31 21:54:18,580][root][INFO] - Step: 1836/4590  |  Loss: 0.6968  |  Score: 46.53 [%]  |  Seq Length: 256.0
[2024-10-31 21:54:23,266][root][INFO] - ########################  DEV REPORT #EP4  ########################
[2024-10-31 21:54:23,266][root][INFO] - Score: 46.16 [%]  |  Evaluation Time: 4.68 [s]
[2024-10-31 21:54:32,308][root][INFO] - ########################  TEST REPORT #EP4  ########################
[2024-10-31 21:54:32,308][root][INFO] - Score: 52.20 [%]  |  Evaluation Time: 9.04 [s]
[2024-10-31 21:54:32,311][root][INFO] - 
[5/ 10 Epoch]
[2024-10-31 21:54:39,841][root][INFO] - Step: 1900/4590  |  Loss: 0.7482  |  Score: 49.41 [%]  |  Seq Length: 256.0
[2024-10-31 21:54:50,813][root][INFO] - Step: 2000/4590  |  Loss: 0.7288  |  Score: 51.75 [%]  |  Seq Length: 256.0
[2024-10-31 21:55:01,733][root][INFO] - Step: 2100/4590  |  Loss: 0.7118  |  Score: 51.12 [%]  |  Seq Length: 256.0
[2024-10-31 21:55:12,631][root][INFO] - Step: 2200/4590  |  Loss: 0.6942  |  Score: 52.75 [%]  |  Seq Length: 256.0
[2024-10-31 21:55:23,700][root][INFO] - Step: 2295/4590  |  Loss: 0.6991  |  Score: 48.55 [%]  |  Seq Length: 256.0
[2024-10-31 21:55:27,926][root][INFO] - ########################  DEV REPORT #EP5  ########################
[2024-10-31 21:55:27,927][root][INFO] - Score: 53.41 [%]  |  Evaluation Time: 4.22 [s]
[2024-10-31 21:55:36,261][root][INFO] - ########################  TEST REPORT #EP5  ########################
[2024-10-31 21:55:36,261][root][INFO] - Score: 49.79 [%]  |  Evaluation Time: 8.33 [s]
[2024-10-31 21:55:36,263][root][INFO] - 
[6/ 10 Epoch]
[2024-10-31 21:55:37,086][root][INFO] - Step: 2300/4590  |  Loss: 0.6886  |  Score: 50.00 [%]  |  Seq Length: 256.0
[2024-10-31 21:55:48,262][root][INFO] - Step: 2400/4590  |  Loss: 0.6942  |  Score: 52.75 [%]  |  Seq Length: 256.0
[2024-10-31 21:55:59,150][root][INFO] - Step: 2500/4590  |  Loss: 0.6913  |  Score: 53.25 [%]  |  Seq Length: 256.0
[2024-10-31 21:56:09,880][root][INFO] - Step: 2600/4590  |  Loss: 0.6910  |  Score: 54.12 [%]  |  Seq Length: 256.0
[2024-10-31 21:56:20,797][root][INFO] - Step: 2700/4590  |  Loss: 0.6967  |  Score: 49.50 [%]  |  Seq Length: 256.0
[2024-10-31 21:56:26,772][root][INFO] - Step: 2754/4590  |  Loss: 0.6954  |  Score: 50.23 [%]  |  Seq Length: 256.0
[2024-10-31 21:56:31,047][root][INFO] - ########################  DEV REPORT #EP6  ########################
[2024-10-31 21:56:31,047][root][INFO] - Score: 46.88 [%]  |  Evaluation Time: 4.27 [s]
[2024-10-31 21:56:39,210][root][INFO] - ########################  TEST REPORT #EP6  ########################
[2024-10-31 21:56:39,210][root][INFO] - Score: 50.43 [%]  |  Evaluation Time: 8.16 [s]
[2024-10-31 21:56:39,213][root][INFO] - 
[7/ 10 Epoch]
[2024-10-31 21:56:44,557][root][INFO] - Step: 2800/4590  |  Loss: 0.6848  |  Score: 56.25 [%]  |  Seq Length: 256.0
[2024-10-31 21:56:55,308][root][INFO] - Step: 2900/4590  |  Loss: 0.6961  |  Score: 52.88 [%]  |  Seq Length: 256.0
[2024-10-31 21:57:06,127][root][INFO] - Step: 3000/4590  |  Loss: 0.6886  |  Score: 52.38 [%]  |  Seq Length: 256.0
[2024-10-31 21:57:17,018][root][INFO] - Step: 3100/4590  |  Loss: 0.6863  |  Score: 55.38 [%]  |  Seq Length: 256.0
[2024-10-31 21:57:27,751][root][INFO] - Step: 3200/4590  |  Loss: 0.6842  |  Score: 55.12 [%]  |  Seq Length: 256.0
[2024-10-31 21:57:29,239][root][INFO] - Step: 3213/4590  |  Loss: 0.7257  |  Score: 47.12 [%]  |  Seq Length: 256.0
[2024-10-31 21:57:33,535][root][INFO] - ########################  DEV REPORT #EP7  ########################
[2024-10-31 21:57:33,535][root][INFO] - Score: 51.70 [%]  |  Evaluation Time: 4.29 [s]
[2024-10-31 21:57:42,651][root][INFO] - ########################  TEST REPORT #EP7  ########################
[2024-10-31 21:57:42,652][root][INFO] - Score: 50.14 [%]  |  Evaluation Time: 9.11 [s]
[2024-10-31 21:57:42,654][root][INFO] - 
[8/ 10 Epoch]
[2024-10-31 21:57:52,430][root][INFO] - Step: 3300/4590  |  Loss: 0.6865  |  Score: 55.89 [%]  |  Seq Length: 256.0
[2024-10-31 21:58:03,398][root][INFO] - Step: 3400/4590  |  Loss: 0.6846  |  Score: 53.87 [%]  |  Seq Length: 256.0
[2024-10-31 21:58:13,977][root][INFO] - Step: 3500/4590  |  Loss: 0.6785  |  Score: 57.50 [%]  |  Seq Length: 256.0
[2024-10-31 21:58:24,810][root][INFO] - Step: 3600/4590  |  Loss: 0.6829  |  Score: 57.00 [%]  |  Seq Length: 256.0
[2024-10-31 21:58:32,512][root][INFO] - Step: 3672/4590  |  Loss: 0.6691  |  Score: 57.12 [%]  |  Seq Length: 256.0
[2024-10-31 21:58:36,682][root][INFO] - ########################  DEV REPORT #EP8  ########################
[2024-10-31 21:58:36,682][root][INFO] - Score: 47.59 [%]  |  Evaluation Time: 4.17 [s]
[2024-10-31 21:58:44,739][root][INFO] - ########################  TEST REPORT #EP8  ########################
[2024-10-31 21:58:44,740][root][INFO] - Score: 50.14 [%]  |  Evaluation Time: 8.06 [s]
[2024-10-31 21:58:44,742][root][INFO] - 
[9/ 10 Epoch]
[2024-10-31 21:58:48,033][root][INFO] - Step: 3700/4590  |  Loss: 0.6708  |  Score: 56.70 [%]  |  Seq Length: 256.0
[2024-10-31 21:58:58,621][root][INFO] - Step: 3800/4590  |  Loss: 0.6717  |  Score: 59.88 [%]  |  Seq Length: 256.0
[2024-10-31 21:59:09,145][root][INFO] - Step: 3900/4590  |  Loss: 0.6637  |  Score: 60.38 [%]  |  Seq Length: 256.0
[2024-10-31 21:59:20,790][root][INFO] - Step: 4000/4590  |  Loss: 0.6871  |  Score: 55.88 [%]  |  Seq Length: 256.0
[2024-10-31 21:59:32,520][root][INFO] - Step: 4100/4590  |  Loss: 0.6684  |  Score: 60.38 [%]  |  Seq Length: 256.0
[2024-10-31 21:59:36,139][root][INFO] - Step: 4131/4590  |  Loss: 0.6667  |  Score: 60.48 [%]  |  Seq Length: 256.0
[2024-10-31 21:59:40,324][root][INFO] - ########################  DEV REPORT #EP9  ########################
[2024-10-31 21:59:40,324][root][INFO] - Score: 47.87 [%]  |  Evaluation Time: 4.18 [s]
[2024-10-31 21:59:48,491][root][INFO] - ########################  TEST REPORT #EP9  ########################
[2024-10-31 21:59:48,491][root][INFO] - Score: 49.72 [%]  |  Evaluation Time: 8.17 [s]
[2024-10-31 21:59:48,494][root][INFO] - 
[10/ 10 Epoch]
[2024-10-31 21:59:56,268][root][INFO] - Step: 4200/4590  |  Loss: 0.6689  |  Score: 58.51 [%]  |  Seq Length: 256.0
[2024-10-31 22:00:07,157][root][INFO] - Step: 4300/4590  |  Loss: 0.6538  |  Score: 59.25 [%]  |  Seq Length: 256.0
[2024-10-31 22:00:18,068][root][INFO] - Step: 4400/4590  |  Loss: 0.6639  |  Score: 61.50 [%]  |  Seq Length: 256.0
[2024-10-31 22:00:28,859][root][INFO] - Step: 4500/4590  |  Loss: 0.6681  |  Score: 58.75 [%]  |  Seq Length: 256.0
[2024-10-31 22:00:38,493][root][INFO] - Step: 4590/4590  |  Loss: 0.6638  |  Score: 60.00 [%]  |  Seq Length: 256.0
[2024-10-31 22:00:42,676][root][INFO] - ########################  DEV REPORT #EP10  ########################
[2024-10-31 22:00:42,676][root][INFO] - Score: 48.30 [%]  |  Evaluation Time: 4.18 [s]
[2024-10-31 22:00:50,819][root][INFO] - ########################  TEST REPORT #EP10  ########################
[2024-10-31 22:00:50,819][root][INFO] - Score: 49.72 [%]  |  Evaluation Time: 8.14 [s]
[2024-10-31 22:00:50,820][root][INFO] - ########################  BEST RESULT  ########################
[2024-10-31 22:00:50,820][root][INFO] - - Epoch: 3
[2024-10-31 22:00:50,820][root][INFO] - - DEV score: 53.84 [%]
[2024-10-31 22:00:50,820][root][INFO] - - TEST score: 49.93 [%]
[2024-10-31 22:00:50,821][root][INFO] - Fine-tuning is done!
[2024-11-01 11:21:44,743][root][INFO] - 

[2024-11-01 11:21:44,743][root][INFO] - This train_log.txt inform the Running Progress.

[2024-11-01 11:21:44,743][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-11-01 11:21:44,743][root][INFO] - 

[2024-11-01 11:21:44,743][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-11-01 11:21:52,458][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-11-01 11:21:52,458][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-11-01 11:21:52,459][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-11-01 11:21:52,460][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-11-01 11:21:52,460][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-11-01 11:21:52,460][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-11-01 11:21:52,461][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-11-01 11:21:52,461][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-11-01 11:21:52,462][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-11-01 11:21:52,462][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-11-01 11:21:52,463][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-11-01 11:21:52,463][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-11-01 11:21:52,464][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-11-01 11:21:52,464][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-11-01 11:21:52,464][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-11-01 11:21:52,465][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-11-01 11:21:52,465][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-11-01 11:21:52,466][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-11-01 11:21:52,466][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-11-01 11:21:52,466][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-11-01 11:21:52,467][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-11-01 11:21:52,467][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-11-01 11:21:52,468][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-11-01 11:21:52,468][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-11-01 11:21:52,470][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-11-01 11:21:52,476][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-11-01 11:21:52,644][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-11-01 11:21:52,646][root][INFO] - Trainable params: 15525888 || all params: 140691456 || trainable: 11.04 %
[2024-11-01 11:21:52,828][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-11-01 11:21:54,754][root][INFO] - 

[2024-11-01 11:21:54,755][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-11-01 11:21:54,755][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-11-01 11:21:54,763][root][INFO] - vocab size              : 51200
[2024-11-01 11:21:54,763][root][INFO] - device                  : gpu
[2024-11-01 11:21:54,763][root][INFO] - random seed             : 1
[2024-11-01 11:21:54,763][root][INFO] - train data size         : 3672
[2024-11-01 11:21:54,763][root][INFO] - max epochs              : 10
[2024-11-01 11:21:54,763][root][INFO] - total steps             : 4590
[2024-11-01 11:21:54,763][root][INFO] - warmup steps            : 0
[2024-11-01 11:21:54,763][root][INFO] - batch size              : 8
[2024-11-01 11:21:54,763][root][INFO] - accumulation steps      : 1
[2024-11-01 11:21:54,764][root][INFO] - optimizer               : adamwscale
[2024-11-01 11:21:54,764][root][INFO] - lr_scheduler            : cosine
[2024-11-01 11:21:54,764][root][INFO] - learning rate           : 0.01
[2024-11-01 11:21:54,764][root][INFO] - max length              : 256

[2024-11-01 11:21:54,764][root][INFO] - LoRA Configuration
[2024-11-01 11:21:54,764][root][INFO] - ㄴ r                    : 32
[2024-11-01 11:21:54,764][root][INFO] - ㄴ alpha                : 128
[2024-11-01 11:21:54,764][root][INFO] - ㄴ dropout              : 0.03

[2024-11-01 11:21:54,764][root][INFO] - SUB2 Configuration
[2024-11-01 11:21:54,764][root][INFO] - ㄴ tok_type             : jamo_var
[2024-11-01 11:21:54,764][root][INFO] - ㄴ hidden_dim           : 768
[2024-11-01 11:21:54,765][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-11-01 11:21:54,765][root][INFO] - ㄴ embedding_norm       : False
[2024-11-01 11:21:54,765][root][INFO] - ㄴ do_combination       : False
[2024-11-01 11:21:54,765][root][INFO] - ㄴ reducer              : linear_pool

[2024-11-01 11:21:54,765][root][INFO] - LoRA in SUB2 Configuration
[2024-11-01 11:21:54,765][root][INFO] - ㄴ r                : 32
[2024-11-01 11:21:54,765][root][INFO] - ㄴ alpha            : 128
[2024-11-01 11:21:54,765][root][INFO] - ㄴ dropout          : 0.03

[2024-11-01 11:21:54,765][root][INFO] - 

[2024-11-01 11:21:54,765][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-11-01 11:21:54,765][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt
[2024-11-01 11:21:54,766][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb
[2024-11-01 11:21:54,766][root][INFO] - * tb interval   : 100

[2024-11-01 11:21:54,766][root][INFO] - 

[2024-11-01 11:21:54,766][root][INFO] - Start the Training !
[2024-11-01 11:21:54,769][root][INFO] - 
[1/ 10 Epoch]
[2024-11-01 11:22:07,445][root][INFO] - Step: 100/4590  |  Loss: 0.7795  |  Score: 49.12 [%]  |  Seq Length: 256.0
[2024-11-01 11:22:18,810][root][INFO] - Step: 200/4590  |  Loss: 0.7110  |  Score: 50.62 [%]  |  Seq Length: 256.0
[2024-11-01 11:22:30,123][root][INFO] - Step: 300/4590  |  Loss: 0.7318  |  Score: 48.38 [%]  |  Seq Length: 256.0
[2024-11-01 11:22:41,303][root][INFO] - Step: 400/4590  |  Loss: 0.7099  |  Score: 50.75 [%]  |  Seq Length: 256.0
[2024-11-01 11:22:48,419][root][INFO] - Step: 459/4590  |  Loss: 0.7076  |  Score: 50.85 [%]  |  Seq Length: 256.0
[2024-11-01 11:22:52,812][root][INFO] - ########################  DEV REPORT #EP1  ########################
[2024-11-01 11:22:52,812][root][INFO] - Score: 46.59 [%]  |  Evaluation Time: 4.39 [s]
[2024-11-01 11:23:01,732][root][INFO] - ########################  TEST REPORT #EP1  ########################
[2024-11-01 11:23:01,732][root][INFO] - Score: 50.28 [%]  |  Evaluation Time: 8.92 [s]
[2024-11-01 11:23:01,733][root][INFO] - 
Save new Best Score (Epoch: 1)
[2024-11-01 11:23:01,735][root][INFO] - 
[2/ 10 Epoch]
[2024-11-01 11:23:06,557][root][INFO] - Step: 500/4590  |  Loss: 0.7127  |  Score: 49.09 [%]  |  Seq Length: 256.0
[2024-11-01 11:23:17,950][root][INFO] - Step: 600/4590  |  Loss: 0.7066  |  Score: 50.75 [%]  |  Seq Length: 256.0
[2024-11-01 11:23:29,493][root][INFO] - Step: 700/4590  |  Loss: 0.7073  |  Score: 49.00 [%]  |  Seq Length: 256.0
[2024-11-01 11:23:41,408][root][INFO] - Step: 800/4590  |  Loss: 0.7039  |  Score: 50.00 [%]  |  Seq Length: 256.0
[2024-11-01 11:23:53,752][root][INFO] - Step: 900/4590  |  Loss: 0.7166  |  Score: 47.88 [%]  |  Seq Length: 256.0
[2024-11-01 11:23:55,800][root][INFO] - Step: 918/4590  |  Loss: 0.7198  |  Score: 36.81 [%]  |  Seq Length: 256.0
[2024-11-01 11:24:00,341][root][INFO] - ########################  DEV REPORT #EP2  ########################
[2024-11-01 11:24:00,342][root][INFO] - Score: 47.02 [%]  |  Evaluation Time: 4.54 [s]
[2024-11-01 11:25:48,226][root][INFO] - 

[2024-11-01 11:25:48,227][root][INFO] - This train_log.txt inform the Running Progress.

[2024-11-01 11:25:48,227][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-11-01 11:25:48,227][root][INFO] - 

[2024-11-01 11:25:48,227][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-11-01 11:25:56,386][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-11-01 11:25:56,386][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-11-01 11:25:56,387][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-11-01 11:25:56,387][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-11-01 11:25:56,388][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-11-01 11:25:56,388][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-11-01 11:25:56,389][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-11-01 11:25:56,389][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-11-01 11:25:56,389][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-11-01 11:25:56,390][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-11-01 11:25:56,390][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-11-01 11:25:56,391][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-11-01 11:25:56,391][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-11-01 11:25:56,392][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-11-01 11:25:56,392][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-11-01 11:25:56,392][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-11-01 11:25:56,393][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-11-01 11:25:56,393][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-11-01 11:25:56,394][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-11-01 11:25:56,394][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-11-01 11:25:56,395][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-11-01 11:25:56,395][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-11-01 11:25:56,395][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-11-01 11:25:56,396][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-11-01 11:25:56,397][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-11-01 11:25:56,404][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-11-01 11:25:56,604][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-11-01 11:25:56,606][root][INFO] - Trainable params: 15525888 || all params: 140691456 || trainable: 11.04 %
[2024-11-01 11:25:56,789][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-11-01 11:25:58,619][root][INFO] - 

[2024-11-01 11:25:58,620][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-11-01 11:25:58,620][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-11-01 11:25:58,628][root][INFO] - vocab size              : 51200
[2024-11-01 11:25:58,628][root][INFO] - device                  : gpu
[2024-11-01 11:25:58,628][root][INFO] - random seed             : 1
[2024-11-01 11:25:58,628][root][INFO] - train data size         : 3672
[2024-11-01 11:25:58,628][root][INFO] - max epochs              : 10
[2024-11-01 11:25:58,628][root][INFO] - total steps             : 4590
[2024-11-01 11:25:58,628][root][INFO] - warmup steps            : 0
[2024-11-01 11:25:58,629][root][INFO] - batch size              : 8
[2024-11-01 11:25:58,629][root][INFO] - accumulation steps      : 1
[2024-11-01 11:25:58,629][root][INFO] - optimizer               : adamwscale
[2024-11-01 11:25:58,629][root][INFO] - lr_scheduler            : cosine
[2024-11-01 11:25:58,629][root][INFO] - learning rate           : 0.01
[2024-11-01 11:25:58,629][root][INFO] - max length              : 256

[2024-11-01 11:25:58,629][root][INFO] - LoRA Configuration
[2024-11-01 11:25:58,629][root][INFO] - ㄴ r                    : 32
[2024-11-01 11:25:58,629][root][INFO] - ㄴ alpha                : 128
[2024-11-01 11:25:58,629][root][INFO] - ㄴ dropout              : 0.03

[2024-11-01 11:25:58,629][root][INFO] - SUB2 Configuration
[2024-11-01 11:25:58,629][root][INFO] - ㄴ tok_type             : jamo_var
[2024-11-01 11:25:58,630][root][INFO] - ㄴ hidden_dim           : 768
[2024-11-01 11:25:58,630][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-11-01 11:25:58,630][root][INFO] - ㄴ embedding_norm       : False
[2024-11-01 11:25:58,630][root][INFO] - ㄴ do_combination       : False
[2024-11-01 11:25:58,630][root][INFO] - ㄴ reducer              : linear_pool

[2024-11-01 11:25:58,630][root][INFO] - LoRA in SUB2 Configuration
[2024-11-01 11:25:58,630][root][INFO] - ㄴ r                : 32
[2024-11-01 11:25:58,630][root][INFO] - ㄴ alpha            : 128
[2024-11-01 11:25:58,630][root][INFO] - ㄴ dropout          : 0.03

[2024-11-01 11:25:58,630][root][INFO] - 

[2024-11-01 11:25:58,631][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-11-01 11:25:58,631][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt
[2024-11-01 11:25:58,631][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb
[2024-11-01 11:25:58,631][root][INFO] - * tb interval   : 100

[2024-11-01 11:25:58,631][root][INFO] - 

[2024-11-01 11:25:58,631][root][INFO] - Start the Training !
[2024-11-01 11:25:58,634][root][INFO] - 
[1/ 10 Epoch]
[2024-11-01 11:26:10,904][root][INFO] - Step: 100/4590  |  Loss: 0.8784  |  Score: 50.62 [%]  |  Seq Length: 256.0
[2024-11-01 11:26:21,790][root][INFO] - Step: 200/4590  |  Loss: 0.7482  |  Score: 49.75 [%]  |  Seq Length: 256.0
[2024-11-01 11:26:32,883][root][INFO] - Step: 300/4590  |  Loss: 0.7649  |  Score: 48.00 [%]  |  Seq Length: 256.0
[2024-11-01 11:26:43,798][root][INFO] - Step: 400/4590  |  Loss: 0.7191  |  Score: 50.62 [%]  |  Seq Length: 256.0
[2024-11-01 11:26:50,715][root][INFO] - Step: 459/4590  |  Loss: 0.7230  |  Score: 47.25 [%]  |  Seq Length: 256.0
[2024-11-01 11:26:55,205][root][INFO] - ########################  DEV REPORT #EP1  ########################
[2024-11-01 11:26:55,206][root][INFO] - Score: 50.57 [%]  |  Evaluation Time: 4.49 [s]
[2024-11-01 11:27:03,646][root][INFO] - ########################  TEST REPORT #EP1  ########################
[2024-11-01 11:27:03,647][root][INFO] - Score: 49.79 [%]  |  Evaluation Time: 8.44 [s]
[2024-11-01 11:27:03,648][root][INFO] - 
Save new Best Score (Epoch: 1)
[2024-11-01 11:27:03,650][root][INFO] - 
[2/ 10 Epoch]
[2024-11-01 11:27:08,512][root][INFO] - Step: 500/4590  |  Loss: 0.7280  |  Score: 50.30 [%]  |  Seq Length: 256.0
[2024-11-01 11:27:20,565][root][INFO] - Step: 600/4590  |  Loss: 0.7331  |  Score: 47.75 [%]  |  Seq Length: 256.0
[2024-11-01 11:27:32,401][root][INFO] - Step: 700/4590  |  Loss: 0.7341  |  Score: 49.50 [%]  |  Seq Length: 256.0
[2024-11-01 11:27:43,474][root][INFO] - Step: 800/4590  |  Loss: 0.7393  |  Score: 47.25 [%]  |  Seq Length: 256.0
[2024-11-01 11:27:54,500][root][INFO] - Step: 900/4590  |  Loss: 0.7349  |  Score: 49.62 [%]  |  Seq Length: 256.0
[2024-11-01 11:27:56,569][root][INFO] - Step: 918/4590  |  Loss: 0.7138  |  Score: 43.06 [%]  |  Seq Length: 256.0
[2024-11-01 11:28:00,794][root][INFO] - ########################  DEV REPORT #EP2  ########################
[2024-11-01 11:28:00,795][root][INFO] - Score: 52.98 [%]  |  Evaluation Time: 4.22 [s]
[2024-11-01 11:28:09,175][root][INFO] - ########################  TEST REPORT #EP2  ########################
[2024-11-01 11:28:09,175][root][INFO] - Score: 49.29 [%]  |  Evaluation Time: 8.38 [s]
[2024-11-01 11:28:09,177][root][INFO] - 
Save new Best Score (Epoch: 2)
[2024-11-01 11:28:09,178][root][INFO] - 
[3/ 10 Epoch]
[2024-11-01 11:28:19,232][root][INFO] - Step: 1000/4590  |  Loss: 0.7101  |  Score: 51.68 [%]  |  Seq Length: 256.0
[2024-11-01 11:28:30,709][root][INFO] - Step: 1100/4590  |  Loss: 0.6982  |  Score: 54.25 [%]  |  Seq Length: 256.0
[2024-11-01 11:28:42,135][root][INFO] - Step: 1200/4590  |  Loss: 0.7194  |  Score: 49.75 [%]  |  Seq Length: 256.0
[2024-11-01 11:28:53,684][root][INFO] - Step: 1300/4590  |  Loss: 0.7141  |  Score: 46.88 [%]  |  Seq Length: 256.0
[2024-11-01 11:29:02,617][root][INFO] - Step: 1377/4590  |  Loss: 0.7052  |  Score: 49.03 [%]  |  Seq Length: 256.0
[2024-11-01 11:29:06,897][root][INFO] - ########################  DEV REPORT #EP3  ########################
[2024-11-01 11:29:06,897][root][INFO] - Score: 52.98 [%]  |  Evaluation Time: 4.28 [s]
[2024-11-01 11:29:15,173][root][INFO] - ########################  TEST REPORT #EP3  ########################
[2024-11-01 11:29:15,174][root][INFO] - Score: 50.78 [%]  |  Evaluation Time: 8.27 [s]
[2024-11-01 11:29:15,175][root][INFO] - 
Save new Best Score (Epoch: 3)
[2024-11-01 11:29:15,176][root][INFO] - 
[4/ 10 Epoch]
[2024-11-01 11:29:18,171][root][INFO] - Step: 1400/4590  |  Loss: 0.7075  |  Score: 50.00 [%]  |  Seq Length: 256.0
[2024-11-01 11:29:29,451][root][INFO] - Step: 1500/4590  |  Loss: 0.7024  |  Score: 49.38 [%]  |  Seq Length: 256.0
[2024-11-01 11:29:40,682][root][INFO] - Step: 1600/4590  |  Loss: 0.7010  |  Score: 50.25 [%]  |  Seq Length: 256.0
[2024-11-01 11:29:51,719][root][INFO] - Step: 1700/4590  |  Loss: 0.6977  |  Score: 54.50 [%]  |  Seq Length: 256.0
[2024-11-01 11:30:02,903][root][INFO] - Step: 1800/4590  |  Loss: 0.7051  |  Score: 50.00 [%]  |  Seq Length: 256.0
[2024-11-01 11:30:06,879][root][INFO] - Step: 1836/4590  |  Loss: 0.6936  |  Score: 53.47 [%]  |  Seq Length: 256.0
[2024-11-01 11:30:11,132][root][INFO] - ########################  DEV REPORT #EP4  ########################
[2024-11-01 11:30:11,133][root][INFO] - Score: 48.30 [%]  |  Evaluation Time: 4.25 [s]
[2024-11-01 11:30:19,360][root][INFO] - ########################  TEST REPORT #EP4  ########################
[2024-11-01 11:30:19,361][root][INFO] - Score: 49.50 [%]  |  Evaluation Time: 8.22 [s]
[2024-11-01 11:30:19,363][root][INFO] - 
[5/ 10 Epoch]
[2024-11-01 11:30:26,798][root][INFO] - Step: 1900/4590  |  Loss: 0.7056  |  Score: 49.02 [%]  |  Seq Length: 256.0
[2024-11-01 11:30:38,241][root][INFO] - Step: 2000/4590  |  Loss: 0.6987  |  Score: 52.50 [%]  |  Seq Length: 256.0
[2024-11-01 14:24:41,233][root][INFO] - 

[2024-11-01 14:24:41,233][root][INFO] - This train_log.txt inform the Running Progress.

[2024-11-01 14:24:41,233][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-11-01 14:24:41,233][root][INFO] - 

[2024-11-01 14:24:41,233][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-11-01 14:24:49,318][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-11-01 14:24:49,319][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-11-01 14:24:49,319][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-11-01 14:24:49,320][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-11-01 14:24:49,321][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-11-01 14:24:49,321][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-11-01 14:24:49,321][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-11-01 14:24:49,322][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-11-01 14:24:49,322][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-11-01 14:24:49,323][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-11-01 14:24:49,323][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-11-01 14:24:49,324][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-11-01 14:24:49,324][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-11-01 14:24:49,325][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-11-01 14:24:49,325][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-11-01 14:24:49,326][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-11-01 14:24:49,326][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-11-01 14:24:49,327][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-11-01 14:24:49,327][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-11-01 14:24:49,328][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-11-01 14:24:49,328][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-11-01 14:24:49,329][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-11-01 14:24:49,329][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-11-01 14:24:49,330][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-11-01 14:24:49,332][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-11-01 14:24:49,337][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-11-01 14:24:49,499][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-11-01 14:24:49,501][root][INFO] - Trainable params: 15525888 || all params: 140691456 || trainable: 11.04 %
[2024-11-01 14:24:49,682][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-11-01 14:24:51,538][root][INFO] - 

[2024-11-01 14:24:51,538][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-11-01 14:24:51,539][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-11-01 14:24:51,546][root][INFO] - vocab size              : 51200
[2024-11-01 14:24:51,546][root][INFO] - device                  : gpu
[2024-11-01 14:24:51,546][root][INFO] - random seed             : 1
[2024-11-01 14:24:51,546][root][INFO] - train data size         : 3672
[2024-11-01 14:24:51,546][root][INFO] - max epochs              : 10
[2024-11-01 14:24:51,546][root][INFO] - total steps             : 4590
[2024-11-01 14:24:51,546][root][INFO] - warmup steps            : 0
[2024-11-01 14:24:51,546][root][INFO] - batch size              : 8
[2024-11-01 14:24:51,547][root][INFO] - accumulation steps      : 1
[2024-11-01 14:24:51,547][root][INFO] - optimizer               : adamwscale
[2024-11-01 14:24:51,547][root][INFO] - lr_scheduler            : cosine
[2024-11-01 14:24:51,547][root][INFO] - learning rate           : 0.01
[2024-11-01 14:24:51,547][root][INFO] - max length              : 256

[2024-11-01 14:24:51,547][root][INFO] - LoRA Configuration
[2024-11-01 14:24:51,547][root][INFO] - ㄴ r                    : 32
[2024-11-01 14:24:51,547][root][INFO] - ㄴ alpha                : 128
[2024-11-01 14:24:51,547][root][INFO] - ㄴ dropout              : 0.03

[2024-11-01 14:24:51,547][root][INFO] - SUB2 Configuration
[2024-11-01 14:24:51,547][root][INFO] - ㄴ tok_type             : jamo_var
[2024-11-01 14:24:51,548][root][INFO] - ㄴ hidden_dim           : 768
[2024-11-01 14:24:51,548][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-11-01 14:24:51,548][root][INFO] - ㄴ embedding_norm       : False
[2024-11-01 14:24:51,548][root][INFO] - ㄴ do_combination       : False
[2024-11-01 14:24:51,548][root][INFO] - ㄴ reducer              : linear_pool

[2024-11-01 14:24:51,548][root][INFO] - LoRA in SUB2 Configuration
[2024-11-01 14:24:51,548][root][INFO] - ㄴ r                : 32
[2024-11-01 14:24:51,548][root][INFO] - ㄴ alpha            : 128
[2024-11-01 14:24:51,548][root][INFO] - ㄴ dropout          : 0.03

[2024-11-01 14:24:51,548][root][INFO] - 

[2024-11-01 14:24:51,548][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-11-01 14:24:51,549][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt
[2024-11-01 14:24:51,549][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb
[2024-11-01 14:24:51,549][root][INFO] - * tb interval   : 100

[2024-11-01 14:24:51,549][root][INFO] - 

[2024-11-01 14:24:51,549][root][INFO] - Start the Training !
[2024-11-01 14:24:51,551][root][INFO] - 
[1/ 10 Epoch]
[2024-11-01 14:25:03,407][root][INFO] - Step: 100/4590  |  Loss: 0.7746  |  Score: 50.00 [%]  |  Seq Length: 256.0
[2024-11-01 14:25:13,763][root][INFO] - Step: 200/4590  |  Loss: 0.7326  |  Score: 49.38 [%]  |  Seq Length: 256.0
[2024-11-01 14:25:24,080][root][INFO] - Step: 300/4590  |  Loss: 0.7452  |  Score: 48.75 [%]  |  Seq Length: 256.0
[2024-11-01 14:25:34,313][root][INFO] - Step: 400/4590  |  Loss: 0.7211  |  Score: 50.25 [%]  |  Seq Length: 256.0
[2024-11-01 14:25:40,856][root][INFO] - Step: 459/4590  |  Loss: 0.7276  |  Score: 49.15 [%]  |  Seq Length: 256.0
[2024-11-01 14:25:44,950][root][INFO] - ########################  DEV REPORT #EP1  ########################
[2024-11-01 14:25:44,950][root][INFO] - Score: 52.41 [%]  |  Evaluation Time: 4.09 [s]
[2024-11-01 14:25:53,066][root][INFO] - ########################  TEST REPORT #EP1  ########################
[2024-11-01 14:25:53,066][root][INFO] - Score: 49.50 [%]  |  Evaluation Time: 8.11 [s]
[2024-11-01 14:25:53,067][root][INFO] - 
Save new Best Score (Epoch: 1)
[2024-11-01 14:25:53,068][root][INFO] - 
[2/ 10 Epoch]
[2024-11-01 14:25:57,640][root][INFO] - Step: 500/4590  |  Loss: 0.7170  |  Score: 46.34 [%]  |  Seq Length: 256.0
[2024-11-01 14:26:06,961][root][INFO] - Step: 600/4590  |  Loss: 0.7078  |  Score: 49.12 [%]  |  Seq Length: 256.0
[2024-11-01 14:26:17,187][root][INFO] - Step: 700/4590  |  Loss: 0.7072  |  Score: 48.88 [%]  |  Seq Length: 256.0
[2024-11-01 14:26:27,399][root][INFO] - Step: 800/4590  |  Loss: 0.7141  |  Score: 49.00 [%]  |  Seq Length: 256.0
[2024-11-01 14:26:37,784][root][INFO] - Step: 900/4590  |  Loss: 0.7235  |  Score: 49.25 [%]  |  Seq Length: 256.0
[2024-11-01 14:26:39,727][root][INFO] - Step: 918/4590  |  Loss: 0.7003  |  Score: 44.44 [%]  |  Seq Length: 256.0
[2024-11-01 14:26:43,888][root][INFO] - ########################  DEV REPORT #EP2  ########################
[2024-11-01 14:26:43,888][root][INFO] - Score: 46.59 [%]  |  Evaluation Time: 4.16 [s]
[2024-11-01 14:26:51,937][root][INFO] - ########################  TEST REPORT #EP2  ########################
[2024-11-01 14:26:51,937][root][INFO] - Score: 50.36 [%]  |  Evaluation Time: 8.05 [s]
[2024-11-01 14:26:51,939][root][INFO] - 
[3/ 10 Epoch]
[2024-11-01 14:27:01,193][root][INFO] - Step: 1000/4590  |  Loss: 0.6958  |  Score: 50.76 [%]  |  Seq Length: 256.0
[2024-11-01 14:27:11,734][root][INFO] - Step: 1100/4590  |  Loss: 0.6924  |  Score: 49.88 [%]  |  Seq Length: 256.0
[2024-11-01 14:27:22,357][root][INFO] - Step: 1200/4590  |  Loss: 0.7079  |  Score: 49.25 [%]  |  Seq Length: 256.0
[2024-11-01 14:27:33,050][root][INFO] - Step: 1300/4590  |  Loss: 0.6946  |  Score: 49.50 [%]  |  Seq Length: 256.0
[2024-11-01 14:27:42,358][root][INFO] - Step: 1377/4590  |  Loss: 0.7051  |  Score: 49.51 [%]  |  Seq Length: 256.0
[2024-11-01 14:27:46,603][root][INFO] - ########################  DEV REPORT #EP3  ########################
[2024-11-01 14:27:46,603][root][INFO] - Score: 50.99 [%]  |  Evaluation Time: 4.24 [s]
[2024-11-01 14:27:54,733][root][INFO] - ########################  TEST REPORT #EP3  ########################
[2024-11-01 14:27:54,733][root][INFO] - Score: 52.27 [%]  |  Evaluation Time: 8.13 [s]
[2024-11-01 14:27:54,734][root][INFO] - 
Save new Best Score (Epoch: 3)
[2024-11-01 14:27:54,735][root][INFO] - 
[4/ 10 Epoch]
[2024-11-01 14:27:57,462][root][INFO] - Step: 1400/4590  |  Loss: 0.7056  |  Score: 47.28 [%]  |  Seq Length: 256.0
[2024-11-01 14:28:07,978][root][INFO] - Step: 1500/4590  |  Loss: 0.6984  |  Score: 52.88 [%]  |  Seq Length: 256.0
[2024-11-01 14:28:18,521][root][INFO] - Step: 1600/4590  |  Loss: 0.6914  |  Score: 55.25 [%]  |  Seq Length: 256.0
[2024-11-01 14:28:29,071][root][INFO] - Step: 1700/4590  |  Loss: 0.6958  |  Score: 53.12 [%]  |  Seq Length: 256.0
[2024-11-01 14:28:39,499][root][INFO] - Step: 1800/4590  |  Loss: 0.6896  |  Score: 52.38 [%]  |  Seq Length: 256.0
[2024-11-01 14:28:43,383][root][INFO] - Step: 1836/4590  |  Loss: 0.6829  |  Score: 55.56 [%]  |  Seq Length: 256.0
[2024-11-01 14:28:47,594][root][INFO] - ########################  DEV REPORT #EP4  ########################
[2024-11-01 14:28:47,594][root][INFO] - Score: 48.58 [%]  |  Evaluation Time: 4.21 [s]
[2024-11-01 14:28:56,167][root][INFO] - ########################  TEST REPORT #EP4  ########################
[2024-11-01 14:28:56,167][root][INFO] - Score: 50.28 [%]  |  Evaluation Time: 8.57 [s]
[2024-11-01 14:28:56,172][root][INFO] - 
[5/ 10 Epoch]
[2024-11-01 14:29:03,640][root][INFO] - Step: 1900/4590  |  Loss: 0.6881  |  Score: 51.95 [%]  |  Seq Length: 256.0
[2024-11-01 14:29:14,659][root][INFO] - Step: 2000/4590  |  Loss: 0.6934  |  Score: 53.75 [%]  |  Seq Length: 256.0
[2024-11-01 14:29:25,661][root][INFO] - Step: 2100/4590  |  Loss: 0.6764  |  Score: 56.38 [%]  |  Seq Length: 256.0
[2024-11-01 14:29:36,490][root][INFO] - Step: 2200/4590  |  Loss: 0.6800  |  Score: 54.87 [%]  |  Seq Length: 256.0
[2024-11-01 14:29:47,473][root][INFO] - Step: 2295/4590  |  Loss: 0.6900  |  Score: 53.29 [%]  |  Seq Length: 256.0
[2024-11-01 14:29:51,680][root][INFO] - ########################  DEV REPORT #EP5  ########################
[2024-11-01 14:29:51,680][root][INFO] - Score: 53.84 [%]  |  Evaluation Time: 4.20 [s]
[2024-11-01 14:29:59,661][root][INFO] - ########################  TEST REPORT #EP5  ########################
[2024-11-01 14:29:59,661][root][INFO] - Score: 50.21 [%]  |  Evaluation Time: 7.98 [s]
[2024-11-01 14:29:59,662][root][INFO] - 
Save new Best Score (Epoch: 5)
[2024-11-01 14:29:59,663][root][INFO] - 
[6/ 10 Epoch]
[2024-11-01 14:30:00,487][root][INFO] - Step: 2300/4590  |  Loss: 0.6800  |  Score: 52.50 [%]  |  Seq Length: 256.0
[2024-11-01 14:30:11,174][root][INFO] - Step: 2400/4590  |  Loss: 0.6603  |  Score: 57.50 [%]  |  Seq Length: 256.0
[2024-11-01 14:30:22,513][root][INFO] - Step: 2500/4590  |  Loss: 0.6779  |  Score: 55.38 [%]  |  Seq Length: 256.0
[2024-11-01 14:30:33,568][root][INFO] - Step: 2600/4590  |  Loss: 0.6826  |  Score: 55.50 [%]  |  Seq Length: 256.0
[2024-11-01 14:30:45,029][root][INFO] - Step: 2700/4590  |  Loss: 0.6695  |  Score: 55.12 [%]  |  Seq Length: 256.0
[2024-11-01 14:30:51,337][root][INFO] - Step: 2754/4590  |  Loss: 0.7039  |  Score: 56.71 [%]  |  Seq Length: 256.0
[2024-11-01 14:30:55,591][root][INFO] - ########################  DEV REPORT #EP6  ########################
[2024-11-01 14:30:55,592][root][INFO] - Score: 48.72 [%]  |  Evaluation Time: 4.25 [s]
[2024-11-01 14:31:04,005][root][INFO] - ########################  TEST REPORT #EP6  ########################
[2024-11-01 14:31:04,006][root][INFO] - Score: 50.36 [%]  |  Evaluation Time: 8.41 [s]
[2024-11-01 14:31:04,008][root][INFO] - 
[7/ 10 Epoch]
[2024-11-01 14:31:09,308][root][INFO] - Step: 2800/4590  |  Loss: 0.6398  |  Score: 60.33 [%]  |  Seq Length: 256.0
[2024-11-01 14:31:20,208][root][INFO] - Step: 2900/4590  |  Loss: 0.6532  |  Score: 58.25 [%]  |  Seq Length: 256.0
[2024-11-01 14:31:30,793][root][INFO] - Step: 3000/4590  |  Loss: 0.6629  |  Score: 55.38 [%]  |  Seq Length: 256.0
[2024-11-01 14:31:41,601][root][INFO] - Step: 3100/4590  |  Loss: 0.6615  |  Score: 59.38 [%]  |  Seq Length: 256.0
[2024-11-01 14:31:52,577][root][INFO] - Step: 3200/4590  |  Loss: 0.6924  |  Score: 55.00 [%]  |  Seq Length: 256.0
[2024-11-01 14:31:54,085][root][INFO] - Step: 3213/4590  |  Loss: 0.6553  |  Score: 58.65 [%]  |  Seq Length: 256.0
[2024-11-01 14:31:58,613][root][INFO] - ########################  DEV REPORT #EP7  ########################
[2024-11-01 14:31:58,614][root][INFO] - Score: 48.01 [%]  |  Evaluation Time: 4.53 [s]
[2024-11-01 14:32:06,746][root][INFO] - ########################  TEST REPORT #EP7  ########################
[2024-11-01 14:32:06,746][root][INFO] - Score: 50.21 [%]  |  Evaluation Time: 8.13 [s]
[2024-11-01 14:32:06,749][root][INFO] - 
[8/ 10 Epoch]
[2024-11-01 14:32:16,245][root][INFO] - Step: 3300/4590  |  Loss: 0.6384  |  Score: 58.05 [%]  |  Seq Length: 256.0
[2024-11-01 14:32:26,654][root][INFO] - Step: 3400/4590  |  Loss: 0.6346  |  Score: 61.12 [%]  |  Seq Length: 256.0
[2024-11-01 14:32:37,069][root][INFO] - Step: 3500/4590  |  Loss: 0.6758  |  Score: 57.63 [%]  |  Seq Length: 256.0
[2024-11-01 14:32:47,512][root][INFO] - Step: 3600/4590  |  Loss: 0.6568  |  Score: 56.62 [%]  |  Seq Length: 256.0
[2024-11-01 14:32:55,668][root][INFO] - Step: 3672/4590  |  Loss: 0.6259  |  Score: 61.28 [%]  |  Seq Length: 256.0
[2024-11-01 14:32:59,904][root][INFO] - ########################  DEV REPORT #EP8  ########################
[2024-11-01 14:32:59,904][root][INFO] - Score: 48.44 [%]  |  Evaluation Time: 4.23 [s]
[2024-11-01 14:33:07,994][root][INFO] - ########################  TEST REPORT #EP8  ########################
[2024-11-01 14:33:07,995][root][INFO] - Score: 50.57 [%]  |  Evaluation Time: 8.09 [s]
[2024-11-01 14:33:07,997][root][INFO] - 
[9/ 10 Epoch]
[2024-11-01 14:33:11,366][root][INFO] - Step: 3700/4590  |  Loss: 0.6384  |  Score: 57.14 [%]  |  Seq Length: 256.0
[2024-11-01 14:33:22,369][root][INFO] - Step: 3800/4590  |  Loss: 0.6165  |  Score: 63.38 [%]  |  Seq Length: 256.0
[2024-11-01 14:33:33,087][root][INFO] - Step: 3900/4590  |  Loss: 0.6370  |  Score: 59.13 [%]  |  Seq Length: 256.0
[2024-11-01 14:33:44,041][root][INFO] - Step: 4000/4590  |  Loss: 0.6450  |  Score: 57.63 [%]  |  Seq Length: 256.0
[2024-11-01 14:33:54,735][root][INFO] - Step: 4100/4590  |  Loss: 0.6318  |  Score: 63.88 [%]  |  Seq Length: 256.0
[2024-11-01 14:33:58,047][root][INFO] - Step: 4131/4590  |  Loss: 0.6778  |  Score: 59.27 [%]  |  Seq Length: 256.0
[2024-11-01 14:34:02,327][root][INFO] - ########################  DEV REPORT #EP9  ########################
[2024-11-01 14:34:02,327][root][INFO] - Score: 48.44 [%]  |  Evaluation Time: 4.28 [s]
[2024-11-01 14:34:10,454][root][INFO] - ########################  TEST REPORT #EP9  ########################
[2024-11-01 14:34:10,454][root][INFO] - Score: 51.42 [%]  |  Evaluation Time: 8.12 [s]
[2024-11-01 14:34:10,456][root][INFO] - 
[10/ 10 Epoch]
[2024-11-01 14:34:18,123][root][INFO] - Step: 4200/4590  |  Loss: 0.6366  |  Score: 62.68 [%]  |  Seq Length: 256.0
[2024-11-01 14:34:29,131][root][INFO] - Step: 4300/4590  |  Loss: 0.6279  |  Score: 60.75 [%]  |  Seq Length: 256.0
[2024-11-01 14:34:40,043][root][INFO] - Step: 4400/4590  |  Loss: 0.6163  |  Score: 62.38 [%]  |  Seq Length: 256.0
[2024-11-01 14:34:50,674][root][INFO] - Step: 4500/4590  |  Loss: 0.6431  |  Score: 61.00 [%]  |  Seq Length: 256.0
[2024-11-01 14:35:00,282][root][INFO] - Step: 4590/4590  |  Loss: 0.6123  |  Score: 65.97 [%]  |  Seq Length: 256.0
[2024-11-01 14:35:04,554][root][INFO] - ########################  DEV REPORT #EP10  ########################
[2024-11-01 14:35:04,555][root][INFO] - Score: 48.86 [%]  |  Evaluation Time: 4.27 [s]
[2024-11-01 14:35:12,679][root][INFO] - ########################  TEST REPORT #EP10  ########################
[2024-11-01 14:35:12,679][root][INFO] - Score: 51.35 [%]  |  Evaluation Time: 8.12 [s]
[2024-11-01 14:35:12,681][root][INFO] - ########################  BEST RESULT  ########################
[2024-11-01 14:35:12,681][root][INFO] - - Epoch: 5
[2024-11-01 14:35:12,681][root][INFO] - - DEV score: 53.84 [%]
[2024-11-01 14:35:12,681][root][INFO] - - TEST score: 50.21 [%]
[2024-11-01 14:35:12,682][root][INFO] - Fine-tuning is done!
[2024-11-01 14:43:45,460][root][INFO] - 

[2024-11-01 14:43:45,460][root][INFO] - This train_log.txt inform the Running Progress.

[2024-11-01 14:43:45,460][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-11-01 14:43:45,460][root][INFO] - 

[2024-11-01 14:43:45,460][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-11-01 14:43:53,087][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-11-01 14:43:53,087][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-11-01 14:43:53,088][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-11-01 14:43:53,088][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-11-01 14:43:53,089][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-11-01 14:43:53,089][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-11-01 14:43:53,090][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-11-01 14:43:53,090][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-11-01 14:43:53,091][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-11-01 14:43:53,092][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-11-01 14:43:53,092][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-11-01 14:43:53,093][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-11-01 14:43:53,093][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-11-01 14:43:53,094][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-11-01 14:43:53,094][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-11-01 14:43:53,095][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-11-01 14:43:53,096][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-11-01 14:43:53,096][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-11-01 14:43:53,097][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-11-01 14:43:53,097][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-11-01 14:43:53,098][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-11-01 14:43:53,098][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-11-01 14:43:53,099][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-11-01 14:43:53,100][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-11-01 14:43:53,102][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-11-01 14:43:53,106][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-11-01 14:43:53,271][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-11-01 14:43:53,273][root][INFO] - Trainable params: 15525888 || all params: 140691456 || trainable: 11.04 %
[2024-11-01 14:43:53,463][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-11-01 14:43:55,351][root][INFO] - 

[2024-11-01 14:43:55,352][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-11-01 14:43:55,352][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-11-01 14:43:55,362][root][INFO] - vocab size              : 51200
[2024-11-01 14:43:55,362][root][INFO] - device                  : gpu
[2024-11-01 14:43:55,362][root][INFO] - random seed             : 1
[2024-11-01 14:43:55,363][root][INFO] - train data size         : 3672
[2024-11-01 14:43:55,363][root][INFO] - max epochs              : 10
[2024-11-01 14:43:55,363][root][INFO] - total steps             : 4590
[2024-11-01 14:43:55,363][root][INFO] - warmup steps            : 0
[2024-11-01 14:43:55,363][root][INFO] - batch size              : 8
[2024-11-01 14:43:55,363][root][INFO] - accumulation steps      : 1
[2024-11-01 14:43:55,363][root][INFO] - optimizer               : adamwscale
[2024-11-01 14:43:55,363][root][INFO] - lr_scheduler            : cosine
[2024-11-01 14:43:55,363][root][INFO] - learning rate           : 0.01
[2024-11-01 14:43:55,363][root][INFO] - max length              : 256

[2024-11-01 14:43:55,363][root][INFO] - LoRA Configuration
[2024-11-01 14:43:55,363][root][INFO] - ㄴ r                    : 32
[2024-11-01 14:43:55,364][root][INFO] - ㄴ alpha                : 128
[2024-11-01 14:43:55,364][root][INFO] - ㄴ dropout              : 0.03

[2024-11-01 14:43:55,364][root][INFO] - SUB2 Configuration
[2024-11-01 14:43:55,364][root][INFO] - ㄴ tok_type             : jamo_var
[2024-11-01 14:43:55,364][root][INFO] - ㄴ hidden_dim           : 768
[2024-11-01 14:43:55,364][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-11-01 14:43:55,364][root][INFO] - ㄴ embedding_norm       : False
[2024-11-01 14:43:55,364][root][INFO] - ㄴ do_combination       : False
[2024-11-01 14:43:55,364][root][INFO] - ㄴ reducer              : linear_pool

[2024-11-01 14:43:55,365][root][INFO] - LoRA in SUB2 Configuration
[2024-11-01 14:43:55,365][root][INFO] - ㄴ r                : 32
[2024-11-01 14:43:55,365][root][INFO] - ㄴ alpha            : 128
[2024-11-01 14:43:55,365][root][INFO] - ㄴ dropout          : 0.03

[2024-11-01 14:43:55,365][root][INFO] - 

[2024-11-01 14:43:55,365][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-11-01 14:43:55,365][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt
[2024-11-01 14:43:55,365][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb
[2024-11-01 14:43:55,365][root][INFO] - * tb interval   : 100

[2024-11-01 14:43:55,365][root][INFO] - 

[2024-11-01 14:43:55,365][root][INFO] - Start the Training !
[2024-11-01 14:43:55,368][root][INFO] - 
[1/ 10 Epoch]
[2024-11-01 14:45:17,322][root][INFO] - 

[2024-11-01 14:45:17,322][root][INFO] - This train_log.txt inform the Running Progress.

[2024-11-01 14:45:17,322][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-11-01 14:45:17,322][root][INFO] - 

[2024-11-01 14:45:17,322][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-11-01 14:45:24,986][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-11-01 14:45:24,987][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-11-01 14:45:24,987][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-11-01 14:45:24,988][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-11-01 14:45:24,988][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-11-01 14:45:24,989][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-11-01 14:45:24,989][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-11-01 14:45:24,990][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-11-01 14:45:24,990][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-11-01 14:45:24,991][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-11-01 14:45:24,992][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-11-01 14:45:24,992][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-11-01 14:45:24,993][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-11-01 14:45:24,993][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-11-01 14:45:24,994][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-11-01 14:45:24,994][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-11-01 14:45:24,995][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-11-01 14:45:24,995][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-11-01 14:45:24,996][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-11-01 14:45:24,996][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-11-01 14:45:24,997][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-11-01 14:45:24,997][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-11-01 14:45:24,998][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-11-01 14:45:24,999][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-11-01 14:45:25,001][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-11-01 14:45:25,006][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-11-01 14:45:25,170][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-11-01 14:45:25,173][root][INFO] - Trainable params: 15525888 || all params: 140691456 || trainable: 11.04 %
[2024-11-01 14:45:25,353][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-11-01 14:45:27,291][root][INFO] - 

[2024-11-01 14:45:27,291][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-11-01 14:45:27,291][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-11-01 14:45:27,299][root][INFO] - vocab size              : 51200
[2024-11-01 14:45:27,299][root][INFO] - device                  : gpu
[2024-11-01 14:45:27,299][root][INFO] - random seed             : 1
[2024-11-01 14:45:27,300][root][INFO] - train data size         : 3672
[2024-11-01 14:45:27,300][root][INFO] - max epochs              : 10
[2024-11-01 14:45:27,300][root][INFO] - total steps             : 4590
[2024-11-01 14:45:27,300][root][INFO] - warmup steps            : 0
[2024-11-01 14:45:27,300][root][INFO] - batch size              : 8
[2024-11-01 14:45:27,300][root][INFO] - accumulation steps      : 1
[2024-11-01 14:45:27,300][root][INFO] - optimizer               : adamwscale
[2024-11-01 14:45:27,300][root][INFO] - lr_scheduler            : cosine
[2024-11-01 14:45:27,300][root][INFO] - learning rate           : 0.01
[2024-11-01 14:45:27,300][root][INFO] - max length              : 256

[2024-11-01 14:45:27,300][root][INFO] - LoRA Configuration
[2024-11-01 14:45:27,300][root][INFO] - ㄴ r                    : 32
[2024-11-01 14:45:27,301][root][INFO] - ㄴ alpha                : 128
[2024-11-01 14:45:27,301][root][INFO] - ㄴ dropout              : 0.03

[2024-11-01 14:45:27,301][root][INFO] - SUB2 Configuration
[2024-11-01 14:45:27,301][root][INFO] - ㄴ tok_type             : jamo_var
[2024-11-01 14:45:27,301][root][INFO] - ㄴ hidden_dim           : 768
[2024-11-01 14:45:27,301][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-11-01 14:45:27,301][root][INFO] - ㄴ embedding_norm       : False
[2024-11-01 14:45:27,301][root][INFO] - ㄴ do_combination       : False
[2024-11-01 14:45:27,301][root][INFO] - ㄴ reducer              : linear_pool

[2024-11-01 14:45:27,302][root][INFO] - LoRA in SUB2 Configuration
[2024-11-01 14:45:27,302][root][INFO] - ㄴ r                : 32
[2024-11-01 14:45:27,302][root][INFO] - ㄴ alpha            : 128
[2024-11-01 14:45:27,302][root][INFO] - ㄴ dropout          : 0.03

[2024-11-01 14:45:27,302][root][INFO] - 

[2024-11-01 14:45:27,302][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-11-01 14:45:27,302][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt
[2024-11-01 14:45:27,302][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb
[2024-11-01 14:45:27,302][root][INFO] - * tb interval   : 100

[2024-11-01 14:45:27,302][root][INFO] - 

[2024-11-01 14:45:27,302][root][INFO] - Start the Training !
[2024-11-01 14:45:27,305][root][INFO] - 
[1/ 10 Epoch]
[2024-11-01 23:24:09,316][root][INFO] - 

[2024-11-01 23:24:09,317][root][INFO] - This train_log.txt inform the Running Progress.

[2024-11-01 23:24:09,317][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-11-01 23:24:09,317][root][INFO] - 

[2024-11-01 23:24:09,317][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-11-01 23:24:17,801][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-11-01 23:24:17,803][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-11-01 23:24:17,803][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-11-01 23:24:17,804][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-11-01 23:24:17,805][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-11-01 23:24:17,805][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-11-01 23:24:17,806][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-11-01 23:24:17,807][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-11-01 23:24:17,808][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-11-01 23:24:17,808][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-11-01 23:24:17,809][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-11-01 23:24:17,810][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-11-01 23:24:17,810][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-11-01 23:24:17,811][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-11-01 23:24:17,812][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-11-01 23:24:17,812][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-11-01 23:24:17,813][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-11-01 23:24:17,814][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-11-01 23:24:17,814][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-11-01 23:24:17,815][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-11-01 23:24:17,816][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-11-01 23:24:17,816][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-11-01 23:24:17,817][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-11-01 23:24:17,818][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-11-01 23:24:17,821][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-11-01 23:24:17,828][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-11-01 23:24:18,025][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-11-01 23:24:18,027][root][INFO] - Trainable params: 15525888 || all params: 140691456 || trainable: 11.04 %
[2024-11-01 23:24:18,257][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-11-01 23:24:20,433][root][INFO] - 

[2024-11-01 23:24:20,433][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-11-01 23:24:20,433][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-11-01 23:24:20,443][root][INFO] - vocab size              : 51200
[2024-11-01 23:24:20,443][root][INFO] - device                  : gpu
[2024-11-01 23:24:20,443][root][INFO] - random seed             : 1
[2024-11-01 23:24:20,443][root][INFO] - train data size         : 3672
[2024-11-01 23:24:20,443][root][INFO] - max epochs              : 10
[2024-11-01 23:24:20,443][root][INFO] - total steps             : 4590
[2024-11-01 23:24:20,443][root][INFO] - warmup steps            : 0
[2024-11-01 23:24:20,443][root][INFO] - batch size              : 8
[2024-11-01 23:24:20,443][root][INFO] - accumulation steps      : 1
[2024-11-01 23:24:20,444][root][INFO] - optimizer               : adamwscale
[2024-11-01 23:24:20,444][root][INFO] - lr_scheduler            : cosine
[2024-11-01 23:24:20,444][root][INFO] - learning rate           : 0.01
[2024-11-01 23:24:20,444][root][INFO] - max length              : 256

[2024-11-01 23:24:20,444][root][INFO] - LoRA Configuration
[2024-11-01 23:24:20,444][root][INFO] - ㄴ r                    : 32
[2024-11-01 23:24:20,444][root][INFO] - ㄴ alpha                : 128
[2024-11-01 23:24:20,444][root][INFO] - ㄴ dropout              : 0.03

[2024-11-01 23:24:20,444][root][INFO] - SUB2 Configuration
[2024-11-01 23:24:20,444][root][INFO] - ㄴ tok_type             : jamo_var
[2024-11-01 23:24:20,444][root][INFO] - ㄴ hidden_dim           : 768
[2024-11-01 23:24:20,445][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-11-01 23:24:20,445][root][INFO] - ㄴ embedding_norm       : False
[2024-11-01 23:24:20,445][root][INFO] - ㄴ do_combination       : False
[2024-11-01 23:24:20,445][root][INFO] - ㄴ reducer              : linear_pool

[2024-11-01 23:24:20,445][root][INFO] - LoRA in SUB2 Configuration
[2024-11-01 23:24:20,445][root][INFO] - ㄴ r                : 32
[2024-11-01 23:24:20,445][root][INFO] - ㄴ alpha            : 128
[2024-11-01 23:24:20,445][root][INFO] - ㄴ dropout          : 0.03

[2024-11-01 23:24:20,445][root][INFO] - 

[2024-11-01 23:24:20,445][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-11-01 23:24:20,446][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt
[2024-11-01 23:24:20,446][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb
[2024-11-01 23:24:20,446][root][INFO] - * tb interval   : 100

[2024-11-01 23:24:20,446][root][INFO] - 

[2024-11-01 23:24:20,446][root][INFO] - Start the Training !
[2024-11-01 23:24:20,449][root][INFO] - 
[1/ 10 Epoch]
[2024-11-01 23:24:32,973][root][INFO] - Step: 100/4590  |  Loss: 0.7632  |  Score: 51.12 [%]  |  Seq Length: 256.0
[2024-11-01 23:24:44,330][root][INFO] - Step: 200/4590  |  Loss: 0.7417  |  Score: 51.25 [%]  |  Seq Length: 256.0
[2024-11-01 23:24:55,935][root][INFO] - Step: 300/4590  |  Loss: 0.8093  |  Score: 52.25 [%]  |  Seq Length: 256.0
[2024-11-01 23:25:07,444][root][INFO] - Step: 400/4590  |  Loss: 0.7274  |  Score: 47.62 [%]  |  Seq Length: 256.0
[2024-11-01 23:25:14,475][root][INFO] - Step: 459/4590  |  Loss: 0.7197  |  Score: 50.21 [%]  |  Seq Length: 256.0
[2024-11-01 23:25:18,799][root][INFO] - ########################  DEV REPORT #EP1  ########################
[2024-11-01 23:25:18,799][root][INFO] - Score: 51.85 [%]  |  Evaluation Time: 4.32 [s]
[2024-11-01 23:25:27,235][root][INFO] - ########################  TEST REPORT #EP1  ########################
[2024-11-01 23:25:27,236][root][INFO] - Score: 53.55 [%]  |  Evaluation Time: 8.43 [s]
[2024-11-01 23:25:27,237][root][INFO] - 
Save new Best Score (Epoch: 1)
[2024-11-01 23:25:27,238][root][INFO] - 
[2/ 10 Epoch]
[2024-11-01 23:25:32,212][root][INFO] - Step: 500/4590  |  Loss: 0.7227  |  Score: 46.04 [%]  |  Seq Length: 256.0
[2024-11-01 23:25:43,851][root][INFO] - Step: 600/4590  |  Loss: 0.7103  |  Score: 49.62 [%]  |  Seq Length: 256.0
[2024-11-01 23:25:55,709][root][INFO] - Step: 700/4590  |  Loss: 0.7365  |  Score: 48.88 [%]  |  Seq Length: 256.0
[2024-11-01 23:26:07,496][root][INFO] - Step: 800/4590  |  Loss: 0.7233  |  Score: 50.62 [%]  |  Seq Length: 256.0
[2024-11-01 23:26:18,916][root][INFO] - Step: 900/4590  |  Loss: 0.7272  |  Score: 49.38 [%]  |  Seq Length: 256.0
[2024-11-01 23:26:20,965][root][INFO] - Step: 918/4590  |  Loss: 0.7553  |  Score: 45.83 [%]  |  Seq Length: 256.0
[2024-11-01 23:26:25,297][root][INFO] - ########################  DEV REPORT #EP2  ########################
[2024-11-01 23:26:25,297][root][INFO] - Score: 46.73 [%]  |  Evaluation Time: 4.33 [s]
[2024-11-01 23:26:33,817][root][INFO] - ########################  TEST REPORT #EP2  ########################
[2024-11-01 23:26:33,817][root][INFO] - Score: 50.14 [%]  |  Evaluation Time: 8.52 [s]
[2024-11-01 23:26:33,819][root][INFO] - 
[3/ 10 Epoch]
[2024-11-01 23:26:57,988][root][INFO] - 

[2024-11-01 23:26:57,988][root][INFO] - This train_log.txt inform the Running Progress.

[2024-11-01 23:26:57,989][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-11-01 23:26:57,989][root][INFO] - 

[2024-11-01 23:26:57,989][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-11-01 23:27:05,500][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-11-01 23:27:05,501][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-11-01 23:27:05,501][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-11-01 23:27:05,502][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-11-01 23:27:05,502][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-11-01 23:27:05,503][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-11-01 23:27:05,503][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-11-01 23:27:05,504][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-11-01 23:27:05,504][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-11-01 23:27:05,505][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-11-01 23:27:05,505][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-11-01 23:27:05,506][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-11-01 23:27:05,506][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-11-01 23:27:05,507][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-11-01 23:27:05,507][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-11-01 23:27:05,508][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-11-01 23:27:05,508][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-11-01 23:27:05,509][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-11-01 23:27:05,509][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-11-01 23:27:05,510][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-11-01 23:27:05,511][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-11-01 23:27:05,511][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-11-01 23:27:05,512][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-11-01 23:27:05,512][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-11-01 23:27:05,514][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-11-01 23:27:05,519][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-11-01 23:27:05,687][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-11-01 23:27:05,689][root][INFO] - Trainable params: 15525888 || all params: 140691456 || trainable: 11.04 %
[2024-11-01 23:27:05,857][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-11-01 23:27:07,775][root][INFO] - 

[2024-11-01 23:27:07,775][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-11-01 23:27:07,775][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-11-01 23:27:07,783][root][INFO] - vocab size              : 51200
[2024-11-01 23:27:07,783][root][INFO] - device                  : gpu
[2024-11-01 23:27:07,784][root][INFO] - random seed             : 1
[2024-11-01 23:27:07,784][root][INFO] - train data size         : 3672
[2024-11-01 23:27:07,784][root][INFO] - max epochs              : 10
[2024-11-01 23:27:07,784][root][INFO] - total steps             : 4590
[2024-11-01 23:27:07,784][root][INFO] - warmup steps            : 0
[2024-11-01 23:27:07,784][root][INFO] - batch size              : 8
[2024-11-01 23:27:07,784][root][INFO] - accumulation steps      : 1
[2024-11-01 23:27:07,784][root][INFO] - optimizer               : adamwscale
[2024-11-01 23:27:07,784][root][INFO] - lr_scheduler            : cosine
[2024-11-01 23:27:07,784][root][INFO] - learning rate           : 0.01
[2024-11-01 23:27:07,784][root][INFO] - max length              : 256

[2024-11-01 23:27:07,785][root][INFO] - LoRA Configuration
[2024-11-01 23:27:07,785][root][INFO] - ㄴ r                    : 32
[2024-11-01 23:27:07,785][root][INFO] - ㄴ alpha                : 128
[2024-11-01 23:27:07,785][root][INFO] - ㄴ dropout              : 0.03

[2024-11-01 23:27:07,785][root][INFO] - SUB2 Configuration
[2024-11-01 23:27:07,785][root][INFO] - ㄴ tok_type             : jamo_var
[2024-11-01 23:27:07,785][root][INFO] - ㄴ hidden_dim           : 768
[2024-11-01 23:27:07,785][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-11-01 23:27:07,785][root][INFO] - ㄴ embedding_norm       : False
[2024-11-01 23:27:07,785][root][INFO] - ㄴ do_combination       : False
[2024-11-01 23:27:07,786][root][INFO] - ㄴ reducer              : linear_pool

[2024-11-01 23:27:07,786][root][INFO] - LoRA in SUB2 Configuration
[2024-11-01 23:27:07,786][root][INFO] - ㄴ r                : 32
[2024-11-01 23:27:07,786][root][INFO] - ㄴ alpha            : 128
[2024-11-01 23:27:07,786][root][INFO] - ㄴ dropout          : 0.03

[2024-11-01 23:27:07,786][root][INFO] - 

[2024-11-01 23:27:07,786][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-11-01 23:27:07,786][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt
[2024-11-01 23:27:07,786][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb
[2024-11-01 23:27:07,786][root][INFO] - * tb interval   : 100

[2024-11-01 23:27:07,786][root][INFO] - 

[2024-11-01 23:27:07,786][root][INFO] - Start the Training !
[2024-11-01 23:27:07,790][root][INFO] - 
[1/ 10 Epoch]
[2024-11-01 23:28:19,676][root][INFO] - 

[2024-11-01 23:28:19,676][root][INFO] - This train_log.txt inform the Running Progress.

[2024-11-01 23:28:19,676][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs
[2024-11-01 23:28:19,676][root][INFO] - 

[2024-11-01 23:28:19,676][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.0001, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-11-01 23:28:26,857][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-11-01 23:28:26,858][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-11-01 23:28:26,858][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-11-01 23:28:26,859][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-11-01 23:28:26,859][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-11-01 23:28:26,859][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-11-01 23:28:26,860][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-11-01 23:28:26,860][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-11-01 23:28:26,861][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-11-01 23:28:26,861][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-11-01 23:28:26,861][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-11-01 23:28:26,862][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-11-01 23:28:26,862][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-11-01 23:28:26,863][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-11-01 23:28:26,863][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-11-01 23:28:26,864][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-11-01 23:28:26,864][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-11-01 23:28:26,864][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-11-01 23:28:26,865][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-11-01 23:28:26,865][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-11-01 23:28:26,866][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-11-01 23:28:26,866][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-11-01 23:28:26,867][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-11-01 23:28:26,867][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-11-01 23:28:26,869][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-11-01 23:28:26,873][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-11-01 23:28:27,041][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-11-01 23:28:27,044][root][INFO] - Trainable params: 15525888 || all params: 140691456 || trainable: 11.04 %
[2024-11-01 23:28:27,212][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-11-01 23:28:29,462][root][INFO] - 

[2024-11-01 23:28:29,462][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-11-01 23:28:29,462][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-11-01 23:28:29,470][root][INFO] - vocab size              : 51200
[2024-11-01 23:28:29,470][root][INFO] - device                  : gpu
[2024-11-01 23:28:29,470][root][INFO] - random seed             : 1
[2024-11-01 23:28:29,470][root][INFO] - train data size         : 3672
[2024-11-01 23:28:29,471][root][INFO] - max epochs              : 10
[2024-11-01 23:28:29,471][root][INFO] - total steps             : 4590
[2024-11-01 23:28:29,471][root][INFO] - warmup steps            : 0
[2024-11-01 23:28:29,471][root][INFO] - batch size              : 8
[2024-11-01 23:28:29,471][root][INFO] - accumulation steps      : 1
[2024-11-01 23:28:29,471][root][INFO] - optimizer               : adamwscale
[2024-11-01 23:28:29,471][root][INFO] - lr_scheduler            : cosine
[2024-11-01 23:28:29,471][root][INFO] - learning rate           : 0.0001
[2024-11-01 23:28:29,471][root][INFO] - max length              : 256

[2024-11-01 23:28:29,471][root][INFO] - LoRA Configuration
[2024-11-01 23:28:29,471][root][INFO] - ㄴ r                    : 32
[2024-11-01 23:28:29,472][root][INFO] - ㄴ alpha                : 128
[2024-11-01 23:28:29,472][root][INFO] - ㄴ dropout              : 0.03

[2024-11-01 23:28:29,472][root][INFO] - SUB2 Configuration
[2024-11-01 23:28:29,472][root][INFO] - ㄴ tok_type             : jamo_var
[2024-11-01 23:28:29,472][root][INFO] - ㄴ hidden_dim           : 768
[2024-11-01 23:28:29,472][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-11-01 23:28:29,472][root][INFO] - ㄴ embedding_norm       : False
[2024-11-01 23:28:29,472][root][INFO] - ㄴ do_combination       : False
[2024-11-01 23:28:29,472][root][INFO] - ㄴ reducer              : linear_pool

[2024-11-01 23:28:29,472][root][INFO] - LoRA in SUB2 Configuration
[2024-11-01 23:28:29,472][root][INFO] - ㄴ r                : 32
[2024-11-01 23:28:29,473][root][INFO] - ㄴ alpha            : 128
[2024-11-01 23:28:29,473][root][INFO] - ㄴ dropout          : 0.03

[2024-11-01 23:28:29,473][root][INFO] - 

[2024-11-01 23:28:29,473][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs
[2024-11-01 23:28:29,473][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/ckpt
[2024-11-01 23:28:29,473][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/tb
[2024-11-01 23:28:29,473][root][INFO] - * tb interval   : 100

[2024-11-01 23:28:29,473][root][INFO] - 

[2024-11-01 23:28:29,473][root][INFO] - Start the Training !
[2024-11-01 23:28:29,477][root][INFO] - 
[1/ 10 Epoch]
[2024-11-01 23:29:02,044][root][INFO] - 

[2024-11-01 23:29:02,044][root][INFO] - This train_log.txt inform the Running Progress.

[2024-11-01 23:29:02,044][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs
[2024-11-01 23:29:02,044][root][INFO] - 

[2024-11-01 23:29:02,044][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.0001, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-11-01 23:29:10,159][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-11-01 23:29:10,160][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-11-01 23:29:10,160][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-11-01 23:29:10,161][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-11-01 23:29:10,161][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-11-01 23:29:10,162][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-11-01 23:29:10,162][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-11-01 23:29:10,163][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-11-01 23:29:10,163][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-11-01 23:29:10,164][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-11-01 23:29:10,164][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-11-01 23:29:10,164][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-11-01 23:29:10,165][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-11-01 23:29:10,165][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-11-01 23:29:10,166][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-11-01 23:29:10,166][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-11-01 23:29:10,167][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-11-01 23:29:10,167][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-11-01 23:29:10,167][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-11-01 23:29:10,168][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-11-01 23:29:10,168][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-11-01 23:29:10,169][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-11-01 23:29:10,169][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-11-01 23:29:10,170][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-11-01 23:29:10,171][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-11-01 23:29:10,175][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-11-01 23:29:10,343][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-11-01 23:29:10,345][root][INFO] - Trainable params: 15525888 || all params: 140691456 || trainable: 11.04 %
[2024-11-01 23:29:10,518][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-11-01 23:29:12,427][root][INFO] - 

[2024-11-01 23:29:12,427][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-11-01 23:29:12,427][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-11-01 23:29:12,435][root][INFO] - vocab size              : 51200
[2024-11-01 23:29:12,435][root][INFO] - device                  : gpu
[2024-11-01 23:29:12,435][root][INFO] - random seed             : 1
[2024-11-01 23:29:12,435][root][INFO] - train data size         : 3672
[2024-11-01 23:29:12,435][root][INFO] - max epochs              : 10
[2024-11-01 23:29:12,435][root][INFO] - total steps             : 4590
[2024-11-01 23:29:12,435][root][INFO] - warmup steps            : 0
[2024-11-01 23:29:12,436][root][INFO] - batch size              : 8
[2024-11-01 23:29:12,436][root][INFO] - accumulation steps      : 1
[2024-11-01 23:29:12,436][root][INFO] - optimizer               : adamwscale
[2024-11-01 23:29:12,436][root][INFO] - lr_scheduler            : cosine
[2024-11-01 23:29:12,436][root][INFO] - learning rate           : 0.0001
[2024-11-01 23:29:12,436][root][INFO] - max length              : 256

[2024-11-01 23:29:12,436][root][INFO] - LoRA Configuration
[2024-11-01 23:29:12,436][root][INFO] - ㄴ r                    : 32
[2024-11-01 23:29:12,436][root][INFO] - ㄴ alpha                : 128
[2024-11-01 23:29:12,436][root][INFO] - ㄴ dropout              : 0.03

[2024-11-01 23:29:12,436][root][INFO] - SUB2 Configuration
[2024-11-01 23:29:12,437][root][INFO] - ㄴ tok_type             : jamo_var
[2024-11-01 23:29:12,437][root][INFO] - ㄴ hidden_dim           : 768
[2024-11-01 23:29:12,437][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-11-01 23:29:12,437][root][INFO] - ㄴ embedding_norm       : False
[2024-11-01 23:29:12,437][root][INFO] - ㄴ do_combination       : False
[2024-11-01 23:29:12,437][root][INFO] - ㄴ reducer              : linear_pool

[2024-11-01 23:29:12,437][root][INFO] - LoRA in SUB2 Configuration
[2024-11-01 23:29:12,437][root][INFO] - ㄴ r                : 32
[2024-11-01 23:29:12,437][root][INFO] - ㄴ alpha            : 128
[2024-11-01 23:29:12,437][root][INFO] - ㄴ dropout          : 0.03

[2024-11-01 23:29:12,437][root][INFO] - 

[2024-11-01 23:29:12,438][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs
[2024-11-01 23:29:12,438][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/ckpt
[2024-11-01 23:29:12,438][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/tb
[2024-11-01 23:29:12,438][root][INFO] - * tb interval   : 100

[2024-11-01 23:29:12,438][root][INFO] - 

[2024-11-01 23:29:12,438][root][INFO] - Start the Training !
[2024-11-01 23:29:12,441][root][INFO] - 
[1/ 10 Epoch]
[2024-11-01 23:29:25,613][root][INFO] - Step: 100/4590  |  Loss: 0.7707  |  Score: 51.00 [%]  |  Seq Length: 256.0
[2024-11-01 23:29:37,133][root][INFO] - Step: 200/4590  |  Loss: 0.7841  |  Score: 50.88 [%]  |  Seq Length: 256.0
[2024-11-01 23:29:48,711][root][INFO] - Step: 300/4590  |  Loss: 0.7684  |  Score: 51.50 [%]  |  Seq Length: 256.0
[2024-11-01 23:30:00,280][root][INFO] - Step: 400/4590  |  Loss: 0.8254  |  Score: 47.50 [%]  |  Seq Length: 256.0
[2024-11-01 23:30:07,362][root][INFO] - Step: 459/4590  |  Loss: 0.7686  |  Score: 48.73 [%]  |  Seq Length: 256.0
[2024-11-01 23:30:11,764][root][INFO] - ########################  DEV REPORT #EP1  ########################
[2024-11-01 23:30:11,764][root][INFO] - Score: 50.28 [%]  |  Evaluation Time: 4.40 [s]
[2024-11-01 23:30:20,078][root][INFO] - ########################  TEST REPORT #EP1  ########################
[2024-11-01 23:30:20,079][root][INFO] - Score: 50.99 [%]  |  Evaluation Time: 8.31 [s]
[2024-11-01 23:30:20,080][root][INFO] - 
Save new Best Score (Epoch: 1)
[2024-11-01 23:30:20,081][root][INFO] - 
[2/ 10 Epoch]
[2024-11-01 23:30:25,006][root][INFO] - Step: 500/4590  |  Loss: 0.7913  |  Score: 48.78 [%]  |  Seq Length: 256.0
[2024-11-01 23:30:36,428][root][INFO] - Step: 600/4590  |  Loss: 0.7909  |  Score: 49.38 [%]  |  Seq Length: 256.0
[2024-11-01 23:30:48,162][root][INFO] - Step: 700/4590  |  Loss: 0.7723  |  Score: 50.75 [%]  |  Seq Length: 256.0
[2024-11-01 23:31:00,775][root][INFO] - Step: 800/4590  |  Loss: 0.7670  |  Score: 52.25 [%]  |  Seq Length: 256.0
[2024-11-01 23:31:12,443][root][INFO] - Step: 900/4590  |  Loss: 0.7648  |  Score: 49.75 [%]  |  Seq Length: 256.0
[2024-11-01 23:31:14,501][root][INFO] - Step: 918/4590  |  Loss: 0.7168  |  Score: 50.69 [%]  |  Seq Length: 256.0
[2024-11-01 23:31:18,876][root][INFO] - ########################  DEV REPORT #EP2  ########################
[2024-11-01 23:31:18,876][root][INFO] - Score: 50.99 [%]  |  Evaluation Time: 4.37 [s]
[2024-11-01 23:40:22,252][root][INFO] - 

[2024-11-01 23:40:22,252][root][INFO] - This train_log.txt inform the Running Progress.

[2024-11-01 23:40:22,252][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs
[2024-11-01 23:40:22,252][root][INFO] - 

[2024-11-01 23:40:22,252][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.0001, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-11-01 23:40:29,999][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-11-01 23:40:29,999][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-11-01 23:40:30,000][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-11-01 23:40:30,000][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-11-01 23:40:30,001][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-11-01 23:40:30,001][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-11-01 23:40:30,002][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-11-01 23:40:30,002][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-11-01 23:40:30,003][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-11-01 23:40:30,003][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-11-01 23:40:30,004][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-11-01 23:40:30,004][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-11-01 23:40:30,005][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-11-01 23:40:30,005][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-11-01 23:40:30,006][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-11-01 23:40:30,006][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-11-01 23:40:30,007][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-11-01 23:40:30,008][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-11-01 23:40:30,008][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-11-01 23:40:30,009][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-11-01 23:40:30,009][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-11-01 23:40:30,010][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-11-01 23:40:30,010][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-11-01 23:40:30,011][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-11-01 23:40:30,013][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-11-01 23:40:30,018][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-11-01 23:47:21,243][root][INFO] - 

[2024-11-01 23:47:21,243][root][INFO] - This train_log.txt inform the Running Progress.

[2024-11-01 23:47:21,243][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs
[2024-11-01 23:47:21,243][root][INFO] - 

[2024-11-01 23:47:21,244][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.0001, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-11-01 23:47:30,265][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-11-01 23:47:30,266][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-11-01 23:47:30,267][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-11-01 23:47:30,267][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-11-01 23:47:30,268][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-11-01 23:47:30,268][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-11-01 23:47:30,269][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-11-01 23:47:30,269][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-11-01 23:47:30,270][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-11-01 23:47:30,270][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-11-01 23:47:30,271][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-11-01 23:47:30,271][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-11-01 23:47:30,272][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-11-01 23:47:30,272][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-11-01 23:47:30,273][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-11-01 23:47:30,273][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-11-01 23:47:30,273][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-11-01 23:47:30,274][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-11-01 23:47:30,274][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-11-01 23:47:30,275][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-11-01 23:47:30,275][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-11-01 23:47:30,276][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-11-01 23:47:30,276][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-11-01 23:47:30,277][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-11-01 23:47:30,278][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-11-01 23:47:30,282][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-11-01 23:47:30,454][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-11-01 23:47:30,456][root][INFO] - Trainable params: 15527424 || all params: 140692992 || trainable: 11.04 %
[2024-11-01 23:47:30,625][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-11-01 23:47:33,171][root][INFO] - 

[2024-11-01 23:47:33,172][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-11-01 23:47:33,172][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-11-01 23:47:33,179][root][INFO] - vocab size              : 51200
[2024-11-01 23:47:33,180][root][INFO] - device                  : gpu
[2024-11-01 23:47:33,180][root][INFO] - random seed             : 1
[2024-11-01 23:47:33,180][root][INFO] - train data size         : 3672
[2024-11-01 23:47:33,180][root][INFO] - max epochs              : 10
[2024-11-01 23:47:33,180][root][INFO] - total steps             : 4590
[2024-11-01 23:47:33,180][root][INFO] - warmup steps            : 0
[2024-11-01 23:47:33,180][root][INFO] - batch size              : 8
[2024-11-01 23:47:33,180][root][INFO] - accumulation steps      : 1
[2024-11-01 23:47:33,181][root][INFO] - optimizer               : adamwscale
[2024-11-01 23:47:33,181][root][INFO] - lr_scheduler            : cosine
[2024-11-01 23:47:33,181][root][INFO] - learning rate           : 0.0001
[2024-11-01 23:47:33,181][root][INFO] - max length              : 256

[2024-11-01 23:47:33,181][root][INFO] - LoRA Configuration
[2024-11-01 23:47:33,181][root][INFO] - ㄴ r                    : 32
[2024-11-01 23:47:33,181][root][INFO] - ㄴ alpha                : 128
[2024-11-01 23:47:33,181][root][INFO] - ㄴ dropout              : 0.03

[2024-11-01 23:47:33,181][root][INFO] - SUB2 Configuration
[2024-11-01 23:47:33,182][root][INFO] - ㄴ tok_type             : jamo_var
[2024-11-01 23:47:33,182][root][INFO] - ㄴ hidden_dim           : 768
[2024-11-01 23:47:33,182][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-11-01 23:47:33,182][root][INFO] - ㄴ embedding_norm       : False
[2024-11-01 23:47:33,182][root][INFO] - ㄴ do_combination       : False
[2024-11-01 23:47:33,182][root][INFO] - ㄴ reducer              : linear_pool

[2024-11-01 23:47:33,182][root][INFO] - LoRA in SUB2 Configuration
[2024-11-01 23:47:33,182][root][INFO] - ㄴ r                : 32
[2024-11-01 23:47:33,183][root][INFO] - ㄴ alpha            : 128
[2024-11-01 23:47:33,183][root][INFO] - ㄴ dropout          : 0.03

[2024-11-01 23:47:33,183][root][INFO] - 

[2024-11-01 23:47:33,183][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs
[2024-11-01 23:47:33,183][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/ckpt
[2024-11-01 23:47:33,183][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/tb
[2024-11-01 23:47:33,183][root][INFO] - * tb interval   : 100

[2024-11-01 23:47:33,183][root][INFO] - 

[2024-11-01 23:47:33,183][root][INFO] - Start the Training !
[2024-11-01 23:47:33,187][root][INFO] - 
[1/ 10 Epoch]
[2024-11-01 23:47:54,429][root][INFO] - 

[2024-11-01 23:47:54,429][root][INFO] - This train_log.txt inform the Running Progress.

[2024-11-01 23:47:54,429][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs
[2024-11-01 23:47:54,429][root][INFO] - 

[2024-11-01 23:47:54,429][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.0001, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-11-01 23:48:02,646][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-11-01 23:48:02,647][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-11-01 23:48:02,648][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-11-01 23:48:02,648][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-11-01 23:48:02,648][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-11-01 23:48:02,649][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-11-01 23:48:02,649][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-11-01 23:48:02,650][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-11-01 23:48:02,650][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-11-01 23:48:02,651][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-11-01 23:48:02,651][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-11-01 23:48:02,652][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-11-01 23:48:02,652][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-11-01 23:48:02,653][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-11-01 23:48:02,653][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-11-01 23:48:02,653][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-11-01 23:48:02,654][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-11-01 23:48:02,654][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-11-01 23:48:02,655][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-11-01 23:48:02,655][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-11-01 23:48:02,656][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-11-01 23:48:02,656][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-11-01 23:48:02,657][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-11-01 23:48:02,657][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-11-01 23:48:02,659][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-11-01 23:48:02,663][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-11-01 23:48:02,831][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-11-01 23:48:02,834][root][INFO] - Trainable params: 15527424 || all params: 140692992 || trainable: 11.04 %
[2024-11-01 23:48:03,009][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-11-01 23:48:06,196][root][INFO] - 

[2024-11-01 23:48:06,196][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-11-01 23:48:06,196][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-11-01 23:48:06,205][root][INFO] - vocab size              : 51200
[2024-11-01 23:48:06,205][root][INFO] - device                  : gpu
[2024-11-01 23:48:06,205][root][INFO] - random seed             : 1
[2024-11-01 23:48:06,205][root][INFO] - train data size         : 3672
[2024-11-01 23:48:06,205][root][INFO] - max epochs              : 10
[2024-11-01 23:48:06,205][root][INFO] - total steps             : 4590
[2024-11-01 23:48:06,205][root][INFO] - warmup steps            : 0
[2024-11-01 23:48:06,205][root][INFO] - batch size              : 8
[2024-11-01 23:48:06,205][root][INFO] - accumulation steps      : 1
[2024-11-01 23:48:06,205][root][INFO] - optimizer               : adamwscale
[2024-11-01 23:48:06,206][root][INFO] - lr_scheduler            : cosine
[2024-11-01 23:48:06,206][root][INFO] - learning rate           : 0.0001
[2024-11-01 23:48:06,206][root][INFO] - max length              : 256

[2024-11-01 23:48:06,206][root][INFO] - LoRA Configuration
[2024-11-01 23:48:06,206][root][INFO] - ㄴ r                    : 32
[2024-11-01 23:48:06,206][root][INFO] - ㄴ alpha                : 128
[2024-11-01 23:48:06,206][root][INFO] - ㄴ dropout              : 0.03

[2024-11-01 23:48:06,206][root][INFO] - SUB2 Configuration
[2024-11-01 23:48:06,206][root][INFO] - ㄴ tok_type             : jamo_var
[2024-11-01 23:48:06,206][root][INFO] - ㄴ hidden_dim           : 768
[2024-11-01 23:48:06,206][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-11-01 23:48:06,207][root][INFO] - ㄴ embedding_norm       : False
[2024-11-01 23:48:06,207][root][INFO] - ㄴ do_combination       : False
[2024-11-01 23:48:06,207][root][INFO] - ㄴ reducer              : linear_pool

[2024-11-01 23:48:06,207][root][INFO] - LoRA in SUB2 Configuration
[2024-11-01 23:48:06,207][root][INFO] - ㄴ r                : 32
[2024-11-01 23:48:06,207][root][INFO] - ㄴ alpha            : 128
[2024-11-01 23:48:06,207][root][INFO] - ㄴ dropout          : 0.03

[2024-11-01 23:48:06,207][root][INFO] - 

[2024-11-01 23:48:06,207][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs
[2024-11-01 23:48:06,207][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/ckpt
[2024-11-01 23:48:06,208][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/tb
[2024-11-01 23:48:06,208][root][INFO] - * tb interval   : 100

[2024-11-01 23:48:06,208][root][INFO] - 

[2024-11-01 23:48:06,208][root][INFO] - Start the Training !
[2024-11-01 23:48:06,211][root][INFO] - 
[1/ 10 Epoch]
[2024-11-01 23:48:32,563][root][INFO] - Step: 100/4590  |  Loss: 0.7544  |  Score: 49.88 [%]  |  Seq Length: 256.0
[2024-11-01 23:48:52,770][root][INFO] - Step: 200/4590  |  Loss: 0.7422  |  Score: 51.25 [%]  |  Seq Length: 256.0
[2024-11-01 23:49:08,856][root][INFO] - 

[2024-11-01 23:49:08,856][root][INFO] - This train_log.txt inform the Running Progress.

[2024-11-01 23:49:08,856][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs
[2024-11-01 23:49:08,856][root][INFO] - 

[2024-11-01 23:49:08,856][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.0001, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-11-01 23:49:16,254][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-11-01 23:49:16,255][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-11-01 23:49:16,255][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-11-01 23:49:16,256][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-11-01 23:49:16,256][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-11-01 23:49:16,257][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-11-01 23:49:16,257][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-11-01 23:49:16,258][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-11-01 23:49:16,258][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-11-01 23:49:16,259][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-11-01 23:49:16,259][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-11-01 23:49:16,260][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-11-01 23:49:16,260][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-11-01 23:49:16,261][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-11-01 23:49:16,261][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-11-01 23:49:16,262][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-11-01 23:49:16,262][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-11-01 23:49:16,263][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-11-01 23:49:16,263][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-11-01 23:49:16,264][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-11-01 23:49:16,265][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-11-01 23:49:16,265][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-11-01 23:49:16,266][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-11-01 23:49:16,266][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-11-01 23:49:16,268][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-11-01 23:49:16,273][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-11-01 23:49:16,441][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-11-01 23:49:16,443][root][INFO] - Trainable params: 15527424 || all params: 140692992 || trainable: 11.04 %
[2024-11-01 23:49:16,619][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-11-01 23:49:18,566][root][INFO] - 

[2024-11-01 23:49:18,567][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-11-01 23:49:18,567][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-11-01 23:49:18,576][root][INFO] - vocab size              : 51200
[2024-11-01 23:49:18,576][root][INFO] - device                  : gpu
[2024-11-01 23:49:18,577][root][INFO] - random seed             : 1
[2024-11-01 23:49:18,577][root][INFO] - train data size         : 3672
[2024-11-01 23:49:18,577][root][INFO] - max epochs              : 10
[2024-11-01 23:49:18,577][root][INFO] - total steps             : 4590
[2024-11-01 23:49:18,577][root][INFO] - warmup steps            : 0
[2024-11-01 23:49:18,577][root][INFO] - batch size              : 8
[2024-11-01 23:49:18,577][root][INFO] - accumulation steps      : 1
[2024-11-01 23:49:18,577][root][INFO] - optimizer               : adamwscale
[2024-11-01 23:49:18,577][root][INFO] - lr_scheduler            : cosine
[2024-11-01 23:49:18,577][root][INFO] - learning rate           : 0.0001
[2024-11-01 23:49:18,577][root][INFO] - max length              : 256

[2024-11-01 23:49:18,578][root][INFO] - LoRA Configuration
[2024-11-01 23:49:18,578][root][INFO] - ㄴ r                    : 32
[2024-11-01 23:49:18,578][root][INFO] - ㄴ alpha                : 128
[2024-11-01 23:49:18,578][root][INFO] - ㄴ dropout              : 0.03

[2024-11-01 23:49:18,578][root][INFO] - SUB2 Configuration
[2024-11-01 23:49:18,578][root][INFO] - ㄴ tok_type             : jamo_var
[2024-11-01 23:49:18,578][root][INFO] - ㄴ hidden_dim           : 768
[2024-11-01 23:49:18,578][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-11-01 23:49:18,578][root][INFO] - ㄴ embedding_norm       : False
[2024-11-01 23:49:18,578][root][INFO] - ㄴ do_combination       : False
[2024-11-01 23:49:18,579][root][INFO] - ㄴ reducer              : linear_pool

[2024-11-01 23:49:18,579][root][INFO] - LoRA in SUB2 Configuration
[2024-11-01 23:49:18,579][root][INFO] - ㄴ r                : 32
[2024-11-01 23:49:18,579][root][INFO] - ㄴ alpha            : 128
[2024-11-01 23:49:18,579][root][INFO] - ㄴ dropout          : 0.03

[2024-11-01 23:49:18,579][root][INFO] - 

[2024-11-01 23:49:18,579][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs
[2024-11-01 23:49:18,579][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/ckpt
[2024-11-01 23:49:18,579][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.0001lr_1rs/tb
[2024-11-01 23:49:18,579][root][INFO] - * tb interval   : 100

[2024-11-01 23:49:18,579][root][INFO] - 

[2024-11-01 23:49:18,579][root][INFO] - Start the Training !
[2024-11-01 23:49:18,582][root][INFO] - 
[1/ 10 Epoch]
[2024-11-01 23:49:31,147][root][INFO] - Step: 100/4590  |  Loss: 0.7544  |  Score: 49.88 [%]  |  Seq Length: 256.0
[2024-11-01 23:49:42,493][root][INFO] - Step: 200/4590  |  Loss: 0.7422  |  Score: 51.25 [%]  |  Seq Length: 256.0
[2024-11-01 23:49:53,970][root][INFO] - Step: 300/4590  |  Loss: 0.7418  |  Score: 52.62 [%]  |  Seq Length: 256.0
[2024-11-01 23:50:05,462][root][INFO] - Step: 400/4590  |  Loss: 0.7433  |  Score: 50.00 [%]  |  Seq Length: 256.0
[2024-11-01 23:50:12,494][root][INFO] - Step: 459/4590  |  Loss: 0.7268  |  Score: 50.21 [%]  |  Seq Length: 256.0
[2024-11-01 23:50:16,767][root][INFO] - ########################  DEV REPORT #EP1  ########################
[2024-11-01 23:50:16,767][root][INFO] - Score: 46.88 [%]  |  Evaluation Time: 4.27 [s]
[2024-11-01 23:50:25,082][root][INFO] - ########################  TEST REPORT #EP1  ########################
[2024-11-01 23:50:25,082][root][INFO] - Score: 50.50 [%]  |  Evaluation Time: 8.31 [s]
[2024-11-01 23:50:25,083][root][INFO] - 
Save new Best Score (Epoch: 1)
[2024-11-01 23:50:25,084][root][INFO] - 
[2/ 10 Epoch]
[2024-11-01 23:50:29,859][root][INFO] - Step: 500/4590  |  Loss: 0.7151  |  Score: 52.44 [%]  |  Seq Length: 256.0
[2024-11-01 23:50:41,312][root][INFO] - Step: 600/4590  |  Loss: 0.7131  |  Score: 52.25 [%]  |  Seq Length: 256.0
[2024-11-01 23:50:54,218][root][INFO] - 

[2024-11-01 23:50:54,218][root][INFO] - This train_log.txt inform the Running Progress.

[2024-11-01 23:50:54,219][root][INFO] - Save the parser information to logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-11-01 23:50:54,219][root][INFO] - 

[2024-11-01 23:50:54,219][root][INFO] - Arguments: {'port': 5678, 'host': 'localhost', 'device': 'gpu', 'mixed_precision': 'bf16', 'eval_only': False, 'predict_only': False, 'seed': 1, 'seeds': '', 'model': {'name': 'skt/kogpt2-base-v2', 'hf_model': True, 'n_positions': 2048, 'ckpt_dir': '', 'compile': True, 'set_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}, 'set_sub2': True, 'sub2': {'tok_type': 'jamo_var', 'lang': 'ko', 'reducer': 'linear_pool', 'hidden_dim': 768, 'max_length': 2048, 'do_combination': False, 'embedding_norm': False, 'combination': {'combination_type': 'gru', 'intermediate_size': 3072, 'num_attention_heads': 12, 'num_trans_layers': 3}, 'add_lora': True, 'lora': {'r': 32, 'alpha': 128, 'dropout': 0.03}}}, 'data': {'language': 'ko', 'tok_type': 'morphemeSubword', 'vocab_size': '32k', 'raw_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated.txt', 'train_data_path': '/data3/user21/KOMBO/datasets/pretraining/concatenated_doc.txt', 'split_by_doc': True, 'is_toyset': False, 'max_length': 256, 'num_workers': 4, 'num_labels': 2, 'remain_lang': 'ko_punc', 'do_hangeulize': False, 'data_remove': False, 'task_name': 'KB_BoolQ'}, 'optim': {'name': 'adamwscale', 'base_lr': 0.01, 'base_lrs': '', 'batch_size': 8, 'total_steps': -1, 'epochs': 10, 'warmup_steps': -1, 'warmup_ratio': 0, 'lr_scheduler': 'cosine', 'weight_decay': 0.0, 'grad_clip': 1.0, 'grad_acc': 1, 'final_cosine': 1e-05, 'early_stop_patience': 3, 'dropout_prob': 0.1}, 'eval': {'eval_steps': 10000, 'steps': 500}, 'checkpoint': {'save_steps': 50000, 'max_number': 3}, 'logging': {'log_steps': 100, 'grad_l2': True, 'weights_l2': True, 'log_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs', 'tb_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb', 'save_dir': 'logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt'}, 'mode': 'nlu_ft'}

[2024-11-01 23:51:02,302][root][INFO] - Replaced transformer.h.0.attn.c_attn with LoRA_Layer
[2024-11-01 23:51:02,303][root][INFO] - Replaced transformer.h.0.attn.c_proj with LoRA_Layer
[2024-11-01 23:51:02,304][root][INFO] - Replaced transformer.h.1.attn.c_attn with LoRA_Layer
[2024-11-01 23:51:02,304][root][INFO] - Replaced transformer.h.1.attn.c_proj with LoRA_Layer
[2024-11-01 23:51:02,305][root][INFO] - Replaced transformer.h.2.attn.c_attn with LoRA_Layer
[2024-11-01 23:51:02,305][root][INFO] - Replaced transformer.h.2.attn.c_proj with LoRA_Layer
[2024-11-01 23:51:02,306][root][INFO] - Replaced transformer.h.3.attn.c_attn with LoRA_Layer
[2024-11-01 23:51:02,306][root][INFO] - Replaced transformer.h.3.attn.c_proj with LoRA_Layer
[2024-11-01 23:51:02,307][root][INFO] - Replaced transformer.h.4.attn.c_attn with LoRA_Layer
[2024-11-01 23:51:02,307][root][INFO] - Replaced transformer.h.4.attn.c_proj with LoRA_Layer
[2024-11-01 23:51:02,308][root][INFO] - Replaced transformer.h.5.attn.c_attn with LoRA_Layer
[2024-11-01 23:51:02,309][root][INFO] - Replaced transformer.h.5.attn.c_proj with LoRA_Layer
[2024-11-01 23:51:02,309][root][INFO] - Replaced transformer.h.6.attn.c_attn with LoRA_Layer
[2024-11-01 23:51:02,310][root][INFO] - Replaced transformer.h.6.attn.c_proj with LoRA_Layer
[2024-11-01 23:51:02,310][root][INFO] - Replaced transformer.h.7.attn.c_attn with LoRA_Layer
[2024-11-01 23:51:02,311][root][INFO] - Replaced transformer.h.7.attn.c_proj with LoRA_Layer
[2024-11-01 23:51:02,311][root][INFO] - Replaced transformer.h.8.attn.c_attn with LoRA_Layer
[2024-11-01 23:51:02,312][root][INFO] - Replaced transformer.h.8.attn.c_proj with LoRA_Layer
[2024-11-01 23:51:02,312][root][INFO] - Replaced transformer.h.9.attn.c_attn with LoRA_Layer
[2024-11-01 23:51:02,313][root][INFO] - Replaced transformer.h.9.attn.c_proj with LoRA_Layer
[2024-11-01 23:51:02,314][root][INFO] - Replaced transformer.h.10.attn.c_attn with LoRA_Layer
[2024-11-01 23:51:02,314][root][INFO] - Replaced transformer.h.10.attn.c_proj with LoRA_Layer
[2024-11-01 23:51:02,315][root][INFO] - Replaced transformer.h.11.attn.c_attn with LoRA_Layer
[2024-11-01 23:51:02,317][root][INFO] - Replaced transformer.h.11.attn.c_proj with LoRA_Layer
[2024-11-01 23:51:02,319][root][INFO] - Trainable params: 1769472 || all params: 126935040 || trainable: 1.39 %
[2024-11-01 23:51:02,324][root][INFO] - Change the max_length to 2046 for the sub2_tokenizer's truncation.
[2024-11-01 23:51:02,493][root][INFO] - Replaced transformer.wte with SUB2_LoRA_Layer
[2024-11-01 23:51:02,496][root][INFO] - Trainable params: 15527424 || all params: 140692992 || trainable: 11.04 %
[2024-11-01 23:51:02,668][accelerate.utils.other][WARNING] - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-11-01 23:51:04,614][root][INFO] - 

[2024-11-01 23:51:04,614][root][INFO] - ========== Fine-tuning on KB_BoolQ ==========
[2024-11-01 23:51:04,614][root][INFO] - model                   : skt/kogpt2-base-v2
[2024-11-01 23:51:04,622][root][INFO] - vocab size              : 51200
[2024-11-01 23:51:04,622][root][INFO] - device                  : gpu
[2024-11-01 23:51:04,622][root][INFO] - random seed             : 1
[2024-11-01 23:51:04,623][root][INFO] - train data size         : 3672
[2024-11-01 23:51:04,623][root][INFO] - max epochs              : 10
[2024-11-01 23:51:04,623][root][INFO] - total steps             : 4590
[2024-11-01 23:51:04,623][root][INFO] - warmup steps            : 0
[2024-11-01 23:51:04,623][root][INFO] - batch size              : 8
[2024-11-01 23:51:04,623][root][INFO] - accumulation steps      : 1
[2024-11-01 23:51:04,623][root][INFO] - optimizer               : adamwscale
[2024-11-01 23:51:04,623][root][INFO] - lr_scheduler            : cosine
[2024-11-01 23:51:04,623][root][INFO] - learning rate           : 0.01
[2024-11-01 23:51:04,623][root][INFO] - max length              : 256

[2024-11-01 23:51:04,623][root][INFO] - LoRA Configuration
[2024-11-01 23:51:04,623][root][INFO] - ㄴ r                    : 32
[2024-11-01 23:51:04,624][root][INFO] - ㄴ alpha                : 128
[2024-11-01 23:51:04,624][root][INFO] - ㄴ dropout              : 0.03

[2024-11-01 23:51:04,624][root][INFO] - SUB2 Configuration
[2024-11-01 23:51:04,624][root][INFO] - ㄴ tok_type             : jamo_var
[2024-11-01 23:51:04,624][root][INFO] - ㄴ hidden_dim           : 768
[2024-11-01 23:51:04,624][root][INFO] - ㄴ sub2_max_length     : 2046
[2024-11-01 23:51:04,624][root][INFO] - ㄴ embedding_norm       : False
[2024-11-01 23:51:04,624][root][INFO] - ㄴ do_combination       : False
[2024-11-01 23:51:04,624][root][INFO] - ㄴ reducer              : linear_pool

[2024-11-01 23:51:04,624][root][INFO] - LoRA in SUB2 Configuration
[2024-11-01 23:51:04,625][root][INFO] - ㄴ r                : 32
[2024-11-01 23:51:04,625][root][INFO] - ㄴ alpha            : 128
[2024-11-01 23:51:04,625][root][INFO] - ㄴ dropout          : 0.03

[2024-11-01 23:51:04,625][root][INFO] - 

[2024-11-01 23:51:04,625][root][INFO] - * log dir       : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs
[2024-11-01 23:51:04,625][root][INFO] - * save dir      : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/ckpt
[2024-11-01 23:51:04,625][root][INFO] - * tb dir        : logs/debug/skt_kogpt2-base-v2/KB_BoolQ/lora_sub2_jamo_var_2048_red-linear_pool_256t_8b_1s_0.01lr_1rs/tb
[2024-11-01 23:51:04,625][root][INFO] - * tb interval   : 100

[2024-11-01 23:51:04,625][root][INFO] - 

[2024-11-01 23:51:04,625][root][INFO] - Start the Training !
[2024-11-01 23:51:04,628][root][INFO] - 
[1/ 10 Epoch]
[2024-11-01 23:51:17,242][root][INFO] - Step: 100/4590  |  Loss: 0.8614  |  Score: 52.25 [%]  |  Seq Length: 256.0
[2024-11-01 23:51:28,434][root][INFO] - Step: 200/4590  |  Loss: 0.7600  |  Score: 48.38 [%]  |  Seq Length: 256.0
[2024-11-01 23:51:39,824][root][INFO] - Step: 300/4590  |  Loss: 0.7655  |  Score: 50.12 [%]  |  Seq Length: 256.0
[2024-11-01 23:51:51,092][root][INFO] - Step: 400/4590  |  Loss: 0.7115  |  Score: 50.12 [%]  |  Seq Length: 256.0
[2024-11-01 23:51:57,613][root][INFO] - Step: 459/4590  |  Loss: 0.7036  |  Score: 51.06 [%]  |  Seq Length: 256.0
[2024-11-01 23:52:01,948][root][INFO] - ########################  DEV REPORT #EP1  ########################
[2024-11-01 23:52:01,948][root][INFO] - Score: 47.02 [%]  |  Evaluation Time: 4.33 [s]
[2024-11-01 23:52:10,396][root][INFO] - ########################  TEST REPORT #EP1  ########################
[2024-11-01 23:52:10,396][root][INFO] - Score: 49.64 [%]  |  Evaluation Time: 8.45 [s]
[2024-11-01 23:52:10,397][root][INFO] - 
Save new Best Score (Epoch: 1)
[2024-11-01 23:52:10,399][root][INFO] - 
[2/ 10 Epoch]
[2024-11-01 23:52:15,286][root][INFO] - Step: 500/4590  |  Loss: 0.7207  |  Score: 49.09 [%]  |  Seq Length: 256.0
[2024-11-01 23:52:26,763][root][INFO] - Step: 600/4590  |  Loss: 0.7047  |  Score: 49.38 [%]  |  Seq Length: 256.0
[2024-11-01 23:52:38,206][root][INFO] - Step: 700/4590  |  Loss: 0.7030  |  Score: 50.62 [%]  |  Seq Length: 256.0
[2024-11-01 23:52:49,623][root][INFO] - Step: 800/4590  |  Loss: 0.7118  |  Score: 49.62 [%]  |  Seq Length: 256.0
[2024-11-01 23:53:01,059][root][INFO] - Step: 900/4590  |  Loss: 0.7223  |  Score: 49.38 [%]  |  Seq Length: 256.0
[2024-11-01 23:53:03,178][root][INFO] - Step: 918/4590  |  Loss: 0.7119  |  Score: 45.83 [%]  |  Seq Length: 256.0
[2024-11-01 23:53:07,471][root][INFO] - ########################  DEV REPORT #EP2  ########################
[2024-11-01 23:53:07,471][root][INFO] - Score: 46.73 [%]  |  Evaluation Time: 4.29 [s]
[2024-11-01 23:53:15,863][root][INFO] - ########################  TEST REPORT #EP2  ########################
[2024-11-01 23:53:15,863][root][INFO] - Score: 50.14 [%]  |  Evaluation Time: 8.39 [s]
[2024-11-01 23:53:15,864][root][INFO] - 
Save new Best Score (Epoch: 2)
[2024-11-01 23:53:15,866][root][INFO] - 
[3/ 10 Epoch]
[2024-11-01 23:53:26,071][root][INFO] - Step: 1000/4590  |  Loss: 0.7012  |  Score: 49.24 [%]  |  Seq Length: 256.0
[2024-11-01 23:53:38,119][root][INFO] - Step: 1100/4590  |  Loss: 0.6961  |  Score: 53.12 [%]  |  Seq Length: 256.0
[2024-11-01 23:53:49,375][root][INFO] - Step: 1200/4590  |  Loss: 0.7133  |  Score: 50.25 [%]  |  Seq Length: 256.0
[2024-11-01 23:54:00,447][root][INFO] - Step: 1300/4590  |  Loss: 0.6989  |  Score: 49.38 [%]  |  Seq Length: 256.0
[2024-11-01 23:54:09,026][root][INFO] - Step: 1377/4590  |  Loss: 0.6978  |  Score: 50.00 [%]  |  Seq Length: 256.0
[2024-11-01 23:54:13,248][root][INFO] - ########################  DEV REPORT #EP3  ########################
[2024-11-01 23:54:13,249][root][INFO] - Score: 52.41 [%]  |  Evaluation Time: 4.22 [s]
[2024-11-01 23:54:21,483][root][INFO] - ########################  TEST REPORT #EP3  ########################
[2024-11-01 23:54:21,483][root][INFO] - Score: 52.84 [%]  |  Evaluation Time: 8.23 [s]
[2024-11-01 23:54:21,484][root][INFO] - 
Save new Best Score (Epoch: 3)
[2024-11-01 23:54:21,485][root][INFO] - 
[4/ 10 Epoch]
[2024-11-01 23:54:24,359][root][INFO] - Step: 1400/4590  |  Loss: 0.7029  |  Score: 45.65 [%]  |  Seq Length: 256.0
[2024-11-01 23:54:35,318][root][INFO] - Step: 1500/4590  |  Loss: 0.7001  |  Score: 52.50 [%]  |  Seq Length: 256.0
[2024-11-01 23:54:46,245][root][INFO] - Step: 1600/4590  |  Loss: 0.7014  |  Score: 55.88 [%]  |  Seq Length: 256.0
[2024-11-01 23:54:57,321][root][INFO] - Step: 1700/4590  |  Loss: 0.6890  |  Score: 53.12 [%]  |  Seq Length: 256.0
[2024-11-01 23:55:07,768][root][INFO] - Step: 1800/4590  |  Loss: 0.6922  |  Score: 53.12 [%]  |  Seq Length: 256.0
[2024-11-01 23:55:11,625][root][INFO] - Step: 1836/4590  |  Loss: 0.6929  |  Score: 52.78 [%]  |  Seq Length: 256.0
[2024-11-01 23:55:16,154][root][INFO] - ########################  DEV REPORT #EP4  ########################
[2024-11-01 23:55:16,154][root][INFO] - Score: 50.00 [%]  |  Evaluation Time: 4.53 [s]
[2024-11-01 23:55:24,898][root][INFO] - ########################  TEST REPORT #EP4  ########################
[2024-11-01 23:55:24,898][root][INFO] - Score: 53.41 [%]  |  Evaluation Time: 8.74 [s]
[2024-11-01 23:55:24,902][root][INFO] - 
[5/ 10 Epoch]
[2024-11-01 23:55:31,913][root][INFO] - Step: 1900/4590  |  Loss: 0.6914  |  Score: 55.08 [%]  |  Seq Length: 256.0
[2024-11-01 23:55:42,862][root][INFO] - Step: 2000/4590  |  Loss: 0.6962  |  Score: 52.88 [%]  |  Seq Length: 256.0
[2024-11-01 23:55:53,506][root][INFO] - Step: 2100/4590  |  Loss: 0.6929  |  Score: 53.50 [%]  |  Seq Length: 256.0
[2024-11-01 23:56:04,075][root][INFO] - Step: 2200/4590  |  Loss: 0.6835  |  Score: 55.25 [%]  |  Seq Length: 256.0
[2024-11-01 23:56:14,127][root][INFO] - Step: 2295/4590  |  Loss: 0.7100  |  Score: 52.89 [%]  |  Seq Length: 256.0
[2024-11-01 23:56:18,287][root][INFO] - ########################  DEV REPORT #EP5  ########################
[2024-11-01 23:56:18,287][root][INFO] - Score: 54.83 [%]  |  Evaluation Time: 4.16 [s]
[2024-11-01 23:56:26,387][root][INFO] - ########################  TEST REPORT #EP5  ########################
[2024-11-01 23:56:26,388][root][INFO] - Score: 50.64 [%]  |  Evaluation Time: 8.10 [s]
[2024-11-01 23:56:26,389][root][INFO] - 
Save new Best Score (Epoch: 5)
[2024-11-01 23:56:26,390][root][INFO] - 
[6/ 10 Epoch]
[2024-11-01 23:56:27,195][root][INFO] - Step: 2300/4590  |  Loss: 0.6813  |  Score: 55.00 [%]  |  Seq Length: 256.0
[2024-11-01 23:56:38,182][root][INFO] - Step: 2400/4590  |  Loss: 0.6818  |  Score: 58.38 [%]  |  Seq Length: 256.0
[2024-11-01 23:56:48,976][root][INFO] - Step: 2500/4590  |  Loss: 0.6860  |  Score: 54.62 [%]  |  Seq Length: 256.0
[2024-11-01 23:56:59,799][root][INFO] - Step: 2600/4590  |  Loss: 0.6751  |  Score: 57.00 [%]  |  Seq Length: 256.0
[2024-11-01 23:57:10,379][root][INFO] - Step: 2700/4590  |  Loss: 0.6824  |  Score: 57.25 [%]  |  Seq Length: 256.0
[2024-11-01 23:57:15,893][root][INFO] - Step: 2754/4590  |  Loss: 0.6737  |  Score: 58.10 [%]  |  Seq Length: 256.0
[2024-11-01 23:57:20,032][root][INFO] - ########################  DEV REPORT #EP6  ########################
[2024-11-01 23:57:20,032][root][INFO] - Score: 50.14 [%]  |  Evaluation Time: 4.14 [s]
[2024-11-01 23:57:28,041][root][INFO] - ########################  TEST REPORT #EP6  ########################
[2024-11-01 23:57:28,042][root][INFO] - Score: 52.91 [%]  |  Evaluation Time: 8.01 [s]
[2024-11-01 23:57:28,044][root][INFO] - 
[7/ 10 Epoch]
[2024-11-01 23:57:33,266][root][INFO] - Step: 2800/4590  |  Loss: 0.6575  |  Score: 58.97 [%]  |  Seq Length: 256.0
[2024-11-01 23:57:43,934][root][INFO] - Step: 2900/4590  |  Loss: 0.6698  |  Score: 58.63 [%]  |  Seq Length: 256.0
[2024-11-01 23:57:54,516][root][INFO] - Step: 3000/4590  |  Loss: 0.6804  |  Score: 56.62 [%]  |  Seq Length: 256.0
[2024-11-01 23:58:05,159][root][INFO] - Step: 3100/4590  |  Loss: 0.6771  |  Score: 54.87 [%]  |  Seq Length: 256.0
[2024-11-01 23:58:15,626][root][INFO] - Step: 3200/4590  |  Loss: 0.6639  |  Score: 59.38 [%]  |  Seq Length: 256.0
[2024-11-01 23:58:16,995][root][INFO] - Step: 3213/4590  |  Loss: 0.6905  |  Score: 55.77 [%]  |  Seq Length: 256.0
[2024-11-01 23:58:21,129][root][INFO] - ########################  DEV REPORT #EP7  ########################
[2024-11-01 23:58:21,129][root][INFO] - Score: 51.56 [%]  |  Evaluation Time: 4.13 [s]
[2024-11-01 23:58:29,162][root][INFO] - ########################  TEST REPORT #EP7  ########################
[2024-11-01 23:58:29,162][root][INFO] - Score: 52.91 [%]  |  Evaluation Time: 8.03 [s]
[2024-11-01 23:58:29,164][root][INFO] - 
[8/ 10 Epoch]
[2024-11-01 23:58:38,591][root][INFO] - Step: 3300/4590  |  Loss: 0.6732  |  Score: 57.90 [%]  |  Seq Length: 256.0
[2024-11-01 23:58:49,114][root][INFO] - Step: 3400/4590  |  Loss: 0.6614  |  Score: 60.75 [%]  |  Seq Length: 256.0
[2024-11-01 23:58:59,800][root][INFO] - Step: 3500/4590  |  Loss: 0.6432  |  Score: 60.50 [%]  |  Seq Length: 256.0
[2024-11-01 23:59:10,617][root][INFO] - Step: 3600/4590  |  Loss: 0.6459  |  Score: 62.25 [%]  |  Seq Length: 256.0
[2024-11-01 23:59:18,761][root][INFO] - Step: 3672/4590  |  Loss: 0.6546  |  Score: 62.15 [%]  |  Seq Length: 256.0
[2024-11-01 23:59:22,944][root][INFO] - ########################  DEV REPORT #EP8  ########################
[2024-11-01 23:59:22,944][root][INFO] - Score: 53.41 [%]  |  Evaluation Time: 4.18 [s]
[2024-11-01 23:59:30,951][root][INFO] - ########################  TEST REPORT #EP8  ########################
[2024-11-01 23:59:30,951][root][INFO] - Score: 51.85 [%]  |  Evaluation Time: 8.00 [s]
[2024-11-01 23:59:30,954][root][INFO] - 
[9/ 10 Epoch]
[2024-11-01 23:59:34,197][root][INFO] - Step: 3700/4590  |  Loss: 0.6398  |  Score: 64.29 [%]  |  Seq Length: 256.0
[2024-11-01 23:59:45,318][root][INFO] - Step: 3800/4590  |  Loss: 0.6315  |  Score: 64.62 [%]  |  Seq Length: 256.0
[2024-11-01 23:59:55,809][root][INFO] - Step: 3900/4590  |  Loss: 0.6471  |  Score: 62.38 [%]  |  Seq Length: 256.0
[2024-11-02 00:00:06,777][root][INFO] - Step: 4000/4590  |  Loss: 0.6648  |  Score: 59.88 [%]  |  Seq Length: 256.0
[2024-11-02 00:00:17,482][root][INFO] - Step: 4100/4590  |  Loss: 0.6204  |  Score: 65.25 [%]  |  Seq Length: 256.0
[2024-11-02 00:00:20,927][root][INFO] - Step: 4131/4590  |  Loss: 0.6710  |  Score: 59.68 [%]  |  Seq Length: 256.0
[2024-11-02 00:00:25,147][root][INFO] - ########################  DEV REPORT #EP9  ########################
[2024-11-02 00:00:25,147][root][INFO] - Score: 53.69 [%]  |  Evaluation Time: 4.22 [s]
[2024-11-02 00:00:33,130][root][INFO] - ########################  TEST REPORT #EP9  ########################
[2024-11-02 00:00:33,130][root][INFO] - Score: 53.20 [%]  |  Evaluation Time: 7.98 [s]
[2024-11-02 00:00:33,132][root][INFO] - 
Save new Best Score (Epoch: 9)
[2024-11-02 00:00:33,133][root][INFO] - 
[10/ 10 Epoch]
[2024-11-02 00:00:40,643][root][INFO] - Step: 4200/4590  |  Loss: 0.6303  |  Score: 62.50 [%]  |  Seq Length: 256.0
[2024-11-02 00:00:51,722][root][INFO] - Step: 4300/4590  |  Loss: 0.6316  |  Score: 67.00 [%]  |  Seq Length: 256.0
